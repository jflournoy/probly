---
title: "Fit model to participant data"
author: "John Flournoy"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    toc: yes
    toc_depth: 2
    number_sections: FALSE
pkgdown:
  as_is: true
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo=F,message=F,warning=F,error=F
)

knitr::read_chunk(system.file('r_aux', 'load_data_for_sim.R', package = 'probly'))
```

```{r}
library(probly)
library(dplyr)
library(tidyr)
library(ggplot2)
library(rstan)
library(bayesplot)

save_out_list <- c()
save_out_dir <- '~/code_new/social-motives-rl-writeup/rda/'
if(!dir.exists(save_out_dir)){
    dir.create(save_out_dir)
}
```

```{r load_data_for_sim}
```

# Method

Using the model presented in the section on [testing simulated data](test-simulated-data.html), I estimate the posterior distribution of the model parameters given the data provided by four groups of participants.
Quality of estimation can be supported by (A), (B), (C), (D).

# Results

## Descriptive plots

Examination of the raw data using scatterplots and non-parametric best-fit lines can help insure that data conform to expectations, and that subsequent model estimates accurately reflect the data.
On average, participants in all samples and all conditions demonstrated learning (Figure \@ref(fig:trialaverages)A, although note that given the small sample size, the foster-care involved sample shows more variability across trials).
There is also some indication that learning occurs more quickly in the two social motive conditions (Figure \@ref(fig:trialaverages)B).

(ref:trialaverages) Average number of optimal responses for each trial across all participants. Best fit lines are generalized additive models with 95% confidence intervals. 

```{r trialaverages, fig.width=9, fig.height=5, cache=T, caption='(ref:trialaverages)'}
splt_averages <- splt %>%
    dplyr::group_by(sample, condition, condition_trial_index) %>%
    dplyr::summarize(p_optimal = mean(correcttrue, na.rm = T)) %>%
    dplyr::ungroup()

splt_averages_over_samples <- splt %>%
    dplyr::group_by(condition, condition_trial_index) %>%
    dplyr::summarize(p_optimal = mean(correcttrue, na.rm = T)) %>%
    dplyr::ungroup()

avg_per_sample_plot <- ggplot2::ggplot(
    splt_averages,
    ggplot2::aes(x = condition_trial_index,
                 y = p_optimal,
                 group = condition,
                 linetype = condition,
                 shape = condition)) +
    ggplot2::geom_point(alpha = .1) +
    ggplot2::geom_smooth(
        color = 'black', size = .5,
        method = 'gam', formula = y ~ s(x, bs = "cr", k = 6, fx = T), se = T) +
    ggplot2::facet_wrap(~sample, nrow = 2) +
    ggplot2::labs(x = 'Within-condition trial number',
                  y = 'Proportion of optimal responses',
                  title = 'A') +
    ggplot2::theme_minimal() + 
    ggplot2::theme(legend.position = 'none') + 
    ggplot2::coord_cartesian(ylim = c(.5, .9))

avg_across_samples <- ggplot2::ggplot(
    splt_averages_over_samples,
    ggplot2::aes(x = condition_trial_index,
                 y = p_optimal,
                 group = condition,
                 linetype = condition,
                 shape = condition)) +
    ggplot2::geom_point(alpha = .1) +
    ggplot2::geom_smooth(
        color = 'black', size = .5,
        method = 'gam', formula = y ~ s(x, bs = "cr", k = 6, fx = T), se = T) +
    ggplot2::labs(x = 'Within-condition trial number',
                  y = 'Proportion of optimal responses',
                  title = 'B') +
    ggplot2::guides(linetype = guide_legend(override.aes=list(fill=NA)),
                    shape = guide_legend(override.aes=list(alpha=.5))) + 
    ggplot2::theme_minimal() + 
    ggplot2::theme(legend.key = element_rect(fill = 'white', color = 'white'),
                   legend.background = element_rect(fill = 'white', color = 'white')) + 
    ggplot2::coord_cartesian(ylim = c(.5, .8))

task_descriptive_figure <- gridExtra::grid.arrange(avg_per_sample_plot, avg_across_samples, nrow = 1)

gridExtra::grid.arrange(task_descriptive_figure)
save_out_list <- c(save_out_list, 'task_descriptive_figure')
```

Participants also reported their confidence about which reponses would give them rewards most often (this single item measured confidence globally).
Confidence generally increases aross trials for all samples (Figure \@ref(fig:confidenceplot)).

(ref:confidenceplot) Self-reported confidence over blocks.

```{r confidenceplot, fig.width=6, fig.height=4, cache=T, fig.cap='(ref:confidenceplot)'}
confidence_description_plot <- ggplot2::ggplot(
    dplyr::filter(splt_confidence, !is.na(sample), sample != 'TDS3'),
    ggplot2::aes(x = block, y = confidence)) +
    ggplot2::geom_violin(ggplot2::aes(group = block), scale = 'area', bw = .4, color = '#666666',alpha = 0,
                         position = position_identity())+
    ggplot2::geom_smooth(
        ggplot2::aes(color = sample),
        method = 'gam', formula = y ~ s(x, bs = "cs", k = 8),
        se = T,
        alpha = .1, size = .5) +
    ggplot2::scale_color_manual(values = ggsci::pal_rickandmorty()(8)[c(2:3,7:8)]) + 
    ggplot2::theme_minimal() +
    ggplot2::guides(color = guide_legend(override.aes=list(fill=NA))) + 
    ggplot2::labs(x = 'Block number',
                  y = 'Self-report confidence about optimal label',
                  color = 'Sample')
confidence_description_plot
save_out_list <- c(save_out_list, 'confidence_description_plot')
```

### Proportion optimal presses per ID & condition

Each of the plots below shows, for each individual, the trajectory of learning across trials.
Individual plots are sorted by the overall proportion of optimal presses.

```{r cache=T}
plot_opt_presses_per_id <- function(splt_df,
                                    ncol = 5,
                                    title = 'Proportion optimal presses per ID & condition',
                                    se = F){
    splt_opt_press <- dplyr::mutate(
        ungroup(splt_df),
        press_opt = as.numeric(pressed_r == (proportion == '20_80')),
        press_opt = (press_opt - .5) * (1 -.05 * (as.numeric(condition) - 1)) + .5)

    splt_opt_press_ranks <- dplyr::summarize(
        dplyr::group_by(splt_opt_press, id),
                        mean_press_opt = mean(press_opt, na.rm = T))
    splt_opt_press_ranks$id_fac <- factor(
        splt_opt_press_ranks$id,
        levels = splt_opt_press_ranks$id[order(splt_opt_press_ranks$mean_press_opt)])

    splt_opt_press <- left_join(splt_opt_press, splt_opt_press_ranks)

    ggplot2::ggplot(
        splt_opt_press,
        ggplot2::aes(
            x = condition_trial_index,
            y = press_opt,
            linetype = condition,
            shape = condition)) +
        ggplot2::geom_point(size = .75, alpha = .125) +
        ggplot2::geom_hline(yintercept = c(.8), alpha = 1, color = 'gray', size = .5) +
        ggplot2::geom_hline(yintercept = c(.5), alpha = 1, color = 'red', size = .5) +
        ggplot2::geom_smooth(se = se, method = 'gam', formula = y ~ s(x, k = 3, fx = T),
                             size = .5, color = 'black', alpha = .2) +
        ggplot2::facet_wrap(~ id_fac, ncol = ncol) +
        ggplot2::coord_cartesian(ylim = c(0, 1)) +
        ggplot2::scale_y_continuous(breaks = c(0,.5,.8)) +
        ggplot2::scale_x_continuous(breaks = c(1,64,128)) +
        ggplot2::theme_minimal() +
        ggplot2::labs(x = 'Trial', y = 'Optimal press (probability)', title = title)
}
```

```{r proportion_trials_per_conditionca, fig.width=10, fig.height=22, cache=F, eval=F}
plot_opt_presses_per_id(
    dplyr::filter(splt, sample == 'Community adolescents'),
    ncol = 5,
    title = 'Community adolescents')
```

```{r proportion_trials_per_conditionfc, fig.width=10, fig.height=10, cache=F, eval=F}
plot_opt_presses_per_id(
    dplyr::filter(splt, sample == 'Foster-care involved adolescents'),
    ncol = 5,
    title = 'Foster-care involved adolescents')
```

```{r proportion_trials_per_conditioncs, fig.width=10, fig.height=22, cache=F, eval=F}
plot_opt_presses_per_id(
    dplyr::filter(splt, sample == 'College students'),
    ncol = 5,
    title = 'College students')
```

```{r proportion_trials_per_conditioncso, fig.width=10, fig.height=44, cache=F, eval=F}
plot_opt_presses_per_id(
    dplyr::filter(splt, sample == 'College students - online'),
    ncol = 5,
    title = 'College students - online')

```

# Results from logistic regression

```{r}
data_dir <- '/data/jflournoy/split/probly'
logistic_saves_dir <- file.path(data_dir,'/logistic/')


#  [1] "id"                    "dir"                   "sample.x"             
#  [4] "correcttrue"           "pressed_r"             "outcome"              
#  [7] "correct_r"             "reward_possible"       "condition"            
# [10] "condition_trial_index" "sex"                   "stim_image"           
# [13] "trial_index"           "block"                 "proportion"           
# [16] "rt"                    "time_elapsed"          "date_time_completed"  
# [19] "opt_is_right"          "press_opt"             "PDS_female_menses"    
# [22] "PDSS_adren"            "PDSS_gender"           "PDSS_gonad"           
# [25] "PDSS_pdss"             "PDS_mean_score"        "gender"               
# [28] "hispanic_yn"           "fsiq2"                 "sample.y"             
# [31] "age"                   "ethnicity" 

null_id_only_m <- probly::CachedFit(
    lme4::glmer(press_opt ~ 1 + (1 | id), 
                family = 'binomial',
                data = splt), 
    rds_filename = file.path(logistic_saves_dir, 'null_id_only_m.rds'))

null_m <- probly::CachedFit(
    lme4::glmer(press_opt ~ 1 + (1 | sample/id), 
                      family = 'binomial',
                      data = splt), 
    rds_filename = file.path(logistic_saves_dir, 'null_m.rds'))

null_stim_rx_m <- probly::CachedFit(
    lme4::glmer(press_opt ~ 1 + (1 | sample/id) + (1 | stim_image), 
                              family = 'binomial', 
                              data = splt), 
    rds_filename = file.path(logistic_saves_dir, 'null_stim_rx_m.rds'))

summary(null_id_only_m)
summary(null_m)
summary(null_stim_rx_m)
arm::invlogit(lme4::fixef(null_id_only_m))
arm::invlogit(lme4::fixef(null_m))
arm::invlogit(lme4::fixef(null_stim_rx_m))
anova(null_id_only_m, null_m, null_stim_rx_m)

summary(null_id_only_bm)

summary(null_bm)

summary(time_m)
arm::invlogit(fixef(time_m)[1] + c(0, fixef(time_m)[2]*-3.45))

summary(condition_m)
arm::invlogit(fixef(condition_m)[1] + c(0, fixef(condition_m)[2:3]))
```

# Results from Bayesian learning models

```{r}
stanfit_2_level <- readRDS('/data/jflournoy/split/probly/splt-tght-rl_2_level-1522962.RDS')
stanfit_2_level_no_b <- readRDS('/data/jflournoy/split/probly/splt-tght-rl_2_level_no_b-1522879.RDS')
stanfit_3_level <- readRDS('/data/jflournoy/split/probly/splt-tght-rl_repar_exp-1522998.RDS')
stanfit_3_level_no_b <- readRDS('/data/jflournoy/split/probly/splt-tght-rl_repar_exp_no_b-1522938.RDS')
stanfit <- stanfit_2_level_no_b

munames <- grep('mu', names(stanfit), value = T)
betanames <- grep('beta', names(stanfit), value = T)
taunames <- grep('tau_\\w{2,3}\\[', names(stanfit), value = T)
lomeganames <- grep('L_Omega', names(stanfit), value = T)

stanfit_2_level_betas <- as.matrix(stanfit_2_level, pars = betanames)
stanfit_2_level_no_b_betas <- as.matrix(stanfit_2_level_no_b, pars = betanames)
stanfit_3_level_betas <- as.matrix(stanfit_3_level, pars = betanames)
stanfit_3_level_no_b_betas <- as.matrix(stanfit_3_level_no_b, pars = betanames)

betas_xi_means <- lapply(list(
    'L2' = stanfit_2_level_betas,
    'L2_no_b' = stanfit_2_level_no_b_betas,
    'L3' = stanfit_3_level_betas,
    'L3_no_b' = stanfit_3_level_no_b_betas),
    function(stanmat){
        apply(stanmat[,betanames[1:(313*3)]], 
              2, function(j) mean(j))
    })

betas_ep_means <- lapply(list(
    'L2' = stanfit_2_level_betas,
    'L2_no_b' = stanfit_2_level_no_b_betas,
    'L3' = stanfit_3_level_betas,
    'L3_no_b' = stanfit_3_level_no_b_betas),
    function(stanmat){
        apply(stanmat[,betanames[(313*3+1):(2*3*313)]], 
              2, function(j) mean(arm::logit(j)))
    })

betas_rho_means <- lapply(list(
    'L2' = stanfit_2_level_betas,
    'L2_no_b' = stanfit_2_level_no_b_betas,
    'L3' = stanfit_3_level_betas,
    'L3_no_b' = stanfit_3_level_no_b_betas),
    function(stanmat){
        apply(stanmat[,betanames[(2*3*313+1):(3*3*313)]], 
              2, function(j) mean(log(j)))
    })

GGally::ggpairs(as.data.frame(betas_xi_means), title = expression(xi))
GGally::ggpairs(as.data.frame(betas_ep_means), title = expression(epsilon))
GGally::ggpairs(as.data.frame(betas_rho_means), title = expression(rho))
```

## Mean parameter estimates

### Learning rate

```{r}
munames <- grep('mu', names(stanfit_2_level), value = T)
stanfitmat <- as.array(stanfit_2_level)
ggplot2::theme_set(ggplot2::theme_minimal())
bayesplot::mcmc_combo(stanfitmat, pars = munames[4:6], facet_args = list(scales = 'fixed'),
                      # transformations = pnorm, 
                      combo = c('areas_ridges', 'trace'), 
                      prob_outer = 1-.005, prob = .95, widths = c(3,4))

mu_ep_1 <- rstan::extract(stanfit_2_level, pars = munames[4])[[1]]
mu_ep_2 <- rstan::extract(stanfit_2_level, pars = munames[5])[[1]]
mu_ep_3 <- rstan::extract(stanfit_2_level, pars = munames[6])[[1]]

bayesplot::mcmc_areas(data.frame(diff_ep2_ep1 = pnorm(mu_ep_2) - pnorm(mu_ep_1),
                                 diff_ep3_ep1 = pnorm(mu_ep_3) - pnorm(mu_ep_1),
                                 diff_ep3_ep2 = pnorm(mu_ep_3) - pnorm(mu_ep_2)), prob_outer = 1-.005, prob = .95)
```

### Size of reward modifier

```{r}
bayesplot::mcmc_combo(stanfitmat, pars = munames[10:12], facet_args = list(scales = 'fixed'),
                      combo = c('areas_ridges', 'trace'), 
                      prob_outer = 1-.005, prob = .95, widths = c(3,4))
```

### Irriducible noise (e.g., "what the whaaa?")

```{r}
bayesplot::mcmc_combo(stanfitmat, pars = munames[1:3], facet_args = list(scales = 'fixed'),
                      transformations = pnorm, combo = c('areas', 'trace'), prob_outer = 1-.005, prob = .95, widths = c(3,4))
```


## Model predicted behavior

Below is a plot of 313 model predicted runs, all using the exact same task structure (arbitrarily, the first run of the task). Behavior is guided by the mean parameter estimates, so any variabilility in behavior is due to the probabilistic nature of selecting either the right or left option based on the model's current knowledge at some point during the task. Another way of framing this is that it is the behavior of the averge actor, in 313 identical, hypothetical runs.

```{r getspltdata}

data(splt)
splt <- splt[!is.na(splt$pressed_r), ]
splt$cue <- as.numeric(as.factor(paste0(splt$condition, '_', splt$sex)))
splt$condition <- factor(splt$condition, levels = c('HngT', 'DtnL', 'PplU'))

# - N number of individuals
# - M number of samples
# - K number of conditions
# - mm sample ID for all individuals
# - Tsubj number of trials for each individual
# - cue an N x max(Tsubj) matrix of cue IDs for
#   each trial
# - n_cues total number of cues
# - condtion an N x max(Tsubj) matrix of condition
#   IDs for each trial
# - outcome is an array with dimensions N x T x 2
#   (response options) with the feedback for each
#   possible response. outcome[,,1] is for
#   correct left-presses, and outcome[,,2] is for
#   correct right-presses.
# - beta_xi, beta_b, beta_eps, beta_rho are N x K
#   matrices of the individually varying parameter
#   coefficients

group_index_mm <- get_sample_index(splt, levels = c("TDS1", "TDS2", "yads", "yads_online"))
N <- dim(group_index_mm)[1]
M <- length(levels(group_index_mm$m_fac))
K <- length(unique(splt$condition))
cue_mat <- get_col_as_trial_matrix(splt, 'cue', id_col = 'id', sample_col = 'sample', trial_col = 'trial_index')
Tsubj <- get_max_trials_per_individual(cue_mat)
condition_mat <- get_col_as_trial_matrix(splt, 'condition', id_col = 'id', sample_col = 'sample', trial_col = 'trial_index')
correct_r_mat <- get_col_as_trial_matrix(splt, 'correct_r', id_col = 'id', sample_col = 'sample', trial_col = 'trial_index')
reward_possible_mat <- get_col_as_trial_matrix(splt, 'reward_possible', id_col = 'id', sample_col = 'sample', trial_col = 'trial_index')
outcome_arr <- array(reward_possible_mat, dim = c(dim(reward_possible_mat), 2))
outcome_arr[,,1][correct_r_mat == 1] <- 0 #reward if left press, but right is correct = 0
outcome_arr[,,2][correct_r_mat == 0] <- 0 #reward if right press, but correct is not right = 0

Tsubj_unif <- Tsubj
Tsubj_unif[] <- Tsubj[1]
condition_mat_unif <- matrix(rep(condition_mat[1,], dim(condition_mat)[1]), byrow = T, nrow = dim(condition_mat)[1])
cue_mat_unif <- matrix(rep(cue_mat[1,], dim(cue_mat)[1]), byrow = T, nrow = dim(cue_mat)[1])
outcome_arr_unif <- outcome_arr
for(i in 1:dim(outcome_arr_unif)[1]){
    outcome_arr_unif[i,,] <- outcome_arr[1,,]
}

beta_xi_mat <- matrix(
    rep(apply(rstan::extract(stanfit_2_level, pars = 'mu_delta_xi')[[1]], c(2,3), mean), 
        313), 
    nrow = 313, 
    byrow = T)
beta_b_mat <- matrix(
    rep(apply(rstan::extract(stanfit_2_level, pars = 'mu_delta_b')[[1]], c(2,3), mean), 
        313), 
    nrow = 313, 
    byrow = T)
beta_ep_mat <- matrix(
    rep(apply(rstan::extract(stanfit_2_level, pars = 'mu_delta_ep')[[1]], c(2,3), mean), 
        313), 
    nrow = 313, 
    byrow = T)
beta_rho_mat <- matrix(
    rep(apply(rstan::extract(stanfit_2_level, pars = 'mu_delta_rho')[[1]], c(2,3), mean), 
        313), 
    nrow = 313, 
    byrow = T)

model_predicted_behavior <- probly::generate_responses(N = N, M = M, K = K, mm = group_index_mm, 
                                               Tsubj = Tsubj_unif, cue = cue_mat_unif, 
                                               n_cues = max(cue_mat_unif, na.rm = T), 
                                               condition = condition_mat_unif, 
                                               outcome = outcome_arr_unif, 
                                               beta_xi = beta_xi_mat, beta_b = beta_b_mat, 
                                               beta_eps = beta_ep_mat, beta_rho = beta_rho_mat)

adf <- dplyr::mutate(
    dplyr::group_by(
        data.frame(id = rep(1:313, 381),
                   condition = factor(as.numeric(condition_mat_unif[,1:381]), 
                                      levels = 1:3,
                                      labels = c('Hungry/Thirsty', 'Dating/Looking', 'Popular/Unpopular')),
                   p_press_right = abs(as.numeric(model_predicted_behavior$p_press_right) - .5)),
        condition, id),
    trial = 1:n())

ggplot2::ggplot(adf,
                ggplot2::aes(x = trial, 
                             y = p_press_right, 
                             group = interaction(condition, id))) + 
    ggplot2::geom_hex(ggplot2::aes(group = NULL, alpha = log(..count..)), fill = '#666666') + 
    ggplot2::geom_line(stat = 'smooth', method = 'gam', formula = y ~ s(x, k = 6, fx = T),
                       alpha = .05) + 
    ggplot2::facet_grid(~condition) + 
    ggplot2::theme_minimal() +
    ggplot2::coord_cartesian(ylim = c(0, .5)) + 
    ggplot2::scale_y_continuous(breaks = c(0, .3, .5), labels = c(.5, .8, 1)) +
    ggplot2::scale_alpha_continuous(breaks = log(c(10, 100, 500, 1000)),
                                    labels = c(10, 100, 500, 1000),
                                    range = c(0, 1),
                                    name = 'log(Num Obs)') +
    ggplot2::labs(x = 'Trial number', y = 'Probability of optimal choice')
```

## Diagnostics


```{r eval=F}
rstan::stan_diag(stanfit, information = 'sample')
rstan::stan_diag(stanfit, information = 'stepsize')
rstan::stan_diag(stanfit, information = 'treedepth')
rstan::stan_diag(stanfit, information = 'divergence')

for(i in 1:length(munames)){
    rstan::stan_par(stanfit, par = munames[i])
}
rstan::stan_rhat(stanfit, pars = munames)
rstan::stan_ess(stanfit, pars = munames)
pairs(stanfit, pars = munames, condition = 'accept_stat__')

for(i in 1:(length(betanames)/313)){
    thesebetanames <- betanames[(313*(i-1) + 1):(313*i)]
    print(
        rstan::stan_rhat(
            stanfit, 
            pars = thesebetanames, binwidth = .005) + 
            geom_vline(xintercept = 1.1) + 
            labs(title = paste0('Rhat: ', thesebetanames[1])))
    print(
        rstan::stan_ess(
            stanfit, 
            pars = thesebetanames, binwidth = .01) + 
            geom_vline(xintercept = .001) + 
            labs(title = paste0('ESS: ', thesebetanames[1])))
}


```

```{r pairs, fig.width=15, fig.height=15, eval=F}
pairs(stanfit, pars = munames)
pairs(stanfit, pars = taunames)
```

```{r pairs2, fig.width=30, fig.height=30}
ep_cor_cov_samps <- probly::extract_cor_cov_samps(stanfit, par_subscript = 'ep')
xi_cor_cov_samps <- probly::extract_cor_cov_samps(stanfit, par_subscript = 'xi')
rho_cor_cov_samps <- probly::extract_cor_cov_samps(stanfit, par_subscript = 'rho')

ep_cov_samps <- as.data.frame(t(apply(ep_cor_cov_samps$Sigma, 3, function(x) {
    d <- c(x[lower.tri(x)], diag(x))
    names(d) <- c('[2,1]', '[3,1]', '[3,2]', '[1,1]', '[2,2]', '[3,3]')
    d
})))
names(ep_cov_samps) <- paste0('ep_', names(ep_cov_samps))
xi_cov_samps <- as.data.frame(t(apply(xi_cor_cov_samps$Sigma, 3, function(x) {
    d <- c(x[lower.tri(x)], diag(x))
    names(d) <- c('[2,1]', '[3,1]', '[3,2]', '[1,1]', '[2,2]', '[3,3]')
    d
})))
names(xi_cov_samps) <- paste0('xi_', names(xi_cov_samps))
rho_cov_samps <- as.data.frame(t(apply(rho_cor_cov_samps$Sigma, 3, function(x) {
    d <- c(x[lower.tri(x)], diag(x))
    names(d) <- c('[2,1]', '[3,1]', '[3,2]', '[1,1]', '[2,2]', '[3,3]')
    d
})))
names(rho_cov_samps) <- paste0('rho_', names(rho_cov_samps))


sumnuts <- bayesplot::nuts_params(stanfit)

ggplot2::theme_set(theme_minimal())
bayesplot::mcmc_pairs(
    cbind(
        ep_cov_samps, 
        rho_cov_samps, 
        xi_cov_samps, 
        Chain = rep(1:6, each=750)), 
    np = sumnuts,
    off_diag_fun = 'hex')
```

```{r echo = F, eval = F}
save(list = save_out_list, file = file.path(save_out_dir, 'fit-model-to-participant-data.rda'))
```
