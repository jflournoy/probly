---
title: "Fit model to participant data"
author: "John Flournoy"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    toc: yes
    toc_depth: 2
    number_sections: FALSE
pkgdown:
as_is: true
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: "/home/jflournoy/Rlibs/probly/bib/dissertation.bib"
csl: "/home/jflournoy/Rlibs/probly/bib/apa-old-doi-prefix.csl"
---
  
```{r setup, include = FALSE}
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
echo=F,message=F,warning=F,error=F
)

knitr::read_chunk(system.file('r_aux', 'load_data_for_sim.R', package = 'probly'))
```

```{r}
library(probly)
library(dplyr)
library(tidyr)
library(ggplot2)
library(rstan)
library(bayesplot)

save_out_list <- c()
save_out_dir <- '~/code_new/social-motives-rl-writeup/rda/'
if(!dir.exists(save_out_dir)){
    dir.create(save_out_dir)
}

data_dir <- '/data/jflournoy/split/probly'
logistic_saves_dir <- file.path(data_dir,'/logistic/')
ggplot2::theme_set(ggplot2::theme_minimal())
```

```{r}
extract_splt_model_pars <- function(stanfit){
    delta_names <- grep('^(mu_)*delta', names(stanfit), value = T)
    sigma_names <- grep('(tau|sigma|Omega)', names(stanfit), value = T)
    beta_names <- grep('beta', names(stanfit), value = T)
    pr_names <- grep('pR_final', names(stanfit), value = T)
    thesamples <- rstan::extract(stanfit, pars = c(delta_names, 
                                                   sigma_names, 
                                                   beta_names,
                                                   pr_names))
    log_lik <- loo::extract_log_lik(stanfit, merge_chains = FALSE)
    return(list(parameters = thesamples, log_lik = log_lik))
}
```

```{r load_data_for_sim}
```

# Method

Using the model presented in the section on [testing simulated data](test-simulated-data.html), I estimate the posterior distribution of the model parameters given the data provided by four groups of participants.
Quality of estimation can be supported by (A), (B), (C), (D).

# Results

## Descriptive plots

Examination of the raw data using scatterplots and non-parametric best-fit lines can help insure that data conform to expectations, and that subsequent model estimates accurately reflect the data.
On average, participants in all samples and all conditions demonstrated learning (Figure \@ref(fig:trialaverages)A, although note that given the small sample size, the foster-care involved sample shows more variability across trials).
There is also some indication that learning occurs more quickly in the two social motive conditions (Figure \@ref(fig:trialaverages)B).

(ref:trialaverages) Average number of optimal responses for each trial across all participants. Best fit lines are generalized additive models with 95% confidence intervals. 

```{r trialaverages, fig.width=9, fig.height=5, cache=T, caption='(ref:trialaverages)'}
splt_averages <- splt %>%
    dplyr::group_by(sample, condition, condition_trial_index) %>%
    dplyr::summarize(p_optimal = mean(correcttrue, na.rm = T)) %>%
    dplyr::ungroup()

splt_averages_over_samples <- splt %>%
    dplyr::group_by(condition, condition_trial_index) %>%
    dplyr::summarize(p_optimal = mean(correcttrue, na.rm = T)) %>%
    dplyr::ungroup()

avg_per_sample_plot <- ggplot2::ggplot(
    splt_averages,
    ggplot2::aes(x = condition_trial_index,
                 y = p_optimal,
                 group = condition,
                 linetype = condition,
                 shape = condition)) +
    ggplot2::geom_point(alpha = .1) +
    ggplot2::geom_smooth(
        color = 'black', size = .5,
        method = 'gam', formula = y ~ s(x, bs = "cr", k = 6, fx = T), se = T) +
    ggplot2::facet_wrap(~sample, nrow = 2) +
    ggplot2::labs(x = 'Within-condition trial number',
                  y = 'Proportion of optimal responses',
                  title = 'A') +
    ggplot2::theme_minimal() + 
    ggplot2::theme(legend.position = 'none') + 
    ggplot2::coord_cartesian(ylim = c(.5, .9))

avg_across_samples <- ggplot2::ggplot(
    splt_averages_over_samples,
    ggplot2::aes(x = condition_trial_index,
                 y = p_optimal,
                 group = condition,
                 linetype = condition,
                 shape = condition)) +
    ggplot2::geom_point(alpha = .1) +
    ggplot2::geom_smooth(
        color = 'black', size = .5,
        method = 'gam', formula = y ~ s(x, bs = "cr", k = 6, fx = T), se = T) +
    ggplot2::labs(x = 'Within-condition trial number',
                  y = 'Proportion of optimal responses',
                  title = 'B') +
    ggplot2::guides(linetype = guide_legend(override.aes=list(fill=NA)),
                    shape = guide_legend(override.aes=list(alpha=.5))) + 
    ggplot2::theme_minimal() + 
    ggplot2::theme(legend.key = element_rect(fill = 'white', color = 'white'),
                   legend.background = element_rect(fill = 'white', color = 'white')) + 
    ggplot2::coord_cartesian(ylim = c(.5, .8))

task_descriptive_figure <- gridExtra::grid.arrange(avg_per_sample_plot, avg_across_samples, nrow = 1)

gridExtra::grid.arrange(task_descriptive_figure)
save_out_list <- c(save_out_list, 'task_descriptive_figure')
```

Participants also reported their confidence about which reponses would give them rewards most often (this single item measured confidence globally).
Confidence generally increases aross trials for all samples (Figure \@ref(fig:confidenceplot)).

(ref:confidenceplot) Self-reported confidence over blocks.

```{r confidenceplot, fig.width=6, fig.height=4, cache=T, fig.cap='(ref:confidenceplot)'}
confidence_description_plot <- ggplot2::ggplot(
    dplyr::filter(splt_confidence, !is.na(sample), sample != 'TDS3'),
    ggplot2::aes(x = block, y = confidence)) +
    ggplot2::geom_violin(ggplot2::aes(group = block), scale = 'area', bw = .4, color = '#666666',alpha = 0,
                         position = position_identity())+
    ggplot2::geom_smooth(
        ggplot2::aes(color = sample),
        method = 'gam', formula = y ~ s(x, bs = "cs", k = 8),
        se = T,
        alpha = .1, size = .5) +
    ggplot2::scale_color_manual(values = ggsci::pal_rickandmorty()(8)[c(2:3,7:8)]) + 
    ggplot2::theme_minimal() +
    ggplot2::guides(color = guide_legend(override.aes=list(fill=NA))) + 
    ggplot2::labs(x = 'Block number',
                  y = 'Self-report confidence about optimal label',
                  color = 'Sample')
confidence_description_plot
save_out_list <- c(save_out_list, 'confidence_description_plot')
```

### Proportion optimal presses per ID & condition

Each of the plots below shows, for each individual, the trajectory of learning across trials.
Individual plots are sorted by the overall proportion of optimal presses.

```{r cache=T}
plot_opt_presses_per_id <- function(splt_df,
                                    ncol = 5,
                                    title = 'Proportion optimal presses per ID & condition',
                                    se = F){
    splt_opt_press <- dplyr::mutate(
        ungroup(splt_df),
        press_opt = as.numeric(pressed_r == (proportion == '20_80')),
        press_opt = (press_opt - .5) * (1 -.05 * (as.numeric(condition) - 1)) + .5)
    
    splt_opt_press_ranks <- dplyr::summarize(
        dplyr::group_by(splt_opt_press, id),
        mean_press_opt = mean(press_opt, na.rm = T))
    splt_opt_press_ranks$id_fac <- factor(
        splt_opt_press_ranks$id,
        levels = splt_opt_press_ranks$id[order(splt_opt_press_ranks$mean_press_opt)])
    
    splt_opt_press <- left_join(splt_opt_press, splt_opt_press_ranks)
    
    ggplot2::ggplot(
        splt_opt_press,
        ggplot2::aes(
            x = condition_trial_index,
            y = press_opt,
            linetype = condition,
            shape = condition)) +
        ggplot2::geom_point(size = .75, alpha = .125) +
        ggplot2::geom_hline(yintercept = c(.8), alpha = 1, color = 'gray', size = .5) +
        ggplot2::geom_hline(yintercept = c(.5), alpha = 1, color = 'red', size = .5) +
        ggplot2::geom_smooth(se = se, method = 'gam', formula = y ~ s(x, k = 3, fx = T),
                             size = .5, color = 'black', alpha = .2) +
        ggplot2::facet_wrap(~ id_fac, ncol = ncol) +
        ggplot2::coord_cartesian(ylim = c(0, 1)) +
        ggplot2::scale_y_continuous(breaks = c(0,.5,.8)) +
        ggplot2::scale_x_continuous(breaks = c(1,64,128)) +
        ggplot2::theme_minimal() +
        ggplot2::labs(x = 'Trial', y = 'Optimal press (probability)', title = title)
}

ggcorplot <- function(adf, use = 'pairwise.complete.obs', method = 'pearson'){
    adf_cor <- cor(adf, use = use, method = method)
    adf_cor[lower.tri(adf_cor)] <- NA
    diag(adf_cor) <- NA
    adf_cor_df <- as.data.frame(adf_cor)
    adf_cor_df$row <- factor(
        dimnames(adf_cor)[[1]],
        levels = dimnames(adf_cor)[[1]])
    adf_cor_df_l <- dplyr::filter(
        dplyr::mutate(
            tidyr::gather(adf_cor_df, 
                          col, value, -row),
            col = factor(col, levels = unique(col)[n():1])),
        row != 'bin_1_to_end',
        col != 'bin_1_to_end')
    
    ggplot2::ggplot(adf_cor_df_l,
                    ggplot2::aes(x = row, y = col, fill = value)) + 
        ggplot2::geom_bin2d(stat = 'identity') + 
        ggplot2::scale_fill_gradientn(na.value = 'white', 
                                      colors = c('white', '#eeeeee', 'black', 'red', 'red'),
                                      values = c(.5, .9,.901, 1), limits = c(0,1),
                                      breaks = c(0,.5,1)) + 
        ggplot2::geom_text(ggplot2::aes(label = row),
                           data = dplyr::filter(adf_cor_df_l,
                                                row == col),
                           size = 3, 
                           hjust = 'left') + 
        ggplot2::coord_cartesian(xlim = c(1, dim(adf_cor)[[2]] + 4)) + 
        ggplot2::theme(axis.text = ggplot2::element_blank(),
                       axis.line = ggplot2::element_blank(),
                       axis.ticks = ggplot2::element_blank(),
                       panel.grid = ggplot2::element_blank()) + 
        ggplot2::labs(x='',y='',fill = 'Correlation')
}
```

```{r proportion_trials_per_conditionca, fig.width=10, fig.height=22, cache=F, eval=F}
plot_opt_presses_per_id(
    dplyr::filter(splt, sample == 'Community adolescents'),
    ncol = 5,
    title = 'Community adolescents')
```

```{r proportion_trials_per_conditionfc, fig.width=10, fig.height=10, cache=F, eval=F}
plot_opt_presses_per_id(
    dplyr::filter(splt, sample == 'Foster-care involved adolescents'),
    ncol = 5,
    title = 'Foster-care involved adolescents')
```

```{r proportion_trials_per_conditioncs, fig.width=10, fig.height=22, cache=F, eval=F}
plot_opt_presses_per_id(
    dplyr::filter(splt, sample == 'College students'),
    ncol = 5,
    title = 'College students')
```

```{r proportion_trials_per_conditioncso, fig.width=10, fig.height=44, cache=F, eval=F}
plot_opt_presses_per_id(
    dplyr::filter(splt, sample == 'College students - online'),
    ncol = 5,
    title = 'College students - online')
```

# Low-Fi

What's the least sophisticated way I could possibly examine these data? Probably by taking the mean of some subset of the trials in each condition. So let's do that and see how much stability there is across bins. since there are `r max(splt$condition_trial_index)` trials per condition, I split this into bins of 16 trials each, for 8 bins total. I will also create overlapping double bins (e.g., bins 1 and 2, bins 2 and 3) increaslingly large bins starting from the beginning (e.g., combine bins 1 and 2, then bins 1-3), as well as starting from the end (bins 7 and 8, then bins 6-8). In this way, I hope to build up a picture for how much a summary of one subset of bins captures about the rest of the bins. I suspect that bins toward the end of the run will capture more information about the rest of the run than bins at the beginning of the run, as would be consistent with learning throughout the task.

```{r fig.width=7, fig.height=4}
max_trials_per_condition <- max(splt$condition_trial_index)
seq_trial_bins <- lapply(seq(1, max(splt$condition_trial_index), 16), function(i) i:(i+15))
names(seq_trial_bins) <- paste0('bin_', 1:8)
doubled_seq_trial_bins <- lapply(seq_along(seq_trial_bins)[-1], function(i){
    c(seq_trial_bins[[i-1]], seq_trial_bins[[i]])
})
names(doubled_seq_trial_bins) <- paste0('bin_', 1:7, ',', 2:8)
growing_bins <- lapply(1:8, function(i) 1:(i*16))
names(growing_bins) <- paste0('bin_start_to_', 1:8)
shrinking_bins <- lapply(1:8, function(i) ((i-1)*16 + 1):128)
names(shrinking_bins) <- paste0('bin_', 1:8, '_to_end')

all_bins <- c(seq_trial_bins, doubled_seq_trial_bins, growing_bins, shrinking_bins)

trial_bin_opt_means <- splt %>% 
    group_by(id, condition) %>%
    do({
        adf <- .
        some_means <- lapply(all_bins, function(bindx){
            mean(adf$press_opt[adf$condition_trial_index %in% bindx], na.rm = T)
        })
        names(some_means) <- names(all_bins)
        as.data.frame(some_means)
    })

trial_opt_reg_coef <- splt %>% 
    mutate(slope_to_end = condition_trial_index - 128) %>%
    group_by(id, condition) %>%
    do({
        adf <- .
        alm <- glm(press_opt ~ 1 + slope_to_end, 
                   family = 'binomial',
                   data = adf)
        coef_df <- as.data.frame(t(coef(alm)))
        names(coef_df) <- paste0('end_', names(coef_df))
        coef_df
    })

time_cond_m <- readRDS(file.path(logistic_saves_dir,'/timeXcondition_smpstim_lm.rds'))

time_cond_pred_df <- distinct(splt, condition, id, sample)
time_cond_pred_df$trial_index_c0_s <- 0
time_cond_pred_df$p_opt <- predict(time_cond_m, newdata = time_cond_pred_df, 
                                   re.form = ~(1 + condition * trial_index_c0_s | sample:id),
                                   type = 'response')
time_cond_rx <- coef(time_cond_m)$`sample:id`
time_cond_rx_df <- as.data.frame(time_cond_rx) %>%
    mutate(id = as.numeric(sub('.*:', '', dimnames(time_cond_rx)[[1]]))) %>%
    dplyr::select(trial_index_c0_s:`id`) %>%
    mutate_at(vars(2:3), funs(. + trial_index_c0_s)) %>%
    gather(condition, slope, -id) %>%
    mutate(condition = sub('condition(.*):trial_index_c0_s', '\\1', condition),
           condition = ifelse(condition == 'trial_index_c0_s',
                              'Hungry/Thirsty',
                              condition))


lowfi_df <- dplyr::full_join(
    dplyr::full_join(
        dplyr::full_join(
            trial_bin_opt_means, 
            trial_opt_reg_coef, 
            by = c('id', 'condition')),
        time_cond_pred_df[,c('id','condition','p_opt')],
        by = c('id', 'condition')),
    time_cond_rx_df,
    by = c('id', 'condition'))

lowfi_df_v_bl <- lowfi_df %>%
    group_by(id) %>%
    mutate_at(vars(bin_1:slope),
              funs(. - .[condition == 'Hungry/Thirsty'])) %>%
    mutate(condition = paste0(condition, ' - Hungry/Thirsty'))

lowfi_df_l <- lowfi_df %>%
    gather(outcome, value, -id, -condition)
lowfi_df_v_bl_l <- lowfi_df_v_bl %>%
    gather(outcome, value, -id, -condition)
splt_lowfi_outcomes <- bind_rows(lowfi_df_l,lowfi_df_v_bl_l)

ggcorplot(lowfi_df[lowfi_df$condition == 'Hungry/Thirsty', c(3:dim(lowfi_df)[2])], method = 'spearman')
ggcorplot(lowfi_df[lowfi_df$condition == 'Dating/Looking', c(3:dim(lowfi_df)[2])], method = 'spearman')
ggcorplot(lowfi_df[lowfi_df$condition == 'Popular/Unpopular', c(3:dim(lowfi_df)[2])], method = 'spearman')
```

```{r echo=F, eval=F}
devtools::use_data(splt_lowfi_outcomes)
```

Examining the above plots where corelations r > .90 are colored red (and r = .9 color black), it appears that the most wide ranging correlations are from bins that combine information during the latter half of the run in various ways. If one had to choose such a summary in these data, the average number of optimal presses from bin 5 through 8 (from trial `r (5-1)*16 + 1` through trial 128) seems like a good candidate. It is highly correlated with average optimal responding from bin 6-end and 7-end, combined bins 5 & 6, combined bins 6 & 7, as well as the combined bins from start to finish ("bin_start_to_8"). Using the intercept from a logistic regression with the trial number predictor centered toward the end of the trial also captures a good deal of variance that corresponds to optimal responding on trials toward the end of the run.

## Age and dev

Here are a few plots of these parameters against age.

```{r}
splt_fsmi$partnered <- as.numeric(splt_fsmi$fsmi_relstatus == relationship_status['long_term'])

splt_fsmi_matestat_scored <- as.data.frame(
    psych::scoreItems(fsmi_key[c('fsmi_mate', 'fsmi_stat')], 
                      splt_fsmi, min = 1, max = 7,
                      missing = TRUE, impute = 'none')$scores)

splt_fsmi_matestat_scored$SID <- splt_fsmi$SID
splt_fsmi_matestat_scored$partnered <- splt_fsmi$partnered

splt_fsmi_ksrq_scored <- as.data.frame(
    psych::scoreItems(ksrq_key[c('k_srq_admiration', 'k_srq_passivity', 
                                 'k_srq_sexual_relationships', 'k_srq_sociability')], 
                      splt_fsmi, min = 1, max = 7,
                      missing = TRUE, impute = 'none')$scores)

splt_fsmi_ksrq_scored$SID <- splt_fsmi$SID

splt_fsmi_upps_scored <- as.data.frame(
    psych::scoreItems(upps_key[c('sensation_seeking','premeditation')], 
                      splt_fsmi, min = 1, max = 4,
                      missing = TRUE, impute = 'none')$scores)

splt_fsmi_upps_scored$SID <- splt_fsmi$SID

lowfi_df_dev <- dplyr::left_join(
    dplyr::left_join(lowfi_df,
                     unique(splt_dev_and_demog), by = c('id' = 'SID')),
    dplyr::left_join(splt_fsmi_matestat_scored, 
                     dplyr::left_join(splt_fsmi_ksrq_scored,
                                      splt_fsmi_upps_scored)),
    by = c('id' = 'SID')) %>%
    mutate(PDS_mean_score = as.numeric(PDS_mean_score)) %>%
    select(-hispanic_yn, -ethnicity)

lowfi_df_dev_qstnr_l <- dplyr::select(
    lowfi_df_dev,
    id, condition, sample, partnered,
    bin_5_to_end, dplyr::contains('fsmi'), dplyr::contains('k_srq'), sensation_seeking, premeditation) %>%
    tidyr::gather(scalename, value, 
                  dplyr::contains('fsmi'), dplyr::contains('k_srq'), sensation_seeking, premeditation)

lowfi_df_dev_v_bl <- lowfi_df_dev %>%
    group_by(id) %>%
    mutate_at(vars(bin_1:slope),
              funs(. - .[condition == 'Hungry/Thirsty']))

lowfi_df_dev_v_bl_qstnr_l <- dplyr::select(
    lowfi_df_dev_v_bl,
    id, condition, sample, partnered,
    bin_5_to_end, dplyr::contains('fsmi'), dplyr::contains('k_srq'), sensation_seeking, premeditation) %>%
    tidyr::gather(scalename, value, 
                  dplyr::contains('fsmi'), dplyr::contains('k_srq'), sensation_seeking, premeditation)

apal <- c("#b71c00",
          "#e44afc",
          "#2f9100",
          "#160055",
          "#ff578f",
          "#015eaa")
sample_colors <- apal[3:4]
gender_colors <- c('#1f78b4', '#33a02c')

ggplot2::theme_set(ggplot2::theme_minimal())
legend_width <- .75
linetype_scale_diffs <- ggplot2::scale_linetype_manual(breaks = sort(condition_labels)[c(1,3)], 
                                                       values = c('longdash', 'dashed'))
linetype_scale <- ggplot2::scale_linetype_manual(breaks = sort(condition_labels)[1:3], 
                                                 values = c('longdash', 'solid', 'dashed')) 


p_opt_diff_age_plot <- ggplot2::ggplot(
    dplyr::mutate(
        dplyr::filter(lowfi_df_dev_v_bl, condition != 'Hungry/Thirsty'),
        agegroup = ifelse(grepl('yads', sample), 'College students', 'Adolescents')),
    ggplot2::aes(x = age, y = bin_5_to_end, 
                 linetype = condition,
                 color = agegroup,
                 group = condition)) + 
    ggplot2::geom_point(alpha = .5, position = position_jitter(width = .15, height = 0)) + 
    ggplot2::geom_smooth(color = 'black',
                         method = 'gam', formula = y ~ s(x, fx = T, k = 3),
                         size = .5, alpha =  .2) + 
    ggplot2::labs(x = 'Age', y = 'Probability of making optimal choice\nversus Hungry/Thirsty baseline',
                  linetype = 'Condition') +
    ggplot2::scale_color_manual(name = 'Age group', 
                                values = sample_colors) +
    linetype_scale_diffs + 
    ggplot2::guides(linetype = guide_legend(override.aes = list(color = 'black'))) + 
    ggplot2::theme(legend.key.width = unit(legend_width, units = 'cm'))

p_opt_early_diff_age_plot <- ggplot2::ggplot(
    dplyr::mutate(
        dplyr::filter(lowfi_df_dev_v_bl, condition != 'Hungry/Thirsty'),
        agegroup = ifelse(grepl('yads', sample), 'College students', 'Adolescents')),
    ggplot2::aes(x = age, y = bin_start_to_4, 
                 linetype = condition,
                 color = agegroup,
                 group = condition)) + 
    ggplot2::geom_point(alpha = .5, position = position_jitter(width = .15, height = 0)) + 
    ggplot2::geom_smooth(color = 'black',
                         method = 'gam', formula = y ~ s(x, fx = T, k = 3),
                         size = .5, alpha =  .2) + 
    ggplot2::labs(x = 'Age', y = 'Probability of making optimal choice\nversus Hungry/Thirsty baseline',
                  linetype = 'Condition') +
    ggplot2::scale_color_manual(name = 'Age group', 
                                values = sample_colors) +
    linetype_scale_diffs + 
    ggplot2::guides(linetype = guide_legend(override.aes = list(color = 'black'))) + 
    ggplot2::theme(legend.key.width = unit(legend_width, units = 'cm'))

p_opt_diff_pds_plot <- ggplot2::ggplot(
    dplyr::filter(lowfi_df_dev_v_bl, condition != 'Hungry/Thirsty'),
    ggplot2::aes(x = PDS_mean_score, y = bin_5_to_end, 
                 linetype = condition, 
                 color = factor(gender, levels = c(0,1), labels = c('Male', 'Female')))) + 
    ggplot2::geom_point(position = position_jitter(), alpha = .5) + 
    ggplot2::geom_smooth(color = '#333333',
                         method = 'gam', formula = y ~ s(x, fx = T, k = 3),
                         size = .5, alpha =  .2) + 
    ggplot2::labs(x = 'PDS mean score', 
                  y = 'Probability of making optimal choice\nversus Hungry/Thirsty baseline',
                  linetype = 'Condition') + 
    ggplot2::facet_grid(~factor(gender, levels = c(0,1), labels = c('Male', 'Female')),
                        scales = 'free_x') +
    ggplot2::scale_color_manual(name = 'Gender', breaks = c(0, 1), values = gender_colors,
                                labels = c('Male', 'Female')) +
    linetype_scale_diffs + 
    ggplot2::theme(legend.key.width = unit(legend_width, units = 'cm'))

p_opt_age_plot <- ggplot2::ggplot(
    dplyr::mutate(
        lowfi_df_dev,
        agegroup = ifelse(grepl('yads', sample), 'College students', 'Adolescents')),
    ggplot2::aes(x = age, y = bin_5_to_end,
                 color = agegroup)) + 
    ggplot2::geom_smooth(color = '#999999',
                         method = 'gam', formula = y ~ s(x, fx = T, k = 5),
                         size = 2, alpha = .2) + 
    ggplot2::geom_point(alpha = .5, position = position_jitter(width = .15, height = 0)) + 
    ggplot2::geom_smooth(ggplot2::aes(linetype = condition,
                                      group = condition),
                         color = '#333333',
                         method = 'gam', formula = y ~ s(x, fx = T, k = 5),
                         size = .5, se = F, alpha = .2) + 
    ggplot2::labs(x = 'Age', y = 'Probability of making optimal choice') +
    ggplot2::scale_color_manual(name = 'Age group', 
                                values = sample_colors) +
    linetype_scale +  
    ggplot2::guides(linetype = guide_legend(override.aes = list(color = 'black'))) + 
    ggplot2::theme(legend.key.width = unit(legend_width, units = 'cm'))

p_opt_early_age_plot <- ggplot2::ggplot(
    dplyr::mutate(
        lowfi_df_dev,
        agegroup = ifelse(grepl('yads', sample), 'College students', 'Adolescents')),
    ggplot2::aes(x = age, y = bin_start_to_4,
                 color = agegroup)) + 
    ggplot2::geom_smooth(color = '#999999',
                         method = 'gam', formula = y ~ s(x, fx = T, k = 5),
                         size = 2, alpha = .2) + 
    ggplot2::geom_point(alpha = .5, position = position_jitter(width = .15, height = 0)) + 
    ggplot2::geom_smooth(ggplot2::aes(linetype = condition,
                                      group = condition),
                         color = '#333333',
                         method = 'gam', formula = y ~ s(x, fx = T, k = 5),
                         size = .5, se = F, alpha = .2) + 
    ggplot2::labs(x = 'Age', y = 'Probability of making optimal choice') +
    ggplot2::scale_color_manual(name = 'Age group', 
                                values = sample_colors) +
    linetype_scale +  
    ggplot2::guides(linetype = guide_legend(override.aes = list(color = 'black'))) + 
    ggplot2::theme(legend.key.width = unit(legend_width, units = 'cm'))

p_opt_pds_plot <- ggplot2::ggplot(
    dplyr::mutate(
        lowfi_df_dev,
        agegroup = ifelse(grepl('yads', sample), 'College students', 'Adolescents')),
    ggplot2::aes(x = PDS_mean_score, y = bin_5_to_end,
                 color = factor(gender, levels = c(0,1), labels = c('male', 'female')))) + 
    ggplot2::geom_smooth(color = '#999999',
                         method = 'gam', formula = y ~ s(x, fx = T, k = 3),
                         size = 2, alpha = .2) + 
    ggplot2::geom_point(position = position_jitter(), alpha = .5) + 
    ggplot2::geom_smooth(ggplot2::aes(linetype = condition,
                                      group = condition),
                         color = '#333333',
                         method = 'gam', formula = y ~ s(x, fx = T, k = 3),
                         size = .5, se = F, alpha = .2) + 
    ggplot2::labs(x = 'PDS mean score', 
                  y = 'Probability of making optimal choice',
                  linetype = 'Condition') + 
    ggplot2::facet_grid(~factor(gender, levels = c(0,1), labels = c('Male', 'Female')),
                        scales = 'free_x') +
    ggplot2::scale_color_manual(name = 'Gender', breaks = c(0, 1), values = gender_colors,
                                labels = c('Male', 'Female')) + 
    linetype_scale + 
    ggplot2::theme(legend.key.width = unit(legend_width, units = 'cm'))

p_opt_fsmi_scales <- ggplot2::ggplot(
    dplyr::filter(lowfi_df_dev_qstnr_l, grepl('yads', sample), 
                  grepl('fsmi', scalename), !(partnered == 1 & scalename == 'fsmi_mate')),
    ggplot2::aes(x = value, y = bin_5_to_end, linetype = condition)) + 
    ggplot2::geom_point(alpha = .25, position = position_jitter(w=.2, h=.01)) + 
    ggplot2::geom_smooth(color = 'black',
                         method = 'lm', formula = y ~ x,
                         size = .5, alpha = .2) + 
    ggplot2::labs(x = 'FSMI scale score', y = 'Probability of making optimal choice',
                  linetype = 'Condition') + 
    linetype_scale +
    ggplot2::facet_wrap(~scalename,
                        labeller = labeller(scalename = c('fsmi_affgrp' = 'Group affiliation',
                                                          'fsmi_mate' = 'Mate seeking',
                                                          'fsmi_stat' = 'Status'))) + 
    ggplot2::scale_x_continuous(breaks = c(1, 4, 7)) + 
    ggplot2::theme(legend.key.width = unit(legend_width, units = 'cm'))

p_opt_ksrq_scales <- ggplot2::ggplot(
    dplyr::filter(lowfi_df_dev_qstnr_l,
                  grepl('k_srq', scalename)),
    ggplot2::aes(x = value, y = bin_5_to_end, linetype = condition)) + 
    ggplot2::geom_point(alpha = .075, position = position_jitter(w=.2, h=.01)) + 
    ggplot2::geom_smooth(color = 'black',
                         method = 'lm', formula = y ~ x,
                         size = .5, alpha = .2) + 
    ggplot2::labs(x = 'K-SRQ scale score', y = 'Probability of making optimal choice',
                  linetype = 'Condition') + 
    linetype_scale +
    ggplot2::facet_wrap(~scalename,
                        labeller = labeller(scalename = c('k_srq_admiration' = 'Admiration',
                                                          'k_srq_passivity' = 'Passivity',
                                                          'k_srq_sexual_relationships' = 'Sexual relationships',
                                                          'k_srq_sociability' = 'Sociability'))) + 
    ggplot2::scale_x_continuous(breaks = c(1, 4, 7)) + 
    ggplot2::theme(legend.key.width = unit(legend_width, units = 'cm'))

p_opt_upps_scales <- ggplot2::ggplot(
    dplyr::filter(lowfi_df_dev_qstnr_l,
                  grepl('(sensation_seeking|premeditation)', scalename)),
    ggplot2::aes(x = value, y = bin_5_to_end, linetype = condition)) + 
    ggplot2::geom_point(alpha = .075, position = position_jitter(w=.2, h=.01)) + 
    ggplot2::geom_smooth(color = 'black',
                         method = 'lm', formula = y ~ x,
                         size = .5, alpha = .2) + 
    ggplot2::labs(x = 'K-SRQ scale score', y = 'Probability of making optimal choice',
                  linetype = 'Condition') + 
    linetype_scale +
    ggplot2::facet_wrap(~scalename,
                        labeller = labeller(scalename = c('sensation_seeking' = 'Sensation Seeking',
                                                          'premeditation' = 'Premeditation'))) + 
    ggplot2::scale_x_continuous(breaks = c(1, 4, 7)) + 
    ggplot2::theme(legend.key.width = unit(legend_width, units = 'cm'))

p_opt_diff_fsmi_scales <- ggplot2::ggplot(
    dplyr::filter(lowfi_df_dev_v_bl_qstnr_l, grepl('yads', sample), 
                  condition != 'Hungry/Thirsty',
                  grepl('fsmi', scalename), !(partnered == 1 & scalename == 'fsmi_mate')),
    ggplot2::aes(x = value, y = bin_5_to_end, linetype = condition)) + 
    ggplot2::geom_point(alpha = .25, position = position_jitter(w=.2, h=.01)) + 
    ggplot2::geom_smooth(color = 'black',
                         method = 'lm', formula = y ~ x,
                         size = .5, alpha = .2) + 
    ggplot2::labs(x = 'FSMI scale score', y = 'Probability of making optimal choice\nversus Hungry/Thirsty baseline',
                  linetype = 'Condition') + 
    linetype_scale_diffs +
    ggplot2::facet_wrap(~scalename,
                        labeller = labeller(scalename = c('fsmi_affgrp' = 'Group affiliation',
                                                          'fsmi_mate' = 'Mate seeking',
                                                          'fsmi_stat' = 'Status'))) + 
    ggplot2::scale_x_continuous(breaks = c(1, 4, 7)) + 
    ggplot2::theme(legend.key.width = unit(legend_width, units = 'cm'))

p_opt_diff_ksrq_scales <- ggplot2::ggplot(
    dplyr::filter(lowfi_df_dev_v_bl_qstnr_l,
                  condition != 'Hungry/Thirsty',
                  grepl('k_srq', scalename)),
    ggplot2::aes(x = value, y = bin_5_to_end, linetype = condition)) + 
    ggplot2::geom_point(alpha = .075, position = position_jitter(w=.2, h=.01)) + 
    ggplot2::geom_smooth(color = 'black',
                         method = 'lm', formula = y ~ x,
                         size = .5, alpha = .2) + 
    ggplot2::labs(x = 'K-SRQ scale score', y = 'Probability of making optimal choice\nversus Hungry/Thirsty baseline',
                  linetype = 'Condition') + 
    linetype_scale_diffs +
    ggplot2::facet_wrap(~scalename,
                        labeller = labeller(scalename = c('k_srq_admiration' = 'Admiration',
                                                          'k_srq_passivity' = 'Passivity',
                                                          'k_srq_sexual_relationships' = 'Sexual relationships',
                                                          'k_srq_sociability' = 'Sociability'))) + 
    ggplot2::scale_x_continuous(breaks = c(1, 4, 7)) + 
    ggplot2::theme(legend.key.width = unit(legend_width, units = 'cm'))

p_opt_diff_upps_scales <- ggplot2::ggplot(
    dplyr::filter(lowfi_df_dev_v_bl_qstnr_l,
                  condition != 'Hungry/Thirsty',
                  grepl('(sensation_seeking|premeditation)', scalename)),
    ggplot2::aes(x = value, y = bin_5_to_end, linetype = condition)) + 
    ggplot2::geom_point(alpha = .075, position = position_jitter(w=.2, h=.01)) + 
    ggplot2::geom_smooth(color = 'black',
                         method = 'lm', formula = y ~ x,
                         size = .5, alpha = .2) + 
    ggplot2::labs(x = 'K-SRQ scale score', y = 'Probability of making optimal choice',
                  linetype = 'Condition') + 
    linetype_scale +
    ggplot2::facet_wrap(~scalename,
                        labeller = labeller(scalename = c('sensation_seeking' = 'Sensation Seeking',
                                                          'premeditation' = 'Premeditation'))) + 
    ggplot2::scale_x_continuous(breaks = c(1, 4, 7)) + 
    ggplot2::theme(legend.key.width = unit(legend_width, units = 'cm'))
```

```{r fig.width=5.875, fig.height=4}
p_opt_age_plot
p_opt_pds_plot
p_opt_diff_age_plot
p_opt_diff_pds_plot
p_opt_fsmi_scales
p_opt_ksrq_scales
p_opt_upps_scales
p_opt_diff_fsmi_scales
p_opt_diff_ksrq_scales
p_opt_diff_upps_scales
```

```{r}
adf_adol_con <- dplyr::filter(lowfi_df_dev_v_bl, condition != 'Hungry/Thirsty',
                              !grepl('yads', sample))
adf_adol <- dplyr::filter(lowfi_df_dev, 
                          !grepl('yads', sample))
adf_col_con <- dplyr::filter(lowfi_df_dev_v_bl, condition != 'Hungry/Thirsty',
                              grepl('yads', sample))
adf_col <- dplyr::filter(lowfi_df_dev, 
                         grepl('yads', sample))

stdCoef.merMod <- function(object) {
  sdy <- sd(lme4::getME(object,"y"))
  sdx <- apply(lme4::getME(object,"X"), 2, sd)
  sc <- lme4::fixef(object)*sdx/sdy
  se.fixef <- coef(summary(object))[,"Std. Error"]
  se <- se.fixef*sdx/sdy
  return(data.frame(stdcoef=sc, stdse=se, stdt = sc / se))
}

mlm_std_beta_table <- function(outcomelist, xstring, adf_adol, adf_col, latex_par = T){
    table_rows <- lapply(outcomelist, function(ystring){
        aform <- as.formula(paste0(ystring, ' ~ ', xstring, ' + (1 | id)'))
        adolmod <- lme4::lmer(aform, data = adf_adol, REML = F)
        colmod <- lme4::lmer(aform, data = adf_col, REML = F)
        std_adol_xvar <- stdCoef.merMod(adolmod)[xstring,]
        std_col_xvar <- stdCoef.merMod(colmod)[xstring,]
        std_df <- dplyr::bind_rows('Adolescent' = std_adol_xvar, 
                                   'College' = std_col_xvar,
                                   .id = 'Sample')
        return(std_df)
    })
    names(table_rows) <- outcomelist
    table_full <- dplyr::bind_rows(table_rows, .id = 'Parameter')
    return(table_full)
}

outcomelist <- list('bin_5_to_end')
age_table <- mlm_std_beta_table(outcomelist, xstring = 'age', adf_adol = adf_adol, adf_col = adf_col)
pds_table <- mlm_std_beta_table(outcomelist, xstring = 'PDS_mean_score', adf_adol = adf_adol, adf_col = adf_col)
age_con_table <- mlm_std_beta_table(outcomelist, xstring = 'age', 
                                adf_adol = adf_adol_con, adf_col = adf_col_con)
pds_con_table <- mlm_std_beta_table(outcomelist, xstring = 'PDS_mean_score', 
                                adf_adol = adf_adol_con, adf_col = adf_col_con)

lowfi_regression_table <- dplyr::bind_rows(
    'Optimal choices $\\sim$ Age' = age_table,
    'Optimal choices $\\sim$ PDS' = pds_table,
    'Optimal choices contrasts $\\sim$ Age' = age_con_table,
    'Optimal choices contrasts $\\sim$ PDS' = pds_con_table,
    .id = 'Variables'
)

```

```{r}
knitr::kable(
    dplyr::select(lowfi_regression_table, -Parameter),
    col.names = c('Variables', 'Age-group', '$\\beta$', '$\\text{SE}_{\\beta}$', '$t$'),
    digits = 2)
```


### Dating/Looking

#### Diff

```{r}
ggcorplot(
    dplyr::select(
        dplyr::ungroup(
            dplyr::filter(lowfi_df_dev_v_bl,
                          condition == 'Dating/Looking')), 
        bin_5_to_end, age:premeditation), method = 'kendall') +
    ggplot2::scale_fill_gradient2(limits = c(-1,1), na.value = 'white')

nada <- lapply(psych::corr.test(dplyr::select(
    dplyr::ungroup(
        dplyr::filter(lowfi_df_dev_v_bl,
                      condition == 'Dating/Looking')), 
    bin_5_to_end, age:premeditation),
    use = 'pairwise.complete.obs', method = 'kendall')[c('r','p')], 
    function(x) print(knitr::kable(x, digits = 3)))
```

#### Raw

```{r}
ggcorplot(
    dplyr::select(
        dplyr::ungroup(
            dplyr::filter(lowfi_df_dev,
                          condition == 'Dating/Looking')), 
        bin_5_to_end, age:premeditation), method = 'kendall') +
    ggplot2::scale_fill_gradient2(limits = c(-1,1), na.value = 'white')

nada <- lapply(psych::corr.test(dplyr::select(
    dplyr::ungroup(
        dplyr::filter(lowfi_df_dev,
                      condition == 'Dating/Looking')), 
    bin_5_to_end, age:premeditation),
    use = 'pairwise.complete.obs', method = 'kendall')[c('r','p')], 
    function(x) print(knitr::kable(x, digits = 3)))
```

### Popular/Unpopular

```{r}
ggcorplot(
    dplyr::select(
        dplyr::ungroup(
            dplyr::filter(lowfi_df_dev_v_bl,
                          condition == 'Popular/Unpopular')), 
        bin_5_to_end, age:premeditation), method = 'kendall') +
    ggplot2::scale_fill_gradient2(limits = c(-1,1), na.value = 'white')

nada <- lapply(psych::corr.test(dplyr::select(
    dplyr::ungroup(
        dplyr::filter(lowfi_df_dev_v_bl,
                      condition == 'Dating/Looking')), 
    bin_5_to_end, age:premeditation),
    use = 'pairwise.complete.obs', method = 'kendall')[c('r','p')], 
    function(x) print(knitr::kable(x, digits = 3)))
```

#### Raw

```{r}
ggcorplot(
    dplyr::select(
        dplyr::ungroup(
            dplyr::filter(lowfi_df_dev,
                          condition == 'Popular/Unpopular')), 
        bin_5_to_end, age:premeditation), method = 'kendall') +
    ggplot2::scale_fill_gradient2(limits = c(-1,1), na.value = 'white')

nada <- lapply(psych::corr.test(dplyr::select(
    dplyr::ungroup(
        dplyr::filter(lowfi_df_dev,
                      condition == 'Dating/Looking')), 
    bin_5_to_end, age:premeditation),
    use = 'pairwise.complete.obs', method = 'kendall')[c('r','p')], 
    function(x) print(knitr::kable(x, digits = 3)))
```

## Bayesian H0 test

```{r}
library(brms)

lowfi_df_dev$condition <- factor(lowfi_df_dev$condition, levels = condition_labels)
lowfi_df_dev$bin_5_to_end_suc <- round(lowfi_df_dev$bin_5_to_end * 64, 0)
lowfi_df_dev$k <- 64

lowfi_df_dev$age_c <- lowfi_df_dev$age - 18

get_prior(bin_5_to_end_suc | trials(k) ~ 1 + condition + s(age, k = 3, by = condition) + (1 | id), 
          data = lowfi_df_dev, family = binomial(link = 'logit'))
b_priors <- set_prior('normal(0, 2)', class = 'b')

age_model <- probly::CachedFit(
    {
        brm(bin_5_to_end_suc | trials(k) ~ 1 + condition + s(age_c, k = 4, by = condition) + (1 | id), 
            data = lowfi_df_dev, family = binomial(link = 'logit'),
            chains = 4, cores = 4, warmup = 2000, iter = 3000,
            prior = b_priors, sample_prior = 'yes')
    }, 
    rds_filename = file.path(data_dir, 'brm_avg_opt_age_model.RDS'))

age_model_bf <- probly::CachedFit(
    {
        brm(bin_5_to_end_suc | trials(k) ~ 1 + poly(age_c,2)*condition + (1 | id), 
            data = lowfi_df_dev, family = binomial(link = 'logit'),
            chains = 4, cores = 4, warmup = 2000, iter = 3000,
            prior = b_priors, sample_prior = 'no',
            save_all_pars = TRUE)
    }, 
    rds_filename = file.path(data_dir, 'brm_avg_opt_age_model_bf.RDS'))

fsmi_model_bf <- probly::CachedFit(
    {
        brm(bin_5_to_end_suc | trials(k) ~ 1 + fsmi_mate*condition + fsmi_stat*condition + (1 | id), 
            data = lowfi_df_dev, family = binomial(link = 'logit'),
            chains = 4, cores = 4, warmup = 2000, iter = 3000,
            prior = b_priors, sample_prior = 'no',
            save_all_pars = TRUE)
    }, 
    rds_filename = file.path(data_dir, 'brm_avg_opt_fsmi_model_bf.RDS'))

ksrq_adm_model_bf <- probly::CachedFit(
    {
        brm(bin_5_to_end_suc | trials(k) ~ 1 + 
                k_srq_admiration*condition + (1 | id), 
            data = lowfi_df_dev, family = binomial(link = 'logit'),
            chains = 4, cores = 4, warmup = 2000, iter = 3000,
            prior = b_priors, sample_prior = 'no',
            save_all_pars = TRUE)
    }, 
    rds_filename = file.path(data_dir, 'brm_avg_opt_ksrq_adm_model_bf.RDS'))

ksrq_sr_model_bf <- probly::CachedFit(
    {
        brm(bin_5_to_end_suc | trials(k) ~ 1 + 
                k_srq_sexual_relationships*condition + (1 | id), 
            data = lowfi_df_dev, family = binomial(link = 'logit'),
            chains = 4, cores = 4, warmup = 2000, iter = 3000,
            prior = b_priors, sample_prior = 'no',
            save_all_pars = TRUE)
    }, 
    rds_filename = file.path(data_dir, 'brm_avg_opt_ksrq_sr_model_bf.RDS'))


null_model_bf <- probly::CachedFit(
    {
        brm(bin_5_to_end_suc | trials(k) ~ 1 + condition + (1 | id), 
            data = lowfi_df_dev, family = binomial(link = 'logit'),
            chains = 4, cores = 4, warmup = 2000, iter = 3000,
            prior = b_priors, sample_prior = 'no',
            save_all_pars = TRUE)
    }, 
    rds_filename = file.path(data_dir, 'brm_avg_opt_null_model_bf.RDS'))

summary(age_model)
summary(age_model_bf)

plot(marginal_effects(age_model), ask = F)
plot(marginal_smooths(age_model), ask = F)

plot(marginal_effects(fsmi_model_bf), ask = F)

plot(marginal_effects(age_model_bf), ask = F)
```

```{r}
brms::pp_check(null_model_bf, nsamples = 50)
```

One thing that is obvious from the posterior predictive check of this model is that there is more going on than a simple bernoulli process. The distribution of y values is lumpy and bi-modal in a way that suggests a mixture of probabilities of success in the latter trials. Conditioning on word-pairs, age, and self report questionnairs does not help explain these differences.

# Results from logistic regression

```{r}
#  [1] "id"                    "dir"                   "sample.x"             
#  [4] "correcttrue"           "pressed_r"             "outcome"              
#  [7] "correct_r"             "reward_possible"       "condition"            
# [10] "condition_trial_index" "sex"                   "stim_image"           
# [13] "trial_index"           "block"                 "proportion"           
# [16] "rt"                    "time_elapsed"          "date_time_completed"  
# [19] "opt_is_right"          "press_opt"             "PDS_female_menses"    
# [22] "PDSS_adren"            "PDSS_gender"           "PDSS_gonad"           
# [25] "PDSS_pdss"             "PDS_mean_score"        "gender"               
# [28] "hispanic_yn"           "fsiq2"                 "sample.y"             
# [31] "age"                   "ethnicity" 

check_stim_rx_null_id_only_m <- probly::CachedFit(
    lme4::glmer(press_opt ~ 1 + (1 | id), 
                family = 'binomial',
                data = splt), 
    rds_filename = file.path(logistic_saves_dir, 'check_stim_rx_null_id_only_m.rds'))

check_stim_rx_null_m <- probly::CachedFit(
    lme4::glmer(press_opt ~ 1 + (1 | sample/id), 
                family = 'binomial',
                data = splt), 
    rds_filename = file.path(logistic_saves_dir, 'check_stim_rx_null_m.rds'))

check_stim_rx_null_stim_rx_m <- probly::CachedFit(
    lme4::glmer(press_opt ~ 1 + (1 | sample/id) + (1 | stim_image), 
                family = 'binomial', 
                data = splt), 
    rds_filename = file.path(logistic_saves_dir, 'check_stim_rx_null_stim_rx_m.rds'))

library(future)
plan(multiprocess)
cond_time_simpl_m_f <- future(probly::CachedFit(
    lme4::glmer(press_opt ~ 1 + condition*condition_trial_index_c0_s + (condition*condition_trial_index_c0_s | id), 
                family = 'binomial', 
                data = splt), 
    rds_filename = file.path(logistic_saves_dir, 'cond_time_simpl_m.rds')))
cond_time_simpl_m <- value(cond_time_simpl_m_f)

timeXcondition_vrysmp_m_f <- future(probly::CachedFit(
    {
        aformula <- press_opt ~ 1 + condition*trial_index_c0_s +
            (1 + condition*trial_index_c0_s | sample:id)
        max_aformula <- lme4::lFormula(aformula, data = splt)
        max_numFx <- length(dimnames(max_aformula$X)[[2]])
        max_numRx <- sum(as.numeric(lapply(max_aformula$reTrms$cnms, function(x) {
            l <- length(x)
            (l*(l-1))/2+l
        })))
        max_maxfun <- max(c(10*(max_numFx+max_numRx+1)^2,1e4))
        lme4_control_options <- lme4::glmerControl(optCtrl=list(maxfun=max_maxfun),
                                                   optimizer = c("nloptwrap", "bobyqa"))
        
        afit <- lme4::glmer(aformula, 
                            family = 'binomial', 
                            data = splt)
    }, 
    rds_filename = file.path(logistic_saves_dir, 'timeXcondition_vrysmp_m.rds')))
resolved(timeXcondition_vrysmp_m_f)
timeXcondition_vrysmp_m <- value(timeXcondition_vrysmp_m_f)

summary(check_stim_rx_null_id_only_m)
summary(check_stim_rx_null_m)
summary(check_stim_rx_null_stim_rx_m)
arm::invlogit(lme4::fixef(check_stim_rx_null_id_only_m))
arm::invlogit(lme4::fixef(check_stim_rx_null_m))
arm::invlogit(lme4::fixef(check_stim_rx_null_stim_rx_m))
options(knitr.kable.NA = '')
knitr::kable(anova(check_stim_rx_null_id_only_m, check_stim_rx_null_m, check_stim_rx_null_stim_rx_m), 
             digits = 4,
             caption = 'Check if adding sample and stimulus random effects makes a difference')

null_id_only_lm <- readRDS(file.path(data_dir, 'logistic', 'null_id_only_lm.rds'))
null_lm <- readRDS(file.path(data_dir, 'logistic', 'null_lm.rds'))
time_lm <- readRDS(file.path(data_dir, 'logistic', 'time_lm.rds'))
condition_lm <- readRDS(file.path(data_dir, 'logistic', 'condition_lm.rds'))

knitr::kable(anova(null_lm,
                   time_lm), 
             digits = 4, 
             caption = 'Compare trial model to null')

knitr::kable(anova(null_lm,
                   condition_lm), 
             digits = 4, 
             caption = 'Compare condition model to null')

# null_id_only_m_form

# # null_m_form
# BM: no warnings

# # time_m_form
# BM: no warnings
#
# LM: no warnings

# # condition_m_form
# BM: Warning: There were 4 divergent transitions after warmup. Increasing adapt_delta above 0.99 may help.

# # timeXcondition_m_form
# BM: Warning: There were 3 divergent transitions after warmup. Increasing adapt_delta above 0.99 may help.
#
# LM: Warning in checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
#   unable to evaluate scaled gradient
# Warning in checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
#   Model failed to converge: degenerate  Hessian with 1 negative eigenvalues

# # timeXcondition_age_m_form
# LM: Warning in checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
#   unable to evaluate scaled gradient
# Warning in checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
#   Model failed to converge: degenerate  Hessian with 2 negative eigenvalues
#
# BM: Warning: There were 1 divergent transitions after warmup. Increasing adapt_delta above 0.99 may help. See
# http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
# Warning: There were 1 chains where the estimated Bayesian Fraction of Missing Information was low. See
# http://mc-stan.org/misc/warnings.html#bfmi-low

# # timeXcondition_dev_m_form: 
# LM: Warning in checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
#   unable to evaluate scaled gradient
# Warning in checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
#   Model failed to converge: degenerate  Hessian with 2 negative eigenvalues
#
# BM: no warnings

# # condition_smpstim_m_form
# LM: no warnings
#
# BM: Warning: There were 9 divergent transitions after warmup. Increasing adapt_delta above 0.99 may help

# # timeXcondition_smpstim_m_form
# LM: no warnings
#
# BM: Warning: There were 1 divergent transitions after warmup. Increasing adapt_delta above 0.99 may help

# # timeXcondition_age_smpstim_m_form
# LM: Warning in checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
#       Model failed to converge with max|grad| = 0.00210204 (tol = 0.001, component 1)
#     Writing result to /gpfs/projects/dsnlab/flournoy/data/splt/probly/logistics/timeXcondition_age_smpstim_lm.rds
#
# BM: Warning: There were 1 divergent transitions after warmup. Increasing adapt_delta above 0.99 may help
# JOB KILLED DUE TO TIMELIMIT

# # timeXcondition_dev_smpstim_m_form
# BM: Warning: There were 1 divergent transitions after warmup. Increasing adapt_delta above 0.99 may help.
#
# LM: Warning in checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
#   Model failed to converge with max|grad| = 0.00190347 (tol = 0.001, component 1)

# # timeXcondition_age_samprx_m_form
# LM: Warning in checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
#   Model failed to converge with max|grad| = 0.00240794 (tol = 0.001, component 1)
#
# BM: Warning: There were 8 divergent transitions after warmup. Increasing adapt_delta above 0.99 may help

# # timeXcondition_dev_samprx_m_form
# LM : Warning in checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
#   Model failed to converge with max|grad| = 1.62416 (tol = 0.001, component 1)
#
# BM: Warning: There were 10 divergent transitions after warmup. Increasing adapt_delta above 0.99 may help

# timeXcondition_age_vrysmp_m_form
# timeXcondition_dev_vrysmp_m_form

timeXcondition_age_vrysmp_lm <- readRDS(file.path(logistic_saves_dir, 'timeXcondition_age_vrysmp_lm.rds'))
summary(timeXcondition_age_vrysmp_lm)

timeXcondition_dev_vrysmp_lm <- readRDS(file.path(logistic_saves_dir, 'timeXcondition_dev_vrysmp_lm.rds'))
summary(timeXcondition_dev_vrysmp_lm)
```

## Relation to FSMI subscales

```{r fsmireliability}
scales_for_reliability_stats <- list(
    'fsmi_mate', 'fsmi_stat', 
    'm2', 'm1',
    'k_srq_sexual_relationships', 'k_srq_admiration',
    'dominance_score', 'prestige_score',
    'sensation_seeking')

fsmi_reliability_list <- lapply(
    scales_for_reliability_stats,
    function(scale_name){
        key <- self_report_motive_keys[[scale_name]]
        nitems <- length(key)
        nfactors <- ifelse(nitems > 6, 3, 1)
        omega_stat <- psych::omega(
            splt_fsmi[,gsub('-', '', key)], 
            nfactors = nfactors, 
            key = as.numeric(gsub('(-*).*', '\\11', key)), 
            plot = F)
        omega_df <- data.frame(scale = scale_name,
                               omega_h = ifelse(nitems > 5, round(omega_stat$omega_h,2), '-'), 
                               omega_t = ifelse(nitems > 5, round(omega_stat$omega.tot,2), '-'),
                               alpha = omega_stat$alpha)
        return(omega_df)
    }
)

fsmi_scale_reliability_df <- do.call(rbind, fsmi_reliability_list)
fsmi_scale_reliability_df$scale <- factor(
    fsmi_scale_reliability_df$scale,
    levels = c('fsmi_mate','fsmi_stat',
               'm2','m1',
               'k_srq_sexual_relationships','k_srq_admiration',
               'dominance_score','prestige_score',
               'sensation_seeking'),
    labels = c('FSMI mate-seeking','FSMI status',
               'K-SRQ sexual relationships factor','K-SRQ admiration factor',
               'K-SRQ sexual relationships scale','K-SRQ admiration scale',
               'Dominance','Prestige',
               'Sensation seeking'))

library(tables)
fsmi_scale_reliability_table <- tables::tabular(
    (Scale = scale) ~ Heading()*identity*DropEmpty(empty='-')*Format(digits = 2)*
        ((`$\\omega_h$` = omega_h)*DropEmpty(empty='-') + 
             (`$\\omega_t$` = omega_t)*DropEmpty(empty='-') + 
             (`$\\alpha$` = alpha)), 
    data = fsmi_scale_reliability_df)
```

```{r results = 'asis'}
Hmisc::html(fsmi_scale_reliability_table)

save_out_list <- c(save_out_list, 'fsmi_scale_reliability_table')
```

In addition to confirmatory factor analyses, scale reliability and general-factor validity were assessed via the $\omega_{h}$ and $\omega_t$ statistics. 
These statistics account for the possibility that variance on items is due to both a general latent factor common to all items, and several group fators that account for variance in subsets of items [@zinbarg2006].
The $\omega_{h}$ statistic captures the proportion of variance due to a general factor, and so is proportional to the degree of expected correspondence between the scale and another measure of the construct (on a different scale, or at retest), and also indexes the validity of the scale as a measure of the ostensible latent construct.
The reliability of the whole scale is indexed by $\omega_t$, which includes variance due to both group and general factors.
Cronebach's $\alpha$ is also reported because it is the default practice, and it may be helpful to demonstrate the differences between the two approaches.
All three statistics were calculated using the `omega` function in the `psych` package for R [@revelle2017].

The mate-seeking and status motive scales from the FSMI showed adequate reliability, with $\alpha$ and $\omega_t$ in the acceptable to excellent range according to some rules of thumb (Table ref(tab:fsmi_reliability). 
However, $\omega_h$ indicates that only about 68% of the variance in FSMI Status and 80% of the variance in FSMI mate-seeking are due to the general factor.
In light of this, any correlations of scale responses with other constructs of interest should be interpretted in light of the possibility that they are due, in part, to associations between variance _not_ due to the general factor (and thus _note_ due to the construct of interest).
Of course, the variance due to other latent causes may also be a reason true relations are attentuated.

```{r fsmiregression}
p_opt_fsmi_scales
p_opt_ksrq_scales
p_opt_diff_fsmi_scales
p_opt_diff_ksrq_scales
```


# Results from Bayesian learning models


## Model comparison

```{r}
stanfit_2_level <- readRDS(file.path(data_dir,'splt-looser-rl_2_level-1528266.RDS'))
stanfit_list <- lapply(
    dir('/data/jflournoy/split/probly/', pattern = '*looser-rl*', full.names = T),
    function(fname){
        stanfit <- readRDS(fname)
        pars <- extract_splt_model_pars(stanfit)
        return(pars)
    }
)
names(stanfit_list) <- dir('/data/jflournoy/split/probly/', pattern = '*looser-rl*')

stan_loos <- lapply(
    stanfit_list,
    function(stanfit){
        r_eff <- loo::relative_eff(exp(stanfit$log_lik))
        loo_l <- loo::loo(stanfit$log_lik, r_eff = r_eff, cores = 8)
        return(loo_l)
    }
)
loo_comp <- loo::compare(x = stan_loos)
knitr::kable(loo_comp)
```

If we can trust these LOO estimates (it would be better to do leave-one-observation-out rather than leave-oune-participant-out), it looks like the three level structure is probably unnecessary but that the full parameterization is helpful.

We can examine differences in some of the parameters across models to get a sense for this as well.

```{r}
mean_pars <- lapply(stanfit_list,
                    function(modext){
                        meanpars <- lapply(modext$parameters, mean)
                        names(meanpars) <- names(modext$parameters)
                        return(meanpars)
                    })

beta_pars <- c('epsilon' = 'beta_ep_prm', 'rho' = 'beta_rho_prm', 
               'xi' = 'beta_xi_prm', 'b' = 'beta_b', 'P("right")[paste(t," = T")]' = 'pR_final')
beta_names_list <- lapply(
    beta_pars,
    function(par){
        grep(paste0('^', par, '\\[\\d+,\\d+\\]'),
             names(mean_pars$`splt-looser-rl_2_level-1528266.RDS`),
             value = T)
    })

names(beta_names_list) <- beta_pars

two_three_level_comparison_plots <- lapply(
    beta_pars,
    function(par){
        adf <- dplyr::data_frame(
            'two level' = unlist(mean_pars$`splt-looser-rl_2_level-1528266.RDS`[beta_names_list[[par]]]),
            'three level' = unlist(mean_pars$`splt-looser-rl_repar_exp-1528144.RDS`[beta_names_list[[par]]]))
        ggplot2::ggplot(
            adf,
            ggplot2::aes(x = `two level`, y = `three level`)) + 
            ggplot2::geom_point() + 
            ggplot2::labs(title = parse(text=bquote(.(names(beta_pars)[which(beta_pars == par)]))),
                          x = '', y = '')
    })

two_three_level_comparison_plots[[1]] <- two_three_level_comparison_plots[[1]] + ggplot2::labs(y = 'Three-level')
two_three_level_comparison_plots[[5]] <- two_three_level_comparison_plots[[5]] + ggplot2::labs(x = 'Two-level')
```

```{r twothreecorplot, fig.width=5.875, fig.height=4, include=F}
gridExtra::grid.arrange(grobs = two_three_level_comparison_plots,
                        ncol = 3)
```

We can also examine the correlation of parameters as we drop the terms $b$ and $\rho$.

```{r}
pR_final <- dplyr::data_frame(
    pR_full = unlist(
        mean_pars$`splt-looser-rl_repar_exp-1528144.RDS`[beta_names_list[['pR_final']]]),
    pR_no_b = unlist(
        mean_pars$`splt-looser-rl_repar_exp_no_b-1528233.RDS`[beta_names_list[['pR_final']]]),
    pR_no_rho = unlist(
        mean_pars$`splt-looser-rl_repar_exp_no_b_no_rho-1527735.RDS`[beta_names_list[['pR_final']]])
)

par_means_for_corplot <- 
    dplyr::data_frame(
        'epsilon["max"]' = qnorm(unlist(
            mean_pars$`splt-looser-rl_2_level-1528266.RDS`[beta_names_list[['beta_ep_prm']]])),
        'epsilon["no b"]' = qnorm(unlist(
            mean_pars$`splt-looser-rl_2_level_no_b-1527784.RDS`[beta_names_list[['beta_ep_prm']]])),
        'epsilon["no b,"~rho]' = qnorm(unlist(
            mean_pars$`splt-looser-rl_2_level_no_b_no_rho-1527729.RDS`[beta_names_list[['beta_ep_prm']]])),
        'xi["max"]' = qnorm(unlist(
            mean_pars$`splt-looser-rl_2_level-1528266.RDS`[beta_names_list[['beta_xi_prm']]])),
        'xi["no b"]' = qnorm(unlist(
            mean_pars$`splt-looser-rl_2_level_no_b-1527784.RDS`[beta_names_list[['beta_xi_prm']]])),
        'xi["no b,"~rho]' = qnorm(unlist(
            mean_pars$`splt-looser-rl_2_level_no_b_no_rho-1527729.RDS`[beta_names_list[['beta_xi_prm']]])),
        'rho["max"]' = log(unlist(
            mean_pars$`splt-looser-rl_2_level-1528266.RDS`[beta_names_list[['beta_rho_prm']]])),
        'rho["no b"]' = log(unlist(
            mean_pars$`splt-looser-rl_2_level_no_b-1527784.RDS`[beta_names_list[['beta_rho_prm']]])),
        'b["max"]' = unlist(
            mean_pars$`splt-looser-rl_2_level-1528266.RDS`[beta_names_list[['beta_b']]])
    )

par_mean_cor_plot <- GGally::ggpairs(par_means_for_corplot, 
                                     labeller = ggplot2::label_parsed, 
                                     lower = list(continuous = GGally::wrap('points', alpha = .3)))

psych::corr.test(dplyr::select(par_means_for_corplot, contains('eps')))
psych::corr.test(dplyr::select(par_means_for_corplot, contains('xi')))
psych::corr.test(dplyr::select(par_means_for_corplot, contains('rho[')))
psych::corr.test(dplyr::select(par_means_for_corplot, contains('max')), method = 'spearman')
psych::corr.test(dplyr::select(par_means_for_corplot, contains('"no b"')))
psych::corr.test(dplyr::select(par_means_for_corplot, contains('"no b,"')))

par_mean_cor_plot_max2l <- GGally::ggpairs(
    dplyr::select(par_means_for_corplot, contains('max'))[c(2,3,1,4)], 
    labeller = ggplot2::label_parsed, 
    lower = list(continuous = GGally::wrap('smooth_loess', alpha = .1, color = 'blue', size = .5)),
    upper = list(continuous = GGally::wrap('cor', method = 'spearman')),
    diag = list(continuous = GGally::wrap('barDiag', bins = 25, fill = '#aaaaaa')))

par_mean_cor_plot_nob2l <- GGally::ggpairs(
    dplyr::select(par_means_for_corplot, contains('"no b"'))[c(2,3,1)], 
    labeller = ggplot2::label_parsed, 
    lower = list(continuous = GGally::wrap('smooth_loess', alpha = .1, color = 'blue', size = .5)),
    upper = list(continuous = GGally::wrap('cor', method = 'spearman')),
    diag = list(continuous = GGally::wrap('barDiag', bins = 25, fill = '#aaaaaa')))
par_mean_cor_plot_nobnorho2l <- GGally::ggpairs(
    dplyr::select(par_means_for_corplot, contains('"no b,"'))[c(2,1)], 
    labeller = ggplot2::label_parsed, 
    lower = list(continuous = GGally::wrap('smooth_loess', alpha = .1, color = 'blue', size = .5)),
    upper = list(continuous = GGally::wrap('cor', method = 'spearman')),
    diag = list(continuous = GGally::wrap('barDiag', bins = 25, fill = '#aaaaaa')))
```

```{r fig.width=20,fig.height=20}
print(par_mean_cor_plot)
```

```{r corparplot, fig.width=5.875, fig.height=4, include=F}
print(par_mean_cor_plot_max2l)
print(par_mean_cor_plot_nob2l)
print(par_mean_cor_plot_nobnorho2l)
```


```{r}
#get just betas 
stanfit_2_level_betas <- stanfit_list$`splt-looser-rl_2_level-1528266.RDS`$parameters[unlist(beta_names_list[1:4])]

#get only those for the 1st condition, hungry/thirsty -- we repeat the hungry/thirsty draws 3 times
ht_cond_for_contrast <- 
    stanfit_2_level_betas[c(rep((1+0*3*313):(313+0*3*313),3),
                            rep((1+1*3*313):(313+1*3*313),3),
                            rep((1+2*3*313):(313+2*3*313),3),
                            rep((1+3*3*313):(313+3*3*313),3))]
stanfit_2_level_betas_contrasted <- lapply(seq_along(stanfit_2_level_betas), function(i)
    stanfit_2_level_betas[[i]] - ht_cond_for_contrast[[i]])
names(stanfit_2_level_betas_contrasted) <- names(stanfit_2_level_betas)

stanfit_2_level_betas_df <- do.call(
    rbind,
    lapply(
        c(stanfit_2_level_betas, stanfit_2_level_betas_contrasted),
        function(row) data.frame(mean = mean(row), median = median(row), sd = sd(row),
                                 upper = quantile(row, probs = .975),
                                 lower = quantile(row, probs = .025))))
stanfit_2_level_betas_df$par <- rownames(stanfit_2_level_betas_df)
stanfit_2_level_betas_df$contrasted <- rep(c(F,T), each = length(stanfit_2_level_betas))

splt_rl_betas <- stanfit_2_level_betas_df %>%
    tidyr::extract(par, c('parameter', 'idx', 'condnum'), 'beta_(.*)\\[(\\d+),([1-3])\\]') %>%
    dplyr::left_join(mutate(distinct(splt_orig, id, sample), idx = as.character(1:n()))) %>%
    dplyr::mutate(condition = condition_labels[as.numeric(condnum)],
                  condition = ifelse(contrasted, 
                                     paste0(condition, ' - ', condition_labels[[1]]),
                                     condition))

splt_rl_betas_sum <- splt_rl_betas %>%
    dplyr::group_by(id,parameter,condition) %>%
    dplyr::filter(!contrasted) %>%
    dplyr::summarize(mean_par = mean(mean)) %>%
    dplyr::mutate(mean_par = case_when(parameter == 'ep_prm' ~ qnorm(mean_par),
                                       parameter == 'xi_prm' ~ qnorm(mean_par),
                                       parameter == 'b' ~ mean_par,
                                       parameter == 'rho_prm' ~ log(mean_par)),
                  condition = factor(condition, levels = condition_labels)) %>%
    left_join(filter(splt_confidence, block == 8))
confidence_parameter_plot <- ggplot2::ggplot(splt_rl_betas_sum,
                                             ggplot2::aes(y = confidence, x = mean_par)) + 
    ggplot2::geom_point(position = ggplot2::position_jitter(w=0,h=.33), alpha = .25) + 
    ggplot2::geom_smooth(alpha = .5, method = 'gam', formula = y ~ s(x),
                         color = 'black', size = .5) + 
    ggplot2::facet_grid(condition~parameter, scales = 'free_x') + 
    ggplot2::scale_y_continuous(breaks = c(1,3,5)) + 
    ggplot2::labs(y = 'Participant\'s confidence rating on final block',
                  x = 'Posterior mean parameter')

print(
    psych::corr.p(
        psych::corr.test(
            filter(splt_rl_betas_sum, parameter == 'rho_prm', block == 8)[,c('mean_par', 'confidence')],
            method = 'spearman')$r,
        n = 300, alpha = .005),
    short = F)
print(
    psych::corr.p(
        psych::corr.test(
            filter(splt_rl_betas_sum, parameter == 'xi_prm', block == 8)[,c('mean_par', 'confidence')],
            method = 'spearman')$r,
        n = 300, alpha = .005),
    short = F)
print(
    psych::corr.p(
        psych::corr.test(
            filter(splt_rl_betas_sum, parameter == 'ep_prm', block == 8)[,c('mean_par', 'confidence')],
            method = 'spearman')$r,
        n = 300, alpha = .005),
    short = F)
```

```{r confidencevparameters, fig.width=5.875, fig.height=4.1, include=F}
parameter_lookup <- c(
    ep_prm = 'epsilon',
    rho_prm = 'rho',
    xi_prm = 'xi',
    b = 'b'
)

confidence_parameter_plot +
    ggplot2::facet_grid(condition ~ parameter, scales = 'free_x', 
                        labeller = ggplot2::labeller(
                            parameter = ggplot2::as_labeller(parameter_lookup,
                                                             default = ggplot2::label_parsed))) + 
    ggplot2::theme_minimal()
```
```{r echo = F, eval = F}
devtools::use_data(splt_rl_betas)
```

## Relation of learning model to low-fi

```{r}
mean_pars_model <- mean_pars$`splt-looser-rl_2_level-1528266.RDS`
splt_learning_betas <- data.frame(par = beta_names_list$beta_ep_prm,
                                  ep_mean = unlist(mean_pars_model[beta_names_list$beta_ep_prm]),
                                  rho_mean = unlist(mean_pars_model[beta_names_list$beta_rho_prm]),
                                  xi_mean = unlist(mean_pars_model[beta_names_list$beta_xi_prm]),
                                  b_mean = unlist(mean_pars_model[beta_names_list$beta_b])) %>%
    mutate(condition_num = as.numeric(sub('.*\\[\\d+,(\\d)\\]', '\\1', par)),
           condition = condition_labels[condition_num],
           idx = as.numeric(sub('.*\\[(\\d+),\\d\\]', '\\1', par)),
           rho_mean_bin = (cut2(rho_mean, cuts = quantile(rho_mean, probs = seq(0, 1, length.out = 13)[-c(1,13)]))),
           xi_mean_bin = (cut2(xi_mean, cuts = mean(xi_mean))),
           eprho = ep_mean*rho_mean*10) %>%
    select(-condition_num, -par) %>%
    left_join(mutate(distinct(splt_orig, id), idx = 1:n()))

lowfi_learning_df <- dplyr::left_join(
    lowfi_df_dev, splt_learning_betas,
    by = c('id', 'condition'))
lowfi_learning_df_corplots <- mutate_all(lowfi_learning_df, as.numeric)

ggcorplot(lowfi_learning_df_corplots[
    lowfi_learning_df_corplots$condition == 'Hungry/Thirsty', c(3:dim(lowfi_learning_df_corplots)[2])], 
    method = 'spearman')
ggcorplot(lowfi_learning_df_corplots[
    lowfi_learning_df_corplots$condition == 'Dating/Looking', c(3:dim(lowfi_learning_df_corplots)[2])], 
    method = 'spearman')
ggcorplot(lowfi_learning_df_corplots[
    lowfi_learning_df_corplots$condition == 'Popular/Unpopular', c(3:dim(lowfi_learning_df_corplots)[2])],
    method = 'spearman')

qplot(lowfi_learning_df$ep_mean, qnorm(lowfi_learning_df$p_opt), geom = c('point', 'smooth'),
      method = 'gam', formula = y ~ s(x, fx = T, k = 3))
qplot(lowfi_learning_df$rho_mean, qnorm(lowfi_learning_df$p_opt), geom = c('point', 'smooth'),
      method = 'gam', formula = y ~ s(x, fx = T, k = 3))
qplot(lowfi_learning_df$eprho, qnorm(lowfi_learning_df$p_opt), geom = c('point', 'smooth'),
      method = 'gam', formula = y ~ s(x, fx = T, k = 4))
qplot(lowfi_learning_df$b_mean, qnorm(lowfi_learning_df$p_opt), geom = c('point', 'smooth'),
      method = 'gam', formula = y ~ s(x, fx = T, k = 6))

pr_final_df <- data_frame(pr_final = unlist(mean_pars_model[beta_names_list$pR_final]),
                          par = beta_names_list$pR_final) %>%
    mutate(cue = as.numeric(sub('.*\\[\\d+,(\\d)\\]', '\\1', par)),
           idx = as.numeric(sub('.*\\[(\\d+),\\d\\]', '\\1', par))) %>%
    select(-par) %>%
    left_join(mutate(distinct(splt_orig, id), idx = 1:n()))

modeled_splt_data <- readRDS(file.path(data_dir, 'splt-looser-data--1527693.RDS'))
modeled_splt_data$cue <- as.numeric(as.factor(paste0(modeled_splt_data$condition, '_', modeled_splt_data$sex)))
modeled_splt_data$condition_num <- as.numeric(modeled_splt_data$condition)
modeled_splt_data_cue_df <- arrange(dplyr::distinct(modeled_splt_data, id, cue, condition, proportion),
                                    id, condition, proportion) %>%
    dplyr::left_join(pr_final_df, by = c('cue', 'id')) %>%
    dplyr::mutate(p_opt_final = dplyr::case_when(proportion == '20_80' ~ pr_final,
                                                 proportion == '80_20' ~ 1-pr_final))

modeled_splt_data_cue_df_cond_sum <- modeled_splt_data_cue_df %>%
    dplyr::group_by(id, condition) %>%
    dplyr::summarize(p_opt_final = mean(p_opt_final)) %>%
    dplyr::mutate(condition = condition_labels[condition])

lowfi_learning_df <- left_join(lowfi_learning_df, modeled_splt_data_cue_df_cond_sum, 
                               by = c('id', 'condition'))
qplot(lowfi_learning_df$p_opt_final, lowfi_learning_df$bin_5_to_end)
qplot(lowfi_learning_df$p_opt_final, lowfi_learning_df$p_opt)

qnorm_cap = .0001
allparameter_plot <- ggplot2::ggplot(lowfi_learning_df,
                                     ggplot2::aes(x = qnorm(ep_mean),
                                                  y = qnorm(bin_5_to_end-qnorm_cap),
                                                  group = xi_mean_bin,
                                                  color = xi_mean)) + 
    ggplot2::geom_hline(yintercept = qnorm(.8), alpha = .5) + 
    ggplot2::geom_hline(yintercept = qnorm(.5), alpha = .5, color = 'blue') + 
    ggplot2::geom_point(alpha = .5) + 
    ggplot2::geom_smooth(method = 'gam', formula = y ~ s(x, fx = T, k = 3), 
                         alpha = .2, color = 'black', size = .5,
                         ggplot2::aes(linetype = xi_mean_bin)) + 
    ggplot2::scale_color_gradient(low = 'red', high = 'blue') + 
    ggplot2::facet_wrap(~rho_mean_bin) + 
    # ggplot2::theme(strip.text = element_blank()) + 
    ggplot2::coord_cartesian(ylim = c(qnorm(.40), qnorm(1-qnorm_cap))) + 
    ggplot2::scale_y_continuous(breaks = qnorm(c(.5, .8, .95, 1 - qnorm_cap)), labels = c(.5, .8, .95, 1)) +
    ggplot2::scale_x_continuous(breaks = qnorm(c(.01, .05, .20)), labels = c(.01, .05, .20)) +
    ggplot2::labs(x = expression(paste('Posterior ',bar(epsilon))),
                  y = 'Probability of choosing optimal label',
                  caption = expression(paste('Facet labels are ranges for posterior ', bar(rho)[i])),
                  color = expression(paste(bar(xi)[i])),
                  linetype = expression(paste(bar(xi)[i], ' range')))
allparameter_plot
```



```{r}
lowfi_learning_pdiffs_df <- group_by(lowfi_learning_df, id) %>% 
    mutate(rho_mean_contrast = rho_mean - rho_mean[condition == 'Hungry/Thirsty'],
           ep_mean_contrast = ep_mean - ep_mean[condition == 'Hungry/Thirsty'],
           xi_mean_contrast = xi_mean - xi_mean[condition == 'Hungry/Thirsty'],
           bin5end_diff = bin_5_to_end - bin_5_to_end[condition == 'Hungry/Thirsty'])

lowfi_learning_pdiffs_df$condition <- factor(lowfi_learning_pdiffs_df$condition, levels = condition_labels)

ggplot2::ggplot(filter(lowfi_learning_pdiffs_df, condition != condition_labels[1]),
                ggplot2::aes(y = ep_mean_contrast,
                             x = fsmi_mate)) + 
    ggplot2::geom_point(alpha = .5) + 
    ggplot2::geom_smooth(method = 'lm',
                         alpha = .2, color = 'black', size = .5) + 
    ggplot2::scale_color_gradient(low = 'red', high = 'blue') + 
    ggplot2::facet_wrap(~condition) + 
    # ggplot2::theme(strip.text = element_blank()) + 
    ggplot2::labs(y = expression(paste('Mean posterior ',beta[epsilon])),
                  x = 'FSMI mate seeking')

ggplot2::ggplot(filter(lowfi_learning_pdiffs_df, condition != condition_labels[1]),
                ggplot2::aes(y = ep_mean_contrast,
                             x = fsmi_stat)) + 
    ggplot2::geom_point(alpha = .5) + 
    ggplot2::geom_smooth(method = 'lm',
                         alpha = .2, color = 'black', size = .5) + 
    ggplot2::scale_color_gradient(low = 'red', high = 'blue') + 
    ggplot2::facet_wrap(~condition) + 
    # ggplot2::theme(strip.text = element_blank()) + 
    ggplot2::labs(y = expression(paste('Median posterior ',beta[epsilon])),
                  x = 'FSMI status')

ggplot2::ggplot(filter(lowfi_learning_pdiffs_df, condition != condition_labels[1]),
                ggplot2::aes(y = rho_mean_contrast,
                             x = fsmi_mate)) + 
    ggplot2::geom_point(alpha = .5) + 
    ggplot2::geom_smooth(method = 'lm',
                         alpha = .2, color = 'black', size = .5) + 
    ggplot2::scale_color_gradient(low = 'red', high = 'blue') + 
    ggplot2::facet_wrap(~condition) + 
    # ggplot2::theme(strip.text = element_blank()) + 
    ggplot2::labs(y = expression(paste('Mean posterior ',beta[rho])),
                  x = 'FSMI mate seeking')

ggplot2::ggplot(filter(lowfi_learning_pdiffs_df, condition != condition_labels[1]),
                ggplot2::aes(y = rho_mean_contrast,
                             x = fsmi_stat)) + 
    ggplot2::geom_point(alpha = .5) + 
    ggplot2::geom_smooth(method = 'lm',
                         alpha = .2, color = 'black', size = .5) + 
    ggplot2::scale_color_gradient(low = 'red', high = 'blue') + 
    ggplot2::facet_wrap(~condition) + 
    # ggplot2::theme(strip.text = element_blank()) + 
    ggplot2::labs(y = expression(paste('Median posterior ',beta[rho])),
                  x = 'FSMI status')

ggplot2::ggplot(filter(lowfi_learning_pdiffs_df, condition != condition_labels[1]),
                ggplot2::aes(y = xi_mean_contrast,
                             x = fsmi_mate)) + 
    ggplot2::geom_point(alpha = .5) + 
    ggplot2::geom_smooth(method = 'lm',
                         alpha = .2, color = 'black', size = .5) + 
    ggplot2::scale_color_gradient(low = 'red', high = 'blue') + 
    ggplot2::facet_wrap(~condition) + 
    # ggplot2::theme(strip.text = element_blank()) + 
    ggplot2::labs(y = expression(paste('Mean posterior ',beta[xi])),
                  x = 'FSMI mate seeking')

ggplot2::ggplot(filter(lowfi_learning_pdiffs_df, condition != condition_labels[1]),
                ggplot2::aes(y = xi_mean_contrast,
                             x = fsmi_stat)) + 
    ggplot2::geom_point(alpha = .5) + 
    ggplot2::geom_smooth(method = 'lm',
                         alpha = .2, color = 'black', size = .5) + 
    ggplot2::scale_color_gradient(low = 'red', high = 'blue') + 
    ggplot2::facet_wrap(~condition) + 
    # ggplot2::theme(strip.text = element_blank()) + 
    ggplot2::labs(y = expression(paste('Median posterior ',beta[xi])),
                  x = 'FSMI status')

ggplot2::ggplot(filter(lowfi_learning_pdiffs_df, condition != condition_labels[1]),
                ggplot2::aes(y = ep_mean_contrast,
                             x = age)) + 
    ggplot2::geom_point(alpha = .5) + 
    ggplot2::geom_smooth(method = 'gam',
                         alpha = .2, color = 'black', size = .5) + 
    ggplot2::scale_color_gradient(low = 'red', high = 'blue') + 
    ggplot2::facet_wrap(~condition) + 
    # ggplot2::theme(strip.text = element_blank()) + 
    ggplot2::labs(y = expression(paste('Median posterior ',beta[xi])),
                  x = 'FSMI status')
```

```{r}

fsmims_ep_test_tau_ms <- cor.test(
    lowfi_learning_pdiffs_df$ep_mean_contrast[lowfi_learning_pdiffs_df$condition == 'Dating/Looking'],
    lowfi_learning_pdiffs_df$fsmi_mate[lowfi_learning_pdiffs_df$condition == 'Dating/Looking'],
    method = 'kendall',
    exact = F)

fsmist_ep_test_tau_st <- cor.test(
    lowfi_learning_pdiffs_df$ep_mean_contrast[lowfi_learning_pdiffs_df$condition == 'Popular/Unpopular'],
    lowfi_learning_pdiffs_df$fsmi_stat[lowfi_learning_pdiffs_df$condition == 'Popular/Unpopular'],
    method = 'kendall',
    exact = F)

fsmims_rho_test_tau_ms <- cor.test(
    lowfi_learning_pdiffs_df$rho_mean_contrast[lowfi_learning_pdiffs_df$condition == 'Dating/Looking'],
    lowfi_learning_pdiffs_df$fsmi_mate[lowfi_learning_pdiffs_df$condition == 'Dating/Looking'],
    method = 'kendall',
    exact = F)

fsmist_rho_test_tau_st <- cor.test(
    lowfi_learning_pdiffs_df$rho_mean_contrast[lowfi_learning_pdiffs_df$condition == 'Popular/Unpopular'],
    lowfi_learning_pdiffs_df$fsmi_stat[lowfi_learning_pdiffs_df$condition == 'Popular/Unpopular'],
    method = 'kendall',
    exact = F)

fsmims_xi_test_tau_ms <- cor.test(
    lowfi_learning_pdiffs_df$xi_mean_contrast[lowfi_learning_pdiffs_df$condition == 'Dating/Looking'],
    lowfi_learning_pdiffs_df$fsmi_mate[lowfi_learning_pdiffs_df$condition == 'Dating/Looking'],
    method = 'kendall',
    exact = F)

fsmist_xi_test_tau_st <- cor.test(
    lowfi_learning_pdiffs_df$xi_mean_contrast[lowfi_learning_pdiffs_df$condition == 'Popular/Unpopular'],
    lowfi_learning_pdiffs_df$fsmi_stat[lowfi_learning_pdiffs_df$condition == 'Popular/Unpopular'],
    method = 'kendall',
    exact = F)

lowfi_learning_pdiffs_df_w <- distinct(lowfi_learning_pdiffs_df) %>%
    filter(condition != condition_labels[1]) %>%
    select(ep_mean, rho_mean, xi_mean, fsmi_mate, fsmi_stat, condition, id) %>%
    gather(key, value, -condition, -id, -fsmi_mate, -fsmi_stat) %>%
    unite(key_cond, key, condition) %>%
    spread(key_cond, value)

ggcorplot(select(ungroup(lowfi_learning_pdiffs_df_w), -id), 
          method = 'spearman') + 
    ggplot2::scale_fill_gradientn(na.value = 'white',
                                  limits = c(-1,1),
                                  values = c(0, .25, .45, .5, .55, .75, 1), 
                                  colors = c('blue', 'blue', 'white', 'white', 'white', 'red', 'red'))

psych::corr.test(select(ungroup(lowfi_learning_pdiffs_df_w), -id), method = 'spearman')
```

```{r}
qplot(lowfi_learning_df$ep_mean[lowfi_learning_df$condition=='Dating/Looking'], 
      lowfi_learning_df$fsmi_mate[lowfi_learning_df$condition=='Dating/Looking'], geom = c('point', 'smooth'),
      method = 'gam', formula = y ~ s(x, fx = T, k = 3))

cor.test(lowfi_learning_df$ep_mean[lowfi_learning_df$condition=='Dating/Looking'], 
         lowfi_learning_df$fsmi_mate[lowfi_learning_df$condition=='Dating/Looking'], method = 'kendall')

qplot(lowfi_learning_df$ep_mean[lowfi_learning_df$condition=='Popular/Unpopular'], 
      lowfi_learning_df$fsmi_stat[lowfi_learning_df$condition=='Popular/Unpopular'], geom = c('point', 'smooth'),
      method = 'gam', formula = y ~ s(x, fx = T, k = 3))

cor.test(lowfi_learning_df$ep_mean[lowfi_learning_df$condition=='Popular/Unpopular'], 
         lowfi_learning_df$fsmi_stat[lowfi_learning_df$condition=='Popular/Unpopular'], method = 'kendall')

```

## Mean parameter estimates

### Learning rate

```{r}
munames <- grep('mu', names(stanfit_2_level), value = T)
stanfitmat <- as.array(stanfit_2_level)

bayesplot::mcmc_combo(stanfitmat, pars = munames[1:3], facet_args = list(scales = 'fixed'),
                      # transformations = pnorm, 
                      combo = c('areas_ridges', 'trace'), 
                      prob_outer = 1-.005, prob = .95, widths = c(3,4))

mu_ep_1 <- rstan::extract(stanfit_2_level, pars = munames[1])[[1]]
mu_ep_2 <- rstan::extract(stanfit_2_level, pars = munames[2])[[1]]
mu_ep_3 <- rstan::extract(stanfit_2_level, pars = munames[3])[[1]]

epsilon_difference_plot <- bayesplot::mcmc_areas(dplyr::data_frame(`e_2 - e_1` = pnorm(mu_ep_2) - pnorm(mu_ep_1),
                                                                   `e_3 - e_1` = pnorm(mu_ep_3) - pnorm(mu_ep_1),
                                                                   `e_3 - e_2` = pnorm(mu_ep_3) - pnorm(mu_ep_2)), prob_outer = 1-.005, prob = .95) + 
    ggplot2::scale_y_discrete(breaks = c('e_3 - e_2', 'e_3 - e_1', 'e_2 - e_1'),
                              labels = c(expression(epsilon[3]-epsilon[2]),
                                         expression(epsilon[3]-epsilon[1]),
                                         expression(epsilon[2]-epsilon[1])))

epsilon_mean_plot <- bayesplot::mcmc_areas(dplyr::data_frame(`e_3` = pnorm(mu_ep_3),
                                                                   `e_2` = pnorm(mu_ep_2),
                                                                   `e_1` = pnorm(mu_ep_1)), 
                                                  prob_outer = 1-.005, prob = .95) + 
    ggplot2::scale_y_discrete(breaks = c('e_3', 'e_2', 'e_1'),
                              labels = c(expression(epsilon[3]),
                                         expression(epsilon[2]),
                                         expression(epsilon[1])))

epsilon_difference_plot

save_out_list <- c(save_out_list, 'epsilon_difference_plot')
```

### Size of reward modifier

```{r}
bayesplot::mcmc_combo(stanfitmat, pars = munames[7:9], facet_args = list(scales = 'fixed'),
                      combo = c('areas_ridges', 'trace'), 
                      prob_outer = 1-.005, prob = .95, widths = c(3,4))

mu_rho_1 <- rstan::extract(stanfit_2_level, pars = munames[7])[[1]]
mu_rho_2 <- rstan::extract(stanfit_2_level, pars = munames[8])[[1]]
mu_rho_3 <- rstan::extract(stanfit_2_level, pars = munames[9])[[1]]

rho_difference_plot <- bayesplot::mcmc_areas(dplyr::data_frame(`r_2 - r_1` = exp(mu_rho_2) - exp(mu_rho_1),
                                                               `r_3 - r_1` = exp(mu_rho_3) - exp(mu_rho_1),
                                                               `r_3 - r_2` = exp(mu_rho_3) - exp(mu_rho_2)), prob_outer = 1-.005, prob = 1-.05) + 
    ggplot2::scale_y_discrete(breaks = c('r_3 - r_2', 'r_3 - r_1', 'r_2 - r_1'),
                              labels = c(expression(rho[3]-rho[2]),
                                         expression(rho[3]-rho[1]),
                                         expression(rho[2]-rho[1])))
rho_difference_plot
```

### Irriducible noise (e.g., "what the whaaa?")

```{r}
bayesplot::mcmc_combo(stanfitmat, pars = munames[4:6], facet_args = list(scales = 'fixed'),
                      transformations = pnorm, combo = c('areas', 'trace'), prob_outer = 1-.005, prob = .95, widths = c(3,4))

mu_xi_1 <- rstan::extract(stanfit_2_level, pars = munames[4])[[1]]
mu_xi_2 <- rstan::extract(stanfit_2_level, pars = munames[5])[[1]]
mu_xi_3 <- rstan::extract(stanfit_2_level, pars = munames[6])[[1]]

xi_difference_plot <- bayesplot::mcmc_areas(
    dplyr::data_frame(`x_2 - x_1` = pnorm(mu_xi_2) - pnorm(mu_xi_1),
                      `x_3 - x_1` = pnorm(mu_xi_3) - pnorm(mu_xi_1),
                      `x_3 - x_2` = pnorm(mu_xi_3) - pnorm(mu_xi_2)), prob_outer = 1-.005, prob = 1-.05) + 
    ggplot2::scale_y_discrete(breaks = c('x_3 - x_2', 'x_3 - x_1', 'x_2 - x_1'),
                              labels = c(expression(xi[3]-xi[2]),
                                         expression(xi[3]-xi[1]),
                                         expression(xi[2]-xi[1])))
xi_difference_plot
```

### Right-Bias

```{r}
bayesplot::mcmc_combo(stanfitmat, pars = munames[10:12], facet_args = list(scales = 'fixed'),
                      combo = c('areas_ridges', 'trace'), 
                      prob_outer = 1-.005, prob = .95, widths = c(3,4))

mu_b_1 <- rstan::extract(stanfit_2_level, pars = munames[10])[[1]]
mu_b_2 <- rstan::extract(stanfit_2_level, pars = munames[11])[[1]]
mu_b_3 <- rstan::extract(stanfit_2_level, pars = munames[12])[[1]]

b_difference_plot <- bayesplot::mcmc_areas(dplyr::data_frame(`b_2 - b_1` = exp(mu_b_2) - exp(mu_b_1),
                                                             `b_3 - b_1` = exp(mu_b_3) - exp(mu_b_1),
                                                             `b_3 - b_2` = exp(mu_b_3) - exp(mu_b_2)), prob_outer = 1-.005, prob = 1-.05) + 
    ggplot2::scale_y_discrete(breaks = c('b_3 - b_2', 'b_3 - b_1', 'b_2 - b_1'),
                              labels = c(expression(b[3]-b[2]),
                                         expression(b[3]-b[1]),
                                         expression(b[2]-b[1])))
b_difference_plot
```

## Variance

```{r}
groupvar <- unlist(lapply(lme4::VarCorr(check_stim_rx_null_stim_rx_m), 
                          function(x) x['(Intercept)','(Intercept)']))
resvar <- pi^2 /3
(icc_cond <- groupvar/(resvar + sum(groupvar)))
```


In a simple logistic regression, grouping responses by participant, nested in sample, and by stimulus image, it turns out that only `r 100*round(icc_cond['id:sample'],2)`% of the variance is due to differences in participants. 
Note, that this is for the average probability of choosing an optimal response. 
Another `r 100*round(icc_cond['sample'],2)`% and `r 100*round(icc_cond['stim_image'],2)`% of the variance is accounted for by sample and stimulus image, respectively. 

In the context of the learning model, it's more difficult to estimate the proportion of total variance accounted for by individually-varying parameters, but there are a couple of ways to get a sense for how much variation in the individual parameters relative to the precision of those estimates.
First, one can examine the proportion of variance accounted for by individual level parameters across each parameter and condition.

```{r}
covsamps_ep <- probly::extract_cor_cov_samps(stanfit_2_level, par_subscript = 'ep')
covsamps_xi <- probly::extract_cor_cov_samps(stanfit_2_level, par_subscript = 'xi')
covsamps_rho <- probly::extract_cor_cov_samps(stanfit_2_level, par_subscript = 'rho')
covsamps_b <- probly::extract_cor_cov_samps(stanfit_2_level, par_subscript = 'b')

prop_var <- function(covmat) diag(covmat) / sum(diag(covmat))

var_prop_ep <- apply(apply(covsamps_ep$Sigma, c(3), prop_var), 1, quantile, probs = c(.025, .5, .975))
var_prop_xi <- apply(apply(covsamps_xi$Sigma, c(3), prop_var), 1, quantile, probs = c(.025, .5, .975))
var_prop_rho <- apply(apply(covsamps_rho$Sigma, c(3), prop_var), 1, quantile, probs = c(.025, .5, .975))
var_prop_b <- apply(apply(covsamps_rho$Sigma, c(3), prop_var), 1, quantile, probs = c(.025, .5, .975))
```

```{r}
varprop_df <- as.data.frame(rbind(var_prop_ep, var_prop_xi, var_prop_rho))
names(varprop_df) <- c('ht', 'dl', 'pu')
varprop_df$tile <- rownames(varprop_df)
varprop_df$par <- rep(c('epsilon', 'xi', 'rho'), each = 3)
varprop_df_l <- tidyr::spread(tidyr::gather(varprop_df, condition, value, -tile, -par), tile, value)
varprop_df_l$condition <- factor(varprop_df_l$condition, 
                                 levels = c('ht', 'dl', 'pu'),
                                 labels = condition_labels)

propvar_plot <- ggplot2::ggplot(varprop_df_l,
                                ggplot2::aes(x = condition, y = `50%`)) + 
    ggplot2::geom_errorbar(ggplot2::aes(ymin = `2.5%`, ymax = `97.5%`), width = 0) + 
    ggplot2::geom_point() + 
    ggplot2::facet_wrap(~par, labeller = label_parsed) + 
    ggplot2::scale_y_continuous(breaks = c(.25, .5, .75), labels = c('25%', '50%', '75%')) + 
    ggplot2::coord_cartesian(ylim = c(0,.75)) + 
    ggplot2::theme(axis.text.x = element_text(angle = 360-45, hjust = .5, vjust = 1)) + 
    ggplot2::labs(x = 'Condition, facetted by ', y = 'Proportion variance in\nindividual-level parameters')
propvar_plot
```

One might also examine the variance of posterior means versus the total variance across iterations.

```{r}
par_regex_list <- unlist(lapply(c('ep_prm', 'xi_prm', 'rho_prm', 'b'), paste0, '\\[\\d+,', 1:3, '\\]'))

apply(covsamps_ep$Sigma, c(1,2), mean)
apply(covsamps_xi$Sigma, c(1,2), mean)
apply(covsamps_rho$Sigma, c(1,2), mean)
apply(covsamps_b$Sigma, c(1,2), mean)

cov2cor(apply(covsamps_ep$Sigma, c(1,2), mean))
cov2cor(apply(covsamps_xi$Sigma, c(1,2), mean))
cov2cor(apply(covsamps_rho$Sigma, c(1,2), mean))
cov2cor(apply(covsamps_b$Sigma, c(1,2), mean))

var_stuff <- lapply(par_regex_list, function(par_regex){
    if(grepl('(xi|ep)', par_regex)){
        par_samples_trans <-  lapply(
            stanfit_2_level_betas[grepl(par_regex,
                                        names(stanfit_2_level_betas))],
            qnorm)
    } else if(grepl('rho', par_regex)){
        par_samples_trans <-  lapply(
            stanfit_2_level_betas[grepl(par_regex,
                                        names(stanfit_2_level_betas))],
            log)
    } else {
        par_samples_trans <-  stanfit_2_level_betas[grepl(par_regex,
                                                          names(stanfit_2_level_betas))]
    }
    par_means <- unlist(lapply(par_samples_trans, mean))
    var_means <- var(par_means)
    var_tot <- var(as.numeric(unlist(par_samples_trans)))
    alist <- list(c(var_means = var_means, var_tot = var_tot, prop_var = var_means/var_tot))
    names(alist) <- sub('(ep|xi|rho|b).*([1-3]).*', '\\1_\\2', par_regex)
    alist
})
knitr::kable(t(as.data.frame(var_stuff)), row.names = TRUE, digits = 2)
```

So the proportion of variance due to the means of individual-level parameter posteriors tends to be highest for $\xi$ and lowest for $\rho$.

## Model predicted behavior

Below is a plot of 313 model predicted runs, all using the exact same task structure (arbitrarily, the first run of the task). Behavior is guided by the mean parameter estimates, so any variabilility in behavior is due to the probabilistic nature of selecting either the right or left option based on the model's current knowledge at some point during the task. Another way of framing this is that it is the behavior of the averge actor, in 313 identical, hypothetical runs.

```{r getspltdata}

data(splt)
splt <- splt[!is.na(splt$pressed_r), ]
splt$cue <- as.numeric(as.factor(paste0(splt$condition, '_', splt$sex)))
splt$condition <- factor(splt$condition, levels = c('HngT', 'DtnL', 'PplU'))

# - N number of individuals
# - M number of samples
# - K number of conditions
# - mm sample ID for all individuals
# - Tsubj number of trials for each individual
# - cue an N x max(Tsubj) matrix of cue IDs for
#   each trial
# - n_cues total number of cues
# - condtion an N x max(Tsubj) matrix of condition
#   IDs for each trial
# - outcome is an array with dimensions N x T x 2
#   (response options) with the feedback for each
#   possible response. outcome[,,1] is for
#   correct left-presses, and outcome[,,2] is for
#   correct right-presses.
# - beta_xi, beta_b, beta_eps, beta_rho are N x K
#   matrices of the individually varying parameter
#   coefficients

set.seed(9991102)

group_index_mm <- get_sample_index(splt, levels = c('TDS1', 'TDS2', 'yads', 'yads_online'))

N <- 313
M <- 4
K <- 3
Tsubj <- rep(384, N)

outcome_arr <- array(
    matrix(
        rep(rep(sample(c(1,5), size = 128, replace = T), 3), N),
        byrow = T, nrow = N),
    dim = c(N, 384, 2))

#for each cue, 80% of the correct answer gets rewarded. repeat across condition to ensure comparability
correct_r_mat <- matrix(
    rep(
        rep(
            c(rbinom(n = 128/2, size = 1, prob = .8),
              rbinom(n = 128/2, size = 1, prob = .2)),
            3),
        N),
    byrow = T, nrow = N)

outcome_arr[,,1][correct_r_mat == 1] <- 0 #reward if left press, but right is correct = 0
outcome_arr[,,2][correct_r_mat == 0] <- 0 #reward if right press, but correct is not right = 0

Tsubj_unif <- Tsubj
condition_mat_unif <- matrix(rep(rep(1:3, each = 128), N), byrow = T, N)
cue_mat_unif <- matrix(rep(rep(1:6, each = 128/2), N), byrow = T, nrow = N)
outcome_arr_unif <- outcome_arr

beta_xi_mat <- matrix(
    rep(apply(rstan::extract(stanfit_2_level, pars = 'mu_delta_xi')[[1]], c(2,3), mean), 
        313), 
    nrow = 313, 
    byrow = T)
# beta_b_mat <- matrix(
#     rep(c(0,0,0), 
#         313), 
#     nrow = 313, 
#     byrow = T)
beta_b_mat <- matrix(
    rep(apply(rstan::extract(stanfit_2_level, pars = 'mu_delta_b')[[1]], c(2,3), mean), 
        313), 
    nrow = 313, 
    byrow = T)
beta_ep_mat <- matrix(
    rep(apply(rstan::extract(stanfit_2_level, pars = 'mu_delta_ep')[[1]], c(2,3), mean), 
        313), 
    nrow = 313, 
    byrow = T)
beta_rho_mat <- matrix(
    rep(apply(rstan::extract(stanfit_2_level, pars = 'mu_delta_rho')[[1]], c(2,3), mean), 
        313), 
    nrow = 313, 
    byrow = T)

shuffle_index <- rep(sample(1:128, size = 128, replace = F), 3) + 
    rep(c(0*128, 1*128, 2*128), each = 128)

cue_mat_unif <- cue_mat_unif[,shuffle_index]
outcome_arr_unif[,,1] <- outcome_arr_unif[,shuffle_index,1] 
outcome_arr_unif[,,2] <- outcome_arr_unif[,shuffle_index,2]

model_predicted_behavior <- probly::generate_responses(N = N, M = M, K = K, mm = group_index_mm, 
                                                       Tsubj = Tsubj_unif, cue = cue_mat_unif, 
                                                       n_cues = max(cue_mat_unif, na.rm = T), 
                                                       condition = condition_mat_unif, 
                                                       outcome = outcome_arr_unif, 
                                                       beta_xi = beta_xi_mat, beta_b = beta_b_mat, 
                                                       beta_eps = beta_ep_mat, beta_rho = beta_rho_mat)
```



```{r, fig.width=5.875, fig.height=4}
set.seed(4277925)
adf <- dplyr::mutate(
    dplyr::group_by(
        data.frame(id = rep(1:313, 384),
                   condition = factor(as.numeric(condition_mat_unif[,1:384]), 
                                      levels = 1:3,
                                      labels = c('Hungry/Thirsty', 'Dating/Looking', 'Popular/Unpopular')),
                   cue = paste0('Cue ', as.numeric(cue_mat_unif[,1:384]) %% 2),
                   p_press_right = as.numeric(model_predicted_behavior$p_press_right)),
        condition, id, cue),
    trial = 1:n(),
    p_press_opt = ifelse(cue == 'Cue 0', 1-p_press_right, p_press_right))

highlight_ids <- sample(1:313, size = 2)

model_mean_predicted_behavior_plot <- ggplot2::ggplot(
    adf,
    ggplot2::aes(x = trial, 
                 y = p_press_opt, 
                 group = interaction(condition, id, cue))) + 
    ggplot2::geom_line(alpha = .04, size = .3) +
    ggplot2::geom_line(alpha = 1, size = .3, 
                       ggplot2::aes(color = factor(id)),
                       data = dplyr::filter(adf, id %in% highlight_ids)) +
    ggplot2::facet_grid(cue~condition, drop = T) + 
    ggplot2::theme_minimal() +
    ggplot2::coord_cartesian(ylim = c(.125, 1)) + 
    ggplot2::scale_y_continuous(breaks = c(.2, .5, .8, 1)) +
    ggplot2::scale_color_manual(values = c('blue', 'darkblue')) + 
    ggplot2::labs(x = 'Trial number', y = 'Model predicted probability of optimal choice',
                  color = 'Example') + 
    ggplot2::theme(panel.grid.minor.y = element_blank())

model_mean_predicted_behavior_plot
```

## Individual probability trajectories

```{r}
make_idxd_df_from_mat <- function(a_splt_mat, value_name){
    a_splt_df <- as.data.frame(a_splt_mat)
    a_splt_df$idx <- 1:dim(a_splt_mat)[1]
    a_splt_df_l <- tidyr::gather(a_splt_df, trial, !!value_name, -idx)
    a_splt_df_l$trial <- as.numeric(gsub('\\w*?(\\d+)\\w*?', '\\1', a_splt_df_l$trial))
    return(a_splt_df_l)
}

extract_beta_matrix_from_beta_df <- function(betas_df, parname, par_transform = 'I', central_tendency = 'mean'){
    if(is.character(par_transform)){
        par_transform <- eval(parse(text = par_transform))
    }
    par_df <- betas_df[betas_df$parameter == parname & betas_df$contrasted == FALSE,]
    par_df <- par_df[, c(central_tendency, 'idx', 'condnum')]
    par_df[,central_tendency] <- par_transform(par_df[,central_tendency])
    par_df$idx <- as.numeric(par_df$idx)
    par_df_w <- tidyr::spread(par_df, condnum, !!central_tendency)
    par_mat <- as.matrix(par_df_w[,grep('idx', names(par_df_w), invert = T)])
    return(par_mat)
}

splt_data_used <- readRDS('/data/jflournoy/split/probly/splt-looser-data--1527693.RDS')

task_structure_used <- probly::make_task_structure_from_data(splt_data_used)
observed_press_r <- probly::get_col_as_trial_matrix(splt_data_used, col = 'pressed_r')

beta_xi_postmean_mat <- extract_beta_matrix_from_beta_df(splt_rl_betas,
                                                         parname = 'xi_prm',
                                                         par_transform = 'qnorm',
                                                         central_tendency = 'mean')
beta_b_postmean_mat <- extract_beta_matrix_from_beta_df(splt_rl_betas,
                                                         parname = 'b',
                                                         par_transform = 'I',
                                                         central_tendency = 'mean')
beta_ep_postmean_mat <- extract_beta_matrix_from_beta_df(splt_rl_betas,
                                                         parname = 'ep_prm',
                                                         par_transform = 'qnorm',
                                                         central_tendency = 'mean')
beta_rho_postmean_mat <- extract_beta_matrix_from_beta_df(splt_rl_betas,
                                                         parname = 'rho_prm',
                                                         par_transform = 'log',
                                                         central_tendency = 'mean')

model_predicted_behavior_individuals_pR <- generate_responses(
    N = task_structure_used$N, 
    M = task_structure_used$M, 
    K = task_structure_used$K, 
    mm = task_structure_used$mm,
    Tsubj = task_structure_used$Tsubj, 
    cue = task_structure_used$cue,
    n_cues = max(task_structure_used$cue, na.rm = T), 
    condition = task_structure_used$condition, 
    outcome = task_structure_used$outcome, 
    beta_xi = beta_xi_postmean_mat, 
    beta_b = beta_b_postmean_mat, 
    beta_eps = beta_ep_postmean_mat, 
    beta_rho = beta_rho_postmean_mat,
    press_right = observed_press_r)

pr_df_l <- left_join(
    make_idxd_df_from_mat(model_predicted_behavior_individuals_pR$p_press_right, 
                          value_name = 'pR'),
    mutate(distinct(splt_data_used, id), idx = 1:n()),
    by = 'idx')

splt_data_used_pR <- left_join(
    mutate(
        group_by(
            arrange(splt_data_used, id, trial_index),
            id),
        trial = 1:n()),
    pr_df_l, by = c('id', 'trial')) %>%
    mutate(pOpt = case_when(
        proportion == '80_20' ~ 1-pR,
        proportion == '20_80' ~ pR,
        TRUE ~ NA_real_))

ggplot2::theme_set(ggplot2::theme_minimal())
ggplot2::ggplot(splt_data_used_pR, 
                ggplot2::aes(x = trial, y = pOpt)) + 
    ggplot2::geom_point(alpha = .04, size = .5) + 
    ggplot2::facet_wrap(~condition)

ggplot2::ggplot(splt_data_used_pR, 
                ggplot2::aes(x = trial, y = pOpt, group = id)) + 
    ggplot2::geom_line(alpha = .02, size = .5) + 
    ggplot2::facet_wrap(~condition)

ggplot2::ggplot(splt_data_used_pR, 
                ggplot2::aes(x = trial, y = pOpt)) + 
    ggplot2::geom_point(alpha = .01, size = .5) + 
    ggplot2::geom_line(
        ggplot2::aes(linetype = condition),
        stat = 'smooth', method = 'gam', formula = y ~ s(x, k = 5)) + 
    ggplot2::coord_cartesian(y = c(.5, 1.01))

ggplot2::ggplot(splt_data_used_pR, 
                ggplot2::aes(x = trial, y = pOpt, group = id)) + 
    ggplot2::geom_line(alpha = .03, size = .5) + 
    ggplot2::facet_grid(sex~condition) +
    ggplot2::coord_cartesian(y = c(.3, 1.1))

splt_data_used_pR_block_id_sum <- dplyr::summarize(
    dplyr::mutate(
        dplyr::group_by(splt_data_used_pR, block, condition, id),
        pressed_opt = case_when(
            proportion == '80_20' ~ 1-pressed_r,
            proportion == '20_80' ~ pressed_r,
            TRUE ~ NA_real_)),
    pressed_opt = mean(pressed_opt),
    pOpt = mean(pOpt),
    n_rt_300 = sum(rt<300))

if(!file.exists(file.path(data_dir, 'splt_data_used_pR_block_sum.RDS'))){
    splt_data_used_pR_block_sum <- dplyr::do(
        # dplyr::mutate(
        #     dplyr::group_by(splt_data_used_pR, block, condition),
        #     pressed_opt = case_when(
        #         proportion == '80_20' ~ 1-pressed_r,
        #         proportion == '20_80' ~ pressed_r,
        #         TRUE ~ NA_real_)),
        dplyr::group_by(splt_data_used_pR_block_id_sum, block, condition),
        {
            adf <- .
            bootstrapped <- boot::boot(adf[,c('pressed_opt', 'pOpt')], 
                                       statistic = function(data, i){c(pressed_opt=mean(unlist(data[i,1])),
                                                                       pOpt=mean(unlist(data[i,2])))}, 
                                       R = 10000,
                                       stype = 'i',
                                       parallel = 'multicore', ncpus = 8)
            bootstrapped_means <- apply(bootstrapped$t, 2, mean)
            
            pressed_opt_ci <- boot::boot.ci(bootstrapped, 
                                            index = 1, 
                                            type = c('perc'))$perc[1,4:5]
            pOpt_ci <- boot::boot.ci(bootstrapped, 
                                     index = 2, 
                                     type = c('perc'))$perc[1,4:5]
            
            data.frame(
                block = adf$block[[1]],
                condition = adf$condition[[1]],
                parameter = c('pressed_opt', 'pOpt'),
                mean = bootstrapped_means,
                lower.ci = c(pressed_opt_ci[1], pOpt_ci[1]),
                upper.ci = c(pressed_opt_ci[2], pOpt_ci[2]))
        })
    saveRDS(splt_data_used_pR_block_sum, file.path(data_dir, 'splt_data_used_pR_block_sum.RDS'))
} else {
    splt_data_used_pR_block_sum <- readRDS(file.path(data_dir, 'splt_data_used_pR_block_sum.RDS'))
}
```

```{r fig.width=5.875, fig.height=3}
splt_data_used_pR_block_sum_plot <- ggplot2::ggplot(splt_data_used_pR_block_sum,
                ggplot2::aes(x = block, y = mean)) +
    ggplot2::geom_ribbon(ggplot2::aes(ymin = lower.ci, 
                                      ymax = upper.ci, 
                                      group = interaction(condition,parameter)),
                         alpha = .125) + 
    ggplot2::geom_line(ggplot2::aes(group = interaction(condition,parameter),
                                    linetype = condition),
                       alpha = 1, size = .5) + 
    ggplot2::facet_grid(~parameter)+
    ggplot2::coord_cartesian(y=c(.55,.8))
splt_data_used_pR_block_sum_plot
```

```{r fig.height=120, fig.width=4}
individual_model_predicted_versus_expected_optchoice_plot <- ggplot2::ggplot(gather(splt_data_used_pR_block_id_sum, parameter, value, pressed_opt, pOpt),
                ggplot2::aes(x = block, y = value)) +
    ggplot2::geom_line(ggplot2::aes(color = parameter, group = parameter),
                       alpha = 1, size = .5) + 
    ggplot2::facet_grid(id~condition)
individual_model_predicted_versus_expected_optchoice_plot
```

```{r fig.width=5.875, fig.height=3}
set.seed(87987611/9740451)
sampled_ids <- sample(unique(splt_data_used_pR_block_id_sum$id), 50)

individual_model_predicted_versus_expected_optchoice_plot_samples <- 
    lapply(1:5, function(i) {
        ggplot2::ggplot(gather(
            filter(splt_data_used_pR_block_id_sum, id %in% sampled_ids[(10*(i-1)+1):(10*i)]), 
            parameter, value, pressed_opt, pOpt),
            ggplot2::aes(x = block, y = value)) +
            ggplot2::geom_line(ggplot2::aes(color = parameter, group = parameter),
                               alpha = 1, size = .5) + 
            ggplot2::facet_grid(condition~id) + 
            ggplot2::theme(legend.position = 'none')})
individual_model_predicted_versus_expected_optchoice_plot_samples
```


```{r fig.width=5.875, fig.height=3}
#model bias
splt_data_used_pR_block_id_sum$bias <- splt_data_used_pR_block_id_sum$pOpt -
    splt_data_used_pR_block_id_sum$pressed_opt
ggplot2::ggplot(splt_data_used_pR_block_id_sum,
                ggplot2::aes(x = block, y = bias)) +
    ggplot2::geom_hline(yintercept = 0, color = '#444444') + 
    ggplot2::geom_point(alpha = .1, size = .5,
                        position = position_jitter()) + 
    ggplot2::geom_line(stat = 'smooth', method = 'loess', size = 1) + 
    ggplot2::facet_grid(~condition)
```

Given these model parameters, and the very strong assumption that nothing would purturb them from session to session, what would be the expected test-retest correlation for the differences between the H/T baseline and D/L or P/U conditions? 
The outcome of interest I'll use is the average difference in the probability of choosing the optimal label in the last half of the run.
This will over-estimate the true test-reliability because we're using what is in practice an unknown latent parameter.
Measurement error in any real situation will attenuate this true reliability.
One way to assess this reliability is to draw random IDs (which in this case corresponds to simulated runs through the task), and then correlate the mean number of optimal presses in each condition across two sets of random draws.
After all, since these simulations are all generating using the model parameters from the same participant, the rank-order of the mean probability of optimal press across the three conditions should be similar across simulations to the extent that the generated behavior is stable.
A less ad hoc approach may be to use an interclass-correlation estimate to partition the variance into that systematically related to condition versus residual variance.
In both cases, the estimate is middling, around .4 and .5 respectively.
We can compare this to the observed reliability of the parameter estimates we obtain from the Bayesian models.

```{r}
mean_modpred_ppressopt <- adf %>%
    dplyr::filter(trial > max(trial)/2) %>%
    dplyr::group_by(id, condition, trial) %>%
    dplyr::summarize(mean_p_opt = mean(p_press_opt)) %>%
    dplyr::group_by(id, condition) %>%
    dplyr::summarize(mean_mean_p_opt = mean(mean_p_opt))

reliability_cors <- replicate(500, {
    idx <- unlist(lapply(sample(1:313, size = 313*2, replace = T),
                         function(id) which(mean_modpred_ppressopt$id == id)))
    cor(mean_modpred_ppressopt$mean_mean_p_opt[idx[1:(313*3)]],
        mean_modpred_ppressopt$mean_mean_p_opt[idx[(313*3+1):(313*3*2)]])  
})

quantile(reliability_cors, probs = c(.025, .5, .975))
hist(reliability_cors)

summary(icc_mod <- lme4::lmer(mean_mean_p_opt ~ 1 + (1 | condition), data = mean_modpred_ppressopt))
condvar <- lme4::VarCorr(icc_mod)$condition['(Intercept)', '(Intercept)']
resvar <- sigma(icc_mod)^2
(icc_cond <- condvar/(resvar + condvar))

# %>%
#     dplyr::group_by(id, trial) %>%
#     dplyr::mutate_at(vars(mean_p_opt),
#                      funs(. - .[condition == 'Hungry/Thirsty'])) %>%
#     dplyr::filter(condition != 'Hungry/Thirsty') 

```

## Extra figs

- individual-level betas + contrast betas for each sample
    - for each individual, sorted

```{r fig.width=5.875, fig.height=5}
ggplot2::theme_set(ggplot2::theme_minimal())
plot_beta_estimates_by_sample <- function(splt_rl_betas, parameter, condition_labels, sort_condition = 'Hungry/Thirsty',
                                          transformation = 'I', inverstrans = 'I', breaks = NULL) {
    id_levels <- unlist(dplyr::distinct(dplyr::arrange(
        splt_rl_betas[splt_rl_betas$parameter == parameter & 
                          splt_rl_betas$condition == sort_condition &
                          !splt_rl_betas$contrasted,],
        mean), id))
    
    tmp_df <- splt_rl_betas[splt_rl_betas$parameter == parameter & !splt_rl_betas$contrasted,]
    tmp_grp_df <- dplyr::summarize(group_by(tmp_df, sample, condition),
                                          mean = mean(mean))
    aplot <- ggplot2::ggplot(tmp_df,
                    ggplot2::aes(x = factor(id, levels = id_levels), 
                                 y = mean)) + 
        ggplot2::geom_errorbar(ggplot2::aes(ymin = lower, ymax = upper), width = 0, alpha = .1) + 
        ggplot2::geom_hline(aes(yintercept = mean), data = tmp_grp_df, color = "#666666") +
        ggplot2::geom_point(size = .5) + 
        ggplot2::facet_grid(sample~factor(condition, levels = condition_labels)) +
        ggplot2::theme(axis.text.x = ggplot2::element_blank(), panel.grid.major.x = ggplot2::element_blank(),
                       panel.grid.minor.y = element_blank()) + 
        ggplot2::labs(x = 'Participant', y = 'Posterior mean and 95% credible interval')
    if(!is.null(breaks)){
        aplot <- aplot + ggplot2::scale_y_continuous(breaks = breaks)
    }
    if(transformation != 'I'){
        aplot <- aplot + ggplot2::coord_trans(y = scales::trans_new('mytrans', transformation, inverstrans))
    }
    return(aplot)
}

beta_estimates_by_sample_ep_prm_plot <- plot_beta_estimates_by_sample(splt_rl_betas, parameter = 'ep_prm', 
                              condition_labels = condition_labels, transformation = 'qnorm',
                              inverstrans = 'pnorm', breaks = c(.001, .01, .05, .2, .5))

beta_estimates_by_sample_rho_prm_plot <- plot_beta_estimates_by_sample(splt_rl_betas, parameter = 'rho_prm', 
                              condition_labels = condition_labels, transformation = 'log',
                              inverstrans = 'exp',
                              breaks = c(.1, .33, 1, 3, 10))

beta_estimates_by_sample_xi_prm_plot <- plot_beta_estimates_by_sample(splt_rl_betas, parameter = 'xi_prm', 
                              condition_labels = condition_labels, transformation = 'qnorm',
                              inverstrans = 'pnorm', breaks = c(.01, .1, .5, .9))

beta_estimates_by_sample_b_plot <- plot_beta_estimates_by_sample(splt_rl_betas, parameter = 'b', 
                              condition_labels = condition_labels)

beta_estimates_by_sample_ep_prm_plot
beta_estimates_by_sample_rho_prm_plot
beta_estimates_by_sample_xi_prm_plot
beta_estimates_by_sample_b_plot
```


## Diagnostics


```{r eval=F}
rstan::stan_diag(stanfit, information = 'sample')
rstan::stan_diag(stanfit, information = 'stepsize')
rstan::stan_diag(stanfit, information = 'treedepth')
rstan::stan_diag(stanfit, information = 'divergence')

for(i in 1:length(munames)){
    rstan::stan_par(stanfit, par = munames[i])
}
rstan::stan_rhat(stanfit, pars = munames)
rstan::stan_ess(stanfit, pars = munames)
pairs(stanfit, pars = munames, condition = 'accept_stat__')

for(i in 1:(length(betanames)/313)){
    thesebetanames <- betanames[(313*(i-1) + 1):(313*i)]
    print(
        rstan::stan_rhat(
            stanfit, 
            pars = thesebetanames, binwidth = .005) + 
            geom_vline(xintercept = 1.1) + 
            labs(title = paste0('Rhat: ', thesebetanames[1])))
    print(
        rstan::stan_ess(
            stanfit, 
            pars = thesebetanames, binwidth = .01) + 
            geom_vline(xintercept = .001) + 
            labs(title = paste0('ESS: ', thesebetanames[1])))
}


```

```{r pairs, fig.width=15, fig.height=15, eval=F}
pairs(stanfit, pars = munames)
pairs(stanfit, pars = taunames)
```

```{r pairs2, fig.width=30, fig.height=30, eval=F}
ep_cor_cov_samps <- probly::extract_cor_cov_samps(stanfit, par_subscript = 'ep')
xi_cor_cov_samps <- probly::extract_cor_cov_samps(stanfit, par_subscript = 'xi')
rho_cor_cov_samps <- probly::extract_cor_cov_samps(stanfit, par_subscript = 'rho')

ep_cov_samps <- as.data.frame(t(apply(ep_cor_cov_samps$Sigma, 3, function(x) {
    d <- c(x[lower.tri(x)], diag(x))
    names(d) <- c('[2,1]', '[3,1]', '[3,2]', '[1,1]', '[2,2]', '[3,3]')
    d
})))
names(ep_cov_samps) <- paste0('ep_', names(ep_cov_samps))
xi_cov_samps <- as.data.frame(t(apply(xi_cor_cov_samps$Sigma, 3, function(x) {
    d <- c(x[lower.tri(x)], diag(x))
    names(d) <- c('[2,1]', '[3,1]', '[3,2]', '[1,1]', '[2,2]', '[3,3]')
    d
})))
names(xi_cov_samps) <- paste0('xi_', names(xi_cov_samps))
rho_cov_samps <- as.data.frame(t(apply(rho_cor_cov_samps$Sigma, 3, function(x) {
    d <- c(x[lower.tri(x)], diag(x))
    names(d) <- c('[2,1]', '[3,1]', '[3,2]', '[1,1]', '[2,2]', '[3,3]')
    d
})))
names(rho_cov_samps) <- paste0('rho_', names(rho_cov_samps))


sumnuts <- bayesplot::nuts_params(stanfit)

ggplot2::theme_set(theme_minimal())
bayesplot::mcmc_pairs(
    cbind(
        ep_cov_samps, 
        rho_cov_samps, 
        xi_cov_samps, 
        Chain = rep(1:6, each=750)), 
    np = sumnuts,
    off_diag_fun = 'hex')
```

```{r echo = F, eval = T}
save.image(file = file.path(save_out_dir, 'fit-model-to-participant-data.rda'))
save(beta_estimates_by_sample_ep_prm_plot,
           beta_estimates_by_sample_rho_prm_plot,
           beta_estimates_by_sample_xi_prm_plot,
           beta_estimates_by_sample_b_plot,
           splt_data_used_pR_block_sum_plot,
           individual_model_predicted_versus_expected_optchoice_plot,
           individual_model_predicted_versus_expected_optchoice_plot_samples,
           file = file.path(save_out_dir, 'extra-model-plots.rda'))
```
