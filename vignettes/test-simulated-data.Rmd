---
title: "Test Simulated Data"
author: "John Flournoy"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: "`r system.file('bib', 'dissertation.bib', package = 'probly')`"
csl: "`r system.file('bib', 'apa-old-doi-prefix.csl', package = 'probly')`"
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


# Aim 1a: Does framing reinforcement learning with (mate-seeking and status) motivational contexts sensitize the learner and potentiate learning?

## Method

**TASK DESCRIPTION HERE**

## Modeling learning

In the context of this task, where the relation between the optimal response and the stimulus is constant, a simple model of the degree of learning could rely on a simple proportion of optimal responses $P_{ok}$ for each condition $k$.
The test of the hypothesis of the effect of framing would then be the difference between conditions in $P_o$. 
This simple model sacrifice precision for simplicity, and so I will be modeling the data using a reinforcement learning model with several parameters that can account for deviations from a strict Rescorla-Wagner (RW) process.
This increases the number of possible comaprisons I am able to make between conditions, which may generate useful information about how motive-domain framing affects the learning process (as modelled, of course), but which also increases the complexity of patterns between conditions and parameters that must be interpretted.
It will be helpful to keep in mind that the framing can only be said to potentiate learning if, regardless of its affect on any model parameters, it does not result in higher proportions of optimal responding.

### Ensuring the model can recover known parameters

Before modeling the task data, I will confirm that this model can recover known parameters from simulated data.
In this section, I simulate data as expected under the Rescorla-Wagner model implemented by @ahn2017 in their go-no-go model 2. 
Their original model handles binary decisions (button-press or no button-press) in response to four different cues. 
However, the form of the learning algorithm is generalizable to other binary choices in response to cues. 
In the case of the Social Probabilistic Learning Task (SPLT), participants are presented with a face (the cue), and must decide to press the left key or the right key. 
They are rewarded probabilistically such that for each cue, one or the other of the response options has an 80% chance of providing reinforcement. 
The go-no-go models used by @ahn2017 were derived from work by @guitart-masip2012. 
Their most flexible reinforcement learning model generates the probability of an action for each trial via N parameters: the learning rate, $\epsilon$, the effective size of reinforcement, $\rho$, a static bias parameter, $b$, an irriducible noise parameter, $\xi$, and a Pavlovian learning parameter, $\pi$.
In the SPLT, trial feedback does not vary by valence (responses result in reward, or no reward, but never punishment), so I use the model that does not include this Pavlonian component. 

#### Reinforcement learning model for the SPLT

The model for an individual $j$'s probability of pressing the right arrow key on trial $t$ given that stimulus $s_{t}$ is presentd, $P(a_{\rightarrow t} | s_{t})_{t}$, is determined by a logistic transformation of the action weight for pressing the right arrow key minus the action weight for pressing the left arrow key. 
This probability is then adjusted by a noise parameter, $0 \leq\xi_{jk}\leq1$ for each participant $j$ in condition $k$.
The noise parameter modulates the degree to which responses are non-systematic. 
When $\xi$ is 1, $P_{it} = .5$, and because each individual has a unique noise parameter for each condition, I am able to account for participants who do not learn during the task, or in a particular condition.
The full equation is:

$$
P(a_{\rightarrow t} | s_{t})_{t} = 
\text{logit}^{-1}\big(W(a_{\rightarrow t}| s_{t}) - W(a_{\leftarrow t}| s_{t})\big)\cdot(1-\xi_{jk}) + \small\frac{\xi_{jk}}{2}.
$$

The action weight is determined by a Rescorla-Wagner (RW) updating equation and individual $j$'s bias parameter, $b_{jk}$, for that condition (which encodes a systematic preference for choosing the left or right response option).
In each condition, the same two words are displayed in the same position, so $b$ encodes a learning-independent preference for one particular word or position.
The equation for the action weight for each action on a particular trial is:

$$
W_{t}(a,s) = \left\{
                \begin{array}{ll}
                  Q_{t}(a, s) + b_{jk}, & \text{if } a=a_{\rightarrow} \\
                  Q_{t}(a, s), & \text{otherwise}
                \end{array}
              \right.
$$
Finally, the RW updating equation that encodes instrumental learning is governed by the individual's learning rate for that condition, $\epsilon_{jk}$, and a scaling parameter $\rho_{jk}$ governing the effective size of the possible rewards $r_t \in \{0, 1, 5\}$:

$$
Q_{t}(a_t, s_t) = Q_{t-1}(a_t, s_t) + \epsilon_{jk}\big(\rho_{jk}r_t - Q_{t-1}(a_t, s_t)\big)
$$

#### Hierarchical Parameters

Each parameter ($\epsilon, \rho, b, \xi$) varries by condition $k \in 1:K$, and by participant $j \in 1:J$ nested in sample $m \in 1:M$. 
The structure of the hierarchical part of the model is the same for each parameter, so the following description for $\epsilon$ will serve as a description for all of the parameters.
For each individual $j$, $\beta_{\epsilon j}$ is a $K$-element row of coefficients for parameter $\epsilon$ for each condition:

$$
\beta_{\epsilon j} \sim \mathcal{N}(\delta_{\epsilon mm[j]}, \Sigma_{\epsilon})
$$
where $\delta_{\epsilon mm[j]}$ is a column of $K$ means for individual $j$'s sample $M$, as indexed in the vector $mm$, and $\Sigma_{\epsilon}$ is a $K\times K$ matrix of the covariance of individual coefficients between conditions.

Finally, across all $M$ samples, the means for each condition k are distributed such that: 

$$
\delta_{\epsilon k} \sim \mathcal{N}(\mu_{\epsilon k}, \sigma_\epsilon)
$$

where $\mu_{\epsilon k}$ is the population mean for parameter $\epsilon$ in condition $k$, and $\sigma$ is a slightly regularizing scale parameter for these means across all conditions and samples. The priors for these final paramters are:

$$
\mu_\epsilon \sim \mathcal{N}(0, 5)\\
\sigma_\epsilon \sim \text{exponential(1)}.
$$

#### Simulating data

I simulate data based on the structure of the sample data, using the same number of participants per sample (see the section on [descriptive statistics](descriptive-statistics.html), as well as precisely the same task structure. 
For this aim, it is important to be able to recover all $\mu_{\theta k}$ for $\theta \in \{\epsilon,\rho,b,\xi\}$ and $k \in \{1,2,3\}$, where 1 = Hungry/Thirsty, 2 = Popular/Unpopular, and 3 = Dating/Looking. Those parameters that account for idiosyncratic devation from RW-expected behavior ($b,\xi$) will not vary by condition. Based on interactive simulation ([here](https://jflournoy.shinyapps.io/rw_model/)), reasonable paramter values for the control condition might be $\mu_\epsilon = -1.65$ and $\mu_\rho = -0.3$ [^1].

The functions that sample parameters as described above are:

```{r}
sample_deltas <- function(mu_vec, sigma, nsamples){
    deltas_vec <- replicate(nsamples,
                            rnorm(n = length(mu_vec), 
                                  mean = mu_vec, sd = sigma))
    deltas <- matrix(deltas_vec,
                     nrow = nsamples,
                     byrow = TRUE)
    return(deltas)
}
sample_betas <- function(deltas, group_index, Sigma){
    require(MASS)
    deltas_mm <- deltas[group_index, ]
    betas_l <- lapply(1:length(group_index), function(j){
        MASS::mvrnorm(1, mu = deltas[group_index[j], ], Sigma = Sigma)
    })
    betas <- do.call(rbind, betas_l)
    return(betas)
}
generate_responses <- function(N, M, K, mm, Tsubj, cue, n_cues, condition, outcome, beta_xi, beta_b, beta_eps, beta_rho){
    # - N number of individuals
    # - M number of samples
    # - K number of conditions
    # - mm sample ID for all individuals
    # - Tsubj number of trials for each individual
    # - cue an N x max(Tsubj) matrix of cue IDs for 
    #   each trial
    # - n_cues total number of cues
    # - condtion an N x max(Tsubj) matrix of condition 
    #   IDs for each trial
    # - outcome is an array with dimensions N x T x 2
    #   (response options) with the feedback for each 
    #   possible response. outcome[,,1] is for 
    #   correct left-presses, and outcome[,,2] is for
    #   correct right-presses.
    # - beta_xi, beta_b, beta_eps, beta_rho are N x K 
    #   matrices of the individually varying parameter 
    #   coeffients
    
    press_right <- matrix(nrow = N, ncol = max(Tsubj)) #the matrix to return
    
    for(i in 1:N){
        wv_r <- numeric(n_cues) #action weight for press-right
        wv_l <- numeric(n_cues) #action weight for press-left
        qv_r <- numeric(n_cues) #Q value for right
        qv_l <- numeric(n_cues) #Q value for left
        p_right <- numeric(n_cues) #probability of pressing right
        
        for(t in 1:Tsubj[i]){
            wv_r[ cue[i, t] ]    <- qv_r[ cue[i, t] ] + beta_b[ i, condition[i, t] ] #add bias
            wv_l[ cue[i, t] ]    <- qv_l[ cue[i, t] ]
            
            p_right[ cue[i, t] ] <- arm::invlogit( wv_r[ cue[i, t] ] - wv_l[ cue[i, t] ] )
            p_right[ cue[i, t] ] <- 
                p_right[ cue[i, t] ] * (1 - beta_xi[ i, condition[i, t] ]) + 
                beta_xi[ i, condition[i, t] ] / 2 #incorporate noise
            
            press_right[i, t]    <- rbinom(n = 1, size = 1, prob = p_right[ cue[i, t] ])
            
            if(press_right[i, t]){ # press_right[i, t] == 1
                qv_r[ cue[i, t] ] <- 
                    qv_r[ cue[i, t] ] + beta_eps[ i, condition[i, t] ] *
                    (beta_rho[ i, condition[i, t] ] * outcome[i, t, 2] - qv_r[ cue[i, t] ])
            } else { # press_right[i, t] == 0
                qv_l[ cue[i, t] ] <- 
                    qv_l[ cue[i, t] ] + beta_eps[ i, condition[i, t] ] *
                    (beta_rho[ i, condition[i, t] ] * outcome[i, t, 1] - qv_l[ cue[i, t] ])
            }
        } # t loop
    }# i loop
    return(press_right)
}

get_sample_index <- function(splt_df, id_col = 'id', sample_col = 'sample', levels = sort(unique(splt_df[, sample_col]))) {
    mm <- unique(splt[, c(id_col, sample_col)])
    rownames(mm) <- 1:dim(mm)[1]
    mm$m_fac <- factor(mm$sample,
                       levels = levels)
    mm$m <- as.numeric(mm$m_fac)
    return(mm)
}

get_col_as_trial_matrix <- function(splt_df, col, id_col = 'id', sample_col = 'sample', trial_col = 'trial_index'){
    #expects all rows with pressed_r == NA to have been removed
    if(!(is.factor(splt_df[, col][[1]]) | is.numeric(splt_df[, col][[1]]))){
        stop("col must be numeric or a factor that will be coerced to numeric.")
    }
    id_sample_index <- paste0(unlist(splt_df[,id_col]), unlist(splt_df[,sample_col]))
    ids <- unique(id_sample_index)
    max_trials <- max(unlist(lapply(split(splt_df, id_sample_index), 
                                    function(x) dim(x)[1])))
    col_mat <- matrix(nrow = length(ids), ncol = max_trials)
    for(id in ids){
        trials_index <- unlist(splt_df[id_sample_index == id, trial_col])
        col_vec <- unlist(splt_df[id_sample_index == id, col])[order(trials_index)]
        col_mat[which(ids == id), 1:length(col_vec)] <- col_vec
    }# end id
    rownames(col_mat) <- ids
    return(col_mat)
}
```

```{r}
library(probly)
data(splt)

splt <- splt[!splt$id %in% c(43873, 'bad_pid', 43603, 43991, 43214), ]
splt <- splt[!(splt$id == 386 & splt$sample == 'yads'), ]
splt$sample <- ifelse(splt$sample == 'TDS3', 'TDS1', splt$sample)
splt <- splt[!is.na(splt$pressed_r), ]
splt$cue <- as.numeric(as.factor(paste0(splt$condition, '_', splt$sex)))
splt$condition <- factor(splt$condition, levels = c('HngT', 'DtnL', 'PplU'))

# - N number of individuals
# - M number of samples
# - K number of conditions
# - mm sample ID for all individuals
# - Tsubj number of trials for each individual
# - cue an N x max(Tsubj) matrix of cue IDs for 
#   each trial
# - n_cues total number of cues
# - condtion an N x max(Tsubj) matrix of condition 
#   IDs for each trial
# - outcome is an array with dimensions N x T x 2
#   (response options) with the feedback for each 
#   possible response. outcome[,,1] is for 
#   correct left-presses, and outcome[,,2] is for
#   correct right-presses.
# - beta_xi, beta_b, beta_eps, beta_rho are N x K 
#   matrices of the individually varying parameter 
#   coeffients

group_index_mm <- get_sample_index(splt, levels = c("TDS1", "TDS2", "yads", "yads_online"))
N <- dim(group_index_mm)[1]
M <-length(levels(group_index_mm$m_fac)) 
K <- length(unique(splt$condition))
cue_mat <- get_col_as_trial_matrix(splt, 'cue', id_col = 'id', sample_col = 'sample', trial_col = 'trial_index')
condition_mat <- get_col_as_trial_matrix(splt, 'condition', id_col = 'id', sample_col = 'sample', trial_col = 'trial_index')
correct_r_mat <- get_col_as_trial_matrix(splt, 'correct_r', id_col = 'id', sample_col = 'sample', trial_col = 'trial_index')
reward_possible_mat <- get_col_as_trial_matrix(splt, 'reward_possible', id_col = 'id', sample_col = 'sample', trial_col = 'trial_index')
outcome_arr <- array(reward_possible_mat, dim = c(dim(reward_possible_mat), 2))
outcome_arr[,,1][correct_r_mat == 1] <- 0
outcome_arr[,,2][correct_r_mat == 0] <- 0

set.seed(99232486)

mu_xi <- rnorm(3, rep(-2.5, 3), .25)
mu_b <- rnorm(3, rep(0, 3), .25)
mu_eps <- c(-1.65, -1.65 + .3, -1.65 + .2)
mu_rho <- c(-.3, -.3 + .35, -.3 + .45)

delta_xi <- sample_deltas(mu_xi, .1, 4)
delta_b <- sample_deltas(mu_b, .1, 4)
delta_eps <- sample_deltas(mu_eps, .1, 4)
delta_rho <- sample_deltas(mu_rho, .1, 4)

Sigma <- matrix(rep(.1, 3*3), nrow = 3)
diag(Sigma) <- 1

Sigma_eps <- Sigma_rho <- Sigma
Sigma_b <- Sigma_xi <- diag(3)

beta_xi <- sample_betas(deltas = delta_xi,
                        group_index = group_index_mm$m,
                        Sigma = Sigma_xi)
beta_b <- sample_betas(deltas = delta_b,
                        group_index = group_index_mm$m,
                        Sigma = Sigma_b)
beta_eps <- sample_betas(deltas = delta_eps,
                        group_index = group_index_mm$m,
                        Sigma = Sigma_eps)
beta_rho <- sample_betas(deltas = delta_rho,
                        group_index = group_index_mm$m,
                        Sigma = Sigma_rho)


```


[^1]: Note that these are the _raw_ parameter values which are transformed such that $\epsilon^\prime \in [0,1]$ and $\rho^\prime \in [0,\infty)$.

# References
