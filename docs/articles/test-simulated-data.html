<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Test Simulated Data • probly</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">probly</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/collect-data.html">Collect and Clean SPLT Data</a>
    </li>
    <li>
      <a href="../articles/descriptive-statistics.html">Descriptive Statistics for SPLT data</a>
    </li>
    <li>
      <a href="../articles/test-simple-3l-mv-model.html">Test Simple 3-level Multivariate Normal Model</a>
    </li>
    <li>
      <a href="../articles/test-simulated-data.html">Test Simulated Data</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Test Simulated Data</h1>
                        <h4 class="author">John Flournoy</h4>
            
            <h4 class="date">2018-03-22</h4>
          </div>

    
    
<div class="contents">
<div id="introduction" class="section level1">
<h1 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h1>
<p>In this vignette, I simulate data as expected under the Rescorla-Wagner model implemented by <span class="citation">Ahn, Haines, &amp; Zhang (2017)</span> in their go-no-go model 2. Their original model handles binary decisions (button-press or no button-press) in response to four different cues. However, the form of the learning algorithm is generalizable to other binary choices in response to cues. In the case of the Social Probabilistic Learning Task (SPLT), participants are presented with a face (the cue), and must decide to press the left key or the right key. They are rewarded probabilistically such that for each cue, one or the other of the response options has an 80% chance of providing reinforcement. The go-no-go models used by <span class="citation">Ahn et al. (2017)</span> were derived from work by <span class="citation">Guitart-Masip et al. (2012)</span>. Their most flexible reinforcement learning model generates the probability of an action for each trial via N parameters: the learning rate, <span class="math inline">\(\epsilon\)</span>, the effective size of reinforcement, <span class="math inline">\(\rho\)</span>, a static bias parameter, <span class="math inline">\(b\)</span>, an irriducible noise parameter, <span class="math inline">\(\xi\)</span>, and a Pavlovian learning parameter, <span class="math inline">\(\pi\)</span>. In the SPLT, trial feedback does not vary by valence (responses result in reward, or no reward, but never punishment), so I use the model that does not include this Pavlonian component.</p>
</div>
<div id="reinforcement-learning-model-for-the-splt" class="section level1">
<h1 class="hasAnchor">
<a href="#reinforcement-learning-model-for-the-splt" class="anchor"></a>Reinforcement learning model for the SPLT</h1>
<p>The model for an individual <span class="math inline">\(j\)</span>’s probability of pressing the right arrow key on trial <span class="math inline">\(t\)</span> given that stimulus <span class="math inline">\(s_{t}\)</span> is presentd, <span class="math inline">\(P(a_{\rightarrow t} | s_{t})_{t}\)</span>, is determined by a logistic transformation of the action weight for pressing the right arrow key minus the action weight for pressing the left arrow key. This probability is then adjusted by a noise parameter, <span class="math inline">\(0 \leq\xi_{jk}\leq1\)</span> for each participant <span class="math inline">\(j\)</span> in condition <span class="math inline">\(k\)</span>. The noise parameter modulates the degree to which responses are non-systematic. When <span class="math inline">\(\xi\)</span> is 1, <span class="math inline">\(P_{it} = .5\)</span>, and because each individual has a unique noise parameter for each condition, I am able to account for participants who do not learn during the task, or in a particular condition. The full equation is:</p>
<p><span class="math display">\[
P(a_{\rightarrow t} | s_{t})_{t} = 
\text{logit}^{-1}\big(W(a_{\rightarrow t}| s_{t}) - W(a_{\leftarrow t}| s_{t})\big)\cdot(1-\xi_{jk}) + \small\frac{\xi_{jk}}{2}.
\]</span></p>
<p>The action weight is determined by a Rescorla-Wagner (RW) updating equation and individual <span class="math inline">\(j\)</span>’s bias parameter, <span class="math inline">\(b_{jk}\)</span>, for that condition (which encodes a systematic preference for choosing the left or right response option). In each condition, the same two words are displayed in the same position, so <span class="math inline">\(b\)</span> encodes a learning-independent preference for one particular word or position. The equation for the action weight for each action on a particular trial is:</p>
<p><span class="math display">\[
W_{t}(a,s) = \left\{
                \begin{array}{ll}
                  Q_{t}(a, s) + b_{jk}, &amp; \text{if } a=a_{\rightarrow} \\
                  Q_{t}(a, s), &amp; \text{otherwise}
                \end{array}
              \right.
\]</span> Finally, the RW updating equation that encodes instrumental learning is governed by the individual’s learning rate for that condition, <span class="math inline">\(\epsilon_{jk}\)</span>, and a scaling parameter <span class="math inline">\(\rho_{jk}\)</span> governing the effective size of the possible rewards <span class="math inline">\(r_t \in \{0, 1, 5\}\)</span>:</p>
<p><span class="math display">\[
Q_{t}(a_t, s_t) = Q_{t-1}(a_t, s_t) + \epsilon_{jk}\big(\rho_{jk}r_t - Q_{t-1}(a_t, s_t)\big)
\]</span></p>
<div id="hierarchical-parameters" class="section level2">
<h2 class="hasAnchor">
<a href="#hierarchical-parameters" class="anchor"></a>Hierarchical Parameters</h2>
<p>Each parameter (<span class="math inline">\(\epsilon, \rho, b, \xi\)</span>) varries by condition <span class="math inline">\(k \in 1:K\)</span>, and by participant <span class="math inline">\(j \in 1:J\)</span> nested in sample <span class="math inline">\(m \in 1:M\)</span>. The structure of the hierarchical part of the model is the same for each parameter, so the following description for <span class="math inline">\(\epsilon\)</span> will serve as a description for all of the parameters. For each individual <span class="math inline">\(j\)</span>, <span class="math inline">\(\beta_{\epsilon j}\)</span> is a <span class="math inline">\(K\)</span>-element row of coefficients for parameter <span class="math inline">\(\epsilon\)</span> for each condition:</p>
<p><span class="math display">\[
\beta_{\epsilon j} \sim \mathcal{N}(\delta_{\epsilon mm[j]}, \Sigma_{\epsilon})
\]</span> where <span class="math inline">\(\delta_{\epsilon mm[j]}\)</span> is a column of <span class="math inline">\(K\)</span> means for individual <span class="math inline">\(j\)</span>’s sample <span class="math inline">\(M\)</span>, as indexed in the vector <span class="math inline">\(mm\)</span>, and <span class="math inline">\(\Sigma_{\epsilon}\)</span> is a <span class="math inline">\(K\times K\)</span> matrix of the covariance of individual coefficients between conditions.</p>
<p>Finally, across all <span class="math inline">\(M\)</span> samples, the means for each condition k are distributed such that:</p>
<p><span class="math display">\[
\delta_{\epsilon k} \sim \mathcal{N}(\mu_{\epsilon k}, \sigma_\epsilon)
\]</span> where <span class="math inline">\(\mu_{\epsilon k}\)</span> is the population mean for parameter <span class="math inline">\(\epsilon\)</span> in condition <span class="math inline">\(k\)</span>, and <span class="math inline">\(\sigma\)</span> is a slightly regularizing scale parameter for these means across all conditions and samples.</p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h1>
<div id="refs" class="references">
<div id="ref-ahn2017">
<p>Ahn, W.-Y., Haines, N., &amp; Zhang, L. (2017). Revealing Neurocomputational Mechanisms of Reinforcement Learning and Decision-Making With the hBayesDM Package. <em>Computational Psychiatry</em>, <em>1</em>, 24–57. doi:<a href="https://doi.org/10.1162/CPSY_a_00002">10.1162/CPSY_a_00002</a></p>
</div>
<div id="ref-guitart-masip2012">
<p>Guitart-Masip, M., Huys, Q. J., Fuentemilla, L., Dayan, P., Duzel, E., &amp; Dolan, R. J. (2012). Go and no-go learning in reward and punishment: Interactions between affect and effect. <em>NeuroImage</em>, <em>62</em>(1), 154–166. doi:<a href="https://doi.org/10.1016/j.neuroimage.2012.04.024">10.1016/j.neuroimage.2012.04.024</a></p>
</div>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#introduction">Introduction</a></li>
      <li>
<a href="#reinforcement-learning-model-for-the-splt">Reinforcement learning model for the SPLT</a><ul class="nav nav-pills nav-stacked">
<li><a href="#hierarchical-parameters">Hierarchical Parameters</a></li>
      </ul>
</li>
      <li><a href="#references">References</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by John Flournoy.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
