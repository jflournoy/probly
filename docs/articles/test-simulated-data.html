<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Test Simulated Data • probly</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">probly</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/collect-data.html">Collect and Clean SPLT Data</a>
    </li>
    <li>
      <a href="../articles/descriptive-statistics.html">Descriptive Statistics for SPLT data</a>
    </li>
    <li>
      <a href="../articles/test-simple-3l-mv-model.html">Test Simple 3-level Multivariate Normal Model</a>
    </li>
    <li>
      <a href="../articles/test-simulated-data.html">Test Simulated Data</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Test Simulated Data</h1>
                        <h4 class="author">John Flournoy</h4>
            
            <h4 class="date">2018-03-28</h4>
          </div>

    
    
<div class="contents">
<p><strong>Aim 1a:</strong> Does framing reinforcement learning with (mate-seeking and status) motivational contexts sensitize the learner and potentiate learning?</p>
<p>My approach to answering this question is to model how learning occurs in each of the three motive contexts and examine differences in how learning occurs.</p>
<div id="task-descriptions" class="section level1">
<h1 class="hasAnchor">
<a href="#task-descriptions" class="anchor"></a>Task Descriptions</h1>
<p><strong>TASK DESCRIPTION HERE</strong></p>
</div>
<div id="a-model-for-reinforcement-learning" class="section level1">
<h1 class="hasAnchor">
<a href="#a-model-for-reinforcement-learning" class="anchor"></a>A model for reinforcement learning</h1>
<p>In the context of this task, where the relation between the optimal response and the stimulus is constant, a simple model of the degree of learning could rely on a simple proportion of optimal responses <span class="math inline">\(P_{ok}\)</span> for each condition <span class="math inline">\(k\)</span>. The test of the hypothesis of the effect of framing would then be the difference between conditions in <span class="math inline">\(P_o\)</span>. This simple model sacrifice precision for simplicity, and so I will be modeling the data using a reinforcement learning model with several parameters that can account for deviations from a strict Rescorla-Wagner (RW) process. This increases the number of possible comparisons I am able to make between conditions, which may generate useful information about how motive-domain framing affects the learning process (as modeled, of course), but which also increases the complexity of patterns between conditions and parameters that must be interpreted. It will be helpful to keep in mind that the framing can only be said to potentiate learning if, regardless of its affect on any model parameters, it does not result in higher proportions of optimal responding.</p>
<p>In this section, I simulate data as expected under the Rescorla-Wagner model implemented by <span class="citation">Ahn, Haines, &amp; Zhang (2017)</span> in their go-no-go model 2. Their original model handles binary decisions (button-press or no button-press) in response to four different cues. However, the form of the learning algorithm is generalizable to other binary choices in response to cues. In the case of the Social Probabilistic Learning Task (SPLT), participants are presented with a face (the cue), and must decide to press the left key or the right key. They are rewarded probabilistically such that for each cue, one or the other of the response options has an 80% chance of providing reinforcement. The go-no-go models used by <span class="citation">Ahn et al. (2017)</span> were derived from work by <span class="citation">Guitart-Masip et al. (2012)</span>. Their most flexible reinforcement learning model generates the probability of an action for each trial via N parameters: the learning rate, <span class="math inline">\(\epsilon\)</span>, the effective size of reinforcement, <span class="math inline">\(\rho\)</span>, a static bias parameter, <span class="math inline">\(b\)</span>, an irreducible noise parameter, <span class="math inline">\(\xi\)</span>, and a Pavlovian learning parameter, <span class="math inline">\(\pi\)</span>. In the SPLT, trial feedback does not vary by valence (responses result in reward, or no reward, but never punishment), so I use the model that does not include this Pavlovian component.</p>
</div>
<div id="reinforcement-learning-model-for-the-splt" class="section level1">
<h1 class="hasAnchor">
<a href="#reinforcement-learning-model-for-the-splt" class="anchor"></a>Reinforcement learning model for the SPLT</h1>
<p>The model for an individual <span class="math inline">\(j\)</span>’s probability of pressing the right arrow key on trial <span class="math inline">\(t\)</span> given that stimulus <span class="math inline">\(s_{t}\)</span> is presented, <span class="math inline">\(P(a_{\rightarrow t} | s_{t})_{t}\)</span>, is determined by a logistic transformation of the action weight for pressing the right arrow key minus the action weight for pressing the left arrow key. This probability is then adjusted by a noise parameter, <span class="math inline">\(0 \leq\xi_{jk}\leq1\)</span> for each participant <span class="math inline">\(j\)</span> in condition <span class="math inline">\(k\)</span>. The noise parameter modulates the degree to which responses are non-systematic. When <span class="math inline">\(\xi\)</span> is 1, <span class="math inline">\(P_{it} = .5\)</span>, and because each individual has a unique noise parameter for each condition, I am able to account for participants who do not learn during the task, or in a particular condition. The full equation is:</p>
<p><span class="math display">\[
P(a_{\rightarrow t} | s_{t})_{t} = 
\text{logit}^{-1}\big(W(a_{\rightarrow t}| s_{t}) - W(a_{\leftarrow t}| s_{t})\big)\cdot(1-\xi_{jk}) + \small\frac{\xi_{jk}}{2}.
\]</span></p>
<p>The action weight is determined by a Rescorla-Wagner (RW) updating equation and individual <span class="math inline">\(j\)</span>’s bias parameter, <span class="math inline">\(b_{jk}\)</span>, for that condition (which encodes a systematic preference for choosing the left or right response option). In each condition, the same two words are displayed in the same position, so <span class="math inline">\(b\)</span> encodes a learning-independent preference for one particular word or position. The equation for the action weight for each action on a particular trial is:</p>
<p><span class="math display">\[
W_{t}(a,s) = \left\{
                \begin{array}{ll}
                  Q_{t}(a, s) + b_{jk}, &amp; \text{if } a=a_{\rightarrow} \\
                  Q_{t}(a, s), &amp; \text{otherwise}
                \end{array}
              \right.
\]</span> Finally, the RW updating equation that encodes instrumental learning is governed by the individual’s learning rate for that condition, <span class="math inline">\(\epsilon_{jk}\)</span>, and a scaling parameter <span class="math inline">\(\rho_{jk}\)</span> governing the effective size of the possible rewards <span class="math inline">\(r_t \in \{0, 1, 5\}\)</span>:</p>
<p><span class="math display">\[
Q_{t}(a_t, s_t) = Q_{t-1}(a_t, s_t) + \epsilon_{jk}\big(\rho_{jk}r_t - Q_{t-1}(a_t, s_t)\big)
\]</span></p>
<div id="hierarchical-parameters" class="section level2">
<h2 class="hasAnchor">
<a href="#hierarchical-parameters" class="anchor"></a>Hierarchical Parameters</h2>
<p>Each parameter (<span class="math inline">\(\epsilon, \rho, b, \xi\)</span>) varies by condition <span class="math inline">\(k \in 1:K\)</span>, and by participant <span class="math inline">\(j \in 1:J\)</span> nested in sample <span class="math inline">\(m \in 1:M\)</span>. The structure of the hierarchical part of the model is the same for each parameter, so the following description for <span class="math inline">\(\epsilon\)</span> will serve as a description for all of the parameters. For each individual <span class="math inline">\(j\)</span>, <span class="math inline">\(\beta_{\epsilon j}\)</span> is a <span class="math inline">\(K\)</span>-element row of coefficients for parameter <span class="math inline">\(\epsilon\)</span> for each condition:</p>
<p><span class="math display">\[
\beta_{\epsilon j} \sim \mathcal{N}(\delta_{\epsilon mm[j]}, \Sigma_{\epsilon})
\]</span> where <span class="math inline">\(\delta_{\epsilon mm[j]}\)</span> is a column of <span class="math inline">\(K\)</span> means for individual <span class="math inline">\(j\)</span>’s sample <span class="math inline">\(M\)</span>, as indexed in the vector <span class="math inline">\(mm\)</span>, and <span class="math inline">\(\Sigma_{\epsilon}\)</span> is a <span class="math inline">\(K\times K\)</span> matrix of the covariance of individual coefficients between conditions.</p>
<p>Finally, across all <span class="math inline">\(M\)</span> samples, the means for each condition k are distributed such that:</p>
<p><span class="math display">\[
\delta_{\epsilon k} \sim \mathcal{N}(\mu_{\epsilon k}, \sigma_\epsilon)
\]</span></p>
<p>where <span class="math inline">\(\mu_{\epsilon k}\)</span> is the population mean for parameter <span class="math inline">\(\epsilon\)</span> in condition <span class="math inline">\(k\)</span>, and <span class="math inline">\(\sigma\)</span> is a slightly regularizing scale parameter for these means across all conditions and samples. The priors for these final parameters are:</p>
<p><span class="math display">\[
\mu_\epsilon \sim \mathcal{N}(0, 5)\\
\sigma_\epsilon \sim \text{exponential(1)}.
\]</span></p>
</div>
</div>
<div id="simulating-data" class="section level1">
<h1 class="hasAnchor">
<a href="#simulating-data" class="anchor"></a>Simulating data</h1>
<p>Before modeling the task data, I will confirm that this model can recover known parameters from simulated data. I simulate data based on the structure of the sample data, using the same number of participants per sample (see the section on <a href="descriptive-statistics.html">descriptive statistics</a>, as well as precisely the same task structure. For this aim, it is important to be able to recover all <span class="math inline">\(\mu_{\theta k}\)</span> for <span class="math inline">\(\theta \in \{\epsilon,\rho,b,\xi\}\)</span> and <span class="math inline">\(k \in \{1,2,3\}\)</span>, where 1 = Hungry/Thirsty, 2 = Popular/Unpopular, and 3 = Dating/Looking. Those parameters that account for idiosyncratic deviation from RW-expected behavior (<span class="math inline">\(b,\xi\)</span>) will not vary by condition. Based on interactive simulation (<a href="https://jflournoy.shinyapps.io/rw_model/">here</a>), reasonable parameter values for the control condition might be <span class="math inline">\(\mu_\epsilon = -1.65\)</span> and <span class="math inline">\(\mu_\rho = -0.3\)</span>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>The <a href="http://github.com/jflournoy/probly"><code>probly</code></a> package contains functions that help generate sample- and individually-varying coefficients for parameters, as well as simulated data from task structure.</p>
<p>One early indication that a model may not be well suited to a particular data set is that the model, given reasonable parameter values, is not able to produce simulated data that looks like the real data one is attempting to model. The simulated data do generally behave similarly to the actual data when we look just at the proportion of optimal presses over time (Figure @ref(fig:simdata) and Figure @ref(fig:simdatasample), below, and Figure X in the section on <a href="http://127.0.0.1:9740/rmd_output/4/descriptive-statistics.html">descriptive statistics</a>).</p>
<div class="figure">
<img src="test-simulated-data_files/figure-html/simdata-1.png" alt="(ref:simdat)" width="576"><p class="caption">
(ref:simdat)
</p>
</div>
<p>(ref:simdat) Simulated task data. This shows the the proportion of optimal presses across all participants for each trial. The best-fit line is a generalized additive model and is only intended for illustrative purposes.</p>
<div class="figure">
<img src="test-simulated-data_files/figure-html/simdatasample-1.png" alt="(ref:simdatsample)" width="576"><p class="caption">
(ref:simdatsample)
</p>
</div>
<p>(ref:simdatsample) Simulated task data for each sample.</p>
</div>
<div id="recovery-of-population-parameters" class="section level1">
<h1 class="hasAnchor">
<a href="#recovery-of-population-parameters" class="anchor"></a>Recovery of population parameters</h1>
<p>The model as described above was fit to simulated data using RStan <span class="citation">(version 2.17.3; Stan Development Team, 2018)</span>, sampling from 4 chains with 1000 warmup iterations and 500 sampling iterations per chain. The posterior means for each parameter are compared to those that generated the simulated task behavior. The plots in Figure @ref(fig:musimfitplot) allow visual comparison of the fitted model posteriors for each parameter to the data-generating population means, as well as to the means of the data-generating parameters for each sample, and for each individual. It is clear from these plots that the parameter estimates from this particular run capture the generating parameters, with two exceptions. First, the estimate of the population mean of the irrudicble noise parameter, <span class="math inline">\(\xi\)</span>, for one condition did not capture the generating parameter. This may be because this parameter was intentionally set very low so that <span class="math inline">\(\xi^\prime \approx 0\)</span>. The second instance occurs with the bias parameter for one condition. This may be acceptable because the parameters of interest safely capture the generating values, and this single condition bias parameter is not very far from the identity line. Additionally, it should be noted that the fitted model does capture the mean of generating <span class="math inline">\(\delta_b\)</span> and <span class="math inline">\(\beta_b\)</span> parameters for all conditions.</p>
<div class="figure">
<img src="test-simulated-data_files/figure-html/musimfitplot-1.png" alt="(ref:musimfitplot)" width="768"><p class="caption">
(ref:musimfitplot)
</p>
</div>
<p>(ref:musimfitplot) Comparison of model fit to generating parameters. Gray bars are 95% credible intervals. Black lines are identinty, where model fit and generating parameters are exactly equal.</p>
</div>
<div id="conclusion" class="section level1">
<h1 class="hasAnchor">
<a href="#conclusion" class="anchor"></a>Conclusion</h1>
<p>The model is able to recover data generating parameters from simulated data and is safe to use in interpretting the data provided by the study participants.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h1>
<div id="refs" class="references">
<div id="ref-ahn2017">
<p>Ahn, W.-Y., Haines, N., &amp; Zhang, L. (2017). Revealing Neurocomputational Mechanisms of Reinforcement Learning and Decision-Making With the hBayesDM Package. <em>Computational Psychiatry</em>, <em>1</em>, 24–57. doi:<a href="https://doi.org/10.1162/CPSY_a_00002">10.1162/CPSY_a_00002</a></p>
</div>
<div id="ref-guitart-masip2012">
<p>Guitart-Masip, M., Huys, Q. J., Fuentemilla, L., Dayan, P., Duzel, E., &amp; Dolan, R. J. (2012). Go and no-go learning in reward and punishment: Interactions between affect and effect. <em>NeuroImage</em>, <em>62</em>(1), 154–166. doi:<a href="https://doi.org/10.1016/j.neuroimage.2012.04.024">10.1016/j.neuroimage.2012.04.024</a></p>
</div>
<div id="ref-standevelopmentteam2018">
<p>Stan Development Team. (2018). RStan: The R interface to Stan. Retrieved from <a href="http://mc-stan.org/" class="uri">http://mc-stan.org/</a></p>
</div>
</div>
</div>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>Note that these are the <em>raw</em> parameter values which are transformed such that <span class="math inline">\(\epsilon^\prime \in [0,1]\)</span> and <span class="math inline">\(\rho^\prime \in [0,\infty)\)</span>.<a href="#fnref1">↩</a></p></li>
</ol>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#task-descriptions">Task Descriptions</a></li>
      <li><a href="#a-model-for-reinforcement-learning">A model for reinforcement learning</a></li>
      <li>
<a href="#reinforcement-learning-model-for-the-splt">Reinforcement learning model for the SPLT</a><ul class="nav nav-pills nav-stacked">
<li><a href="#hierarchical-parameters">Hierarchical Parameters</a></li>
      </ul>
</li>
      <li><a href="#simulating-data">Simulating data</a></li>
      <li><a href="#recovery-of-population-parameters">Recovery of population parameters</a></li>
      <li><a href="#conclusion">Conclusion</a></li>
      <li><a href="#references">References</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by John Flournoy.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
