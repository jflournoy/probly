<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Test Simulated Data • probly</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">probly</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/collect-data.html">Collect and Clean SPLT Data</a>
    </li>
    <li>
      <a href="../articles/descriptive-statistics.html">Descriptive Statistics for SPLT data</a>
    </li>
    <li>
      <a href="../articles/test-simple-3l-mv-model.html">Test Simple 3-level Multivariate Normal Model</a>
    </li>
    <li>
      <a href="../articles/test-simulated-data.html">Test Simulated Data</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Test Simulated Data</h1>
                        <h4 class="author">John Flournoy</h4>
            
            <h4 class="date">2018-03-23</h4>
          </div>

    
    
<div class="contents">
<div id="aim-1a-does-framing-reinforcement-learning-with-mate-seeking-and-status-motivational-contexts-sensitize-the-learner-and-potentiate-learning" class="section level1">
<h1 class="hasAnchor">
<a href="#aim-1a-does-framing-reinforcement-learning-with-mate-seeking-and-status-motivational-contexts-sensitize-the-learner-and-potentiate-learning" class="anchor"></a>Aim 1a: Does framing reinforcement learning with (mate-seeking and status) motivational contexts sensitize the learner and potentiate learning?</h1>
<div id="method" class="section level2">
<h2 class="hasAnchor">
<a href="#method" class="anchor"></a>Method</h2>
<p><strong>TASK DESCRIPTION HERE</strong></p>
</div>
<div id="modeling-learning" class="section level2">
<h2 class="hasAnchor">
<a href="#modeling-learning" class="anchor"></a>Modeling learning</h2>
<p>In the context of this task, where the relation between the optimal response and the stimulus is constant, a simple model of the degree of learning could rely on a simple proportion of optimal responses <span class="math inline">\(P_{ok}\)</span> for each condition <span class="math inline">\(k\)</span>. The test of the hypothesis of the effect of framing would then be the difference between conditions in <span class="math inline">\(P_o\)</span>. This simple model sacrifice precision for simplicity, and so I will be modeling the data using a reinforcement learning model with several parameters that can account for deviations from a strict Rescorla-Wagner (RW) process. This increases the number of possible comaprisons I am able to make between conditions, which may generate useful information about how motive-domain framing affects the learning process (as modelled, of course), but which also increases the complexity of patterns between conditions and parameters that must be interpretted. It will be helpful to keep in mind that the framing can only be said to potentiate learning if, regardless of its affect on any model parameters, it does not result in higher proportions of optimal responding.</p>
<div id="ensuring-the-model-can-recover-known-parameters" class="section level3">
<h3 class="hasAnchor">
<a href="#ensuring-the-model-can-recover-known-parameters" class="anchor"></a>Ensuring the model can recover known parameters</h3>
<p>Before modeling the task data, I will confirm that this model can recover known parameters from simulated data. In this section, I simulate data as expected under the Rescorla-Wagner model implemented by <span class="citation">Ahn, Haines, &amp; Zhang (2017)</span> in their go-no-go model 2. Their original model handles binary decisions (button-press or no button-press) in response to four different cues. However, the form of the learning algorithm is generalizable to other binary choices in response to cues. In the case of the Social Probabilistic Learning Task (SPLT), participants are presented with a face (the cue), and must decide to press the left key or the right key. They are rewarded probabilistically such that for each cue, one or the other of the response options has an 80% chance of providing reinforcement. The go-no-go models used by <span class="citation">Ahn et al. (2017)</span> were derived from work by <span class="citation">Guitart-Masip et al. (2012)</span>. Their most flexible reinforcement learning model generates the probability of an action for each trial via N parameters: the learning rate, <span class="math inline">\(\epsilon\)</span>, the effective size of reinforcement, <span class="math inline">\(\rho\)</span>, a static bias parameter, <span class="math inline">\(b\)</span>, an irriducible noise parameter, <span class="math inline">\(\xi\)</span>, and a Pavlovian learning parameter, <span class="math inline">\(\pi\)</span>. In the SPLT, trial feedback does not vary by valence (responses result in reward, or no reward, but never punishment), so I use the model that does not include this Pavlonian component.</p>
<div id="reinforcement-learning-model-for-the-splt" class="section level4">
<h4 class="hasAnchor">
<a href="#reinforcement-learning-model-for-the-splt" class="anchor"></a>Reinforcement learning model for the SPLT</h4>
<p>The model for an individual <span class="math inline">\(j\)</span>’s probability of pressing the right arrow key on trial <span class="math inline">\(t\)</span> given that stimulus <span class="math inline">\(s_{t}\)</span> is presentd, <span class="math inline">\(P(a_{\rightarrow t} | s_{t})_{t}\)</span>, is determined by a logistic transformation of the action weight for pressing the right arrow key minus the action weight for pressing the left arrow key. This probability is then adjusted by a noise parameter, <span class="math inline">\(0 \leq\xi_{jk}\leq1\)</span> for each participant <span class="math inline">\(j\)</span> in condition <span class="math inline">\(k\)</span>. The noise parameter modulates the degree to which responses are non-systematic. When <span class="math inline">\(\xi\)</span> is 1, <span class="math inline">\(P_{it} = .5\)</span>, and because each individual has a unique noise parameter for each condition, I am able to account for participants who do not learn during the task, or in a particular condition. The full equation is:</p>
<p><span class="math display">\[
P(a_{\rightarrow t} | s_{t})_{t} = 
\text{logit}^{-1}\big(W(a_{\rightarrow t}| s_{t}) - W(a_{\leftarrow t}| s_{t})\big)\cdot(1-\xi_{jk}) + \small\frac{\xi_{jk}}{2}.
\]</span></p>
<p>The action weight is determined by a Rescorla-Wagner (RW) updating equation and individual <span class="math inline">\(j\)</span>’s bias parameter, <span class="math inline">\(b_{jk}\)</span>, for that condition (which encodes a systematic preference for choosing the left or right response option). In each condition, the same two words are displayed in the same position, so <span class="math inline">\(b\)</span> encodes a learning-independent preference for one particular word or position. The equation for the action weight for each action on a particular trial is:</p>
<p><span class="math display">\[
W_{t}(a,s) = \left\{
                \begin{array}{ll}
                  Q_{t}(a, s) + b_{jk}, &amp; \text{if } a=a_{\rightarrow} \\
                  Q_{t}(a, s), &amp; \text{otherwise}
                \end{array}
              \right.
\]</span> Finally, the RW updating equation that encodes instrumental learning is governed by the individual’s learning rate for that condition, <span class="math inline">\(\epsilon_{jk}\)</span>, and a scaling parameter <span class="math inline">\(\rho_{jk}\)</span> governing the effective size of the possible rewards <span class="math inline">\(r_t \in \{0, 1, 5\}\)</span>:</p>
<p><span class="math display">\[
Q_{t}(a_t, s_t) = Q_{t-1}(a_t, s_t) + \epsilon_{jk}\big(\rho_{jk}r_t - Q_{t-1}(a_t, s_t)\big)
\]</span></p>
</div>
<div id="hierarchical-parameters" class="section level4">
<h4 class="hasAnchor">
<a href="#hierarchical-parameters" class="anchor"></a>Hierarchical Parameters</h4>
<p>Each parameter (<span class="math inline">\(\epsilon, \rho, b, \xi\)</span>) varries by condition <span class="math inline">\(k \in 1:K\)</span>, and by participant <span class="math inline">\(j \in 1:J\)</span> nested in sample <span class="math inline">\(m \in 1:M\)</span>. The structure of the hierarchical part of the model is the same for each parameter, so the following description for <span class="math inline">\(\epsilon\)</span> will serve as a description for all of the parameters. For each individual <span class="math inline">\(j\)</span>, <span class="math inline">\(\beta_{\epsilon j}\)</span> is a <span class="math inline">\(K\)</span>-element row of coefficients for parameter <span class="math inline">\(\epsilon\)</span> for each condition:</p>
<p><span class="math display">\[
\beta_{\epsilon j} \sim \mathcal{N}(\delta_{\epsilon mm[j]}, \Sigma_{\epsilon})
\]</span> where <span class="math inline">\(\delta_{\epsilon mm[j]}\)</span> is a column of <span class="math inline">\(K\)</span> means for individual <span class="math inline">\(j\)</span>’s sample <span class="math inline">\(M\)</span>, as indexed in the vector <span class="math inline">\(mm\)</span>, and <span class="math inline">\(\Sigma_{\epsilon}\)</span> is a <span class="math inline">\(K\times K\)</span> matrix of the covariance of individual coefficients between conditions.</p>
<p>Finally, across all <span class="math inline">\(M\)</span> samples, the means for each condition k are distributed such that:</p>
<p><span class="math display">\[
\delta_{\epsilon k} \sim \mathcal{N}(\mu_{\epsilon k}, \sigma_\epsilon)
\]</span></p>
<p>where <span class="math inline">\(\mu_{\epsilon k}\)</span> is the population mean for parameter <span class="math inline">\(\epsilon\)</span> in condition <span class="math inline">\(k\)</span>, and <span class="math inline">\(\sigma\)</span> is a slightly regularizing scale parameter for these means across all conditions and samples. The priors for these final paramters are:</p>
<p><span class="math display">\[
\mu_\epsilon \sim \mathcal{N}(0, 5)\\
\sigma_\epsilon \sim \text{exponential(1)}.
\]</span></p>
</div>
<div id="simulating-data" class="section level4">
<h4 class="hasAnchor">
<a href="#simulating-data" class="anchor"></a>Simulating data</h4>
<p>I simulate data based on the structure of the sample data, using the same number of participants per sample (see the section on <a href="descriptive-statistics.html">descriptive statistics</a>, as well as precisely the same task structure. For this aim, it is important to be able to recover all <span class="math inline">\(\mu_{\theta k}\)</span> for <span class="math inline">\(\theta \in \{\epsilon,\rho,b,\xi\}\)</span> and <span class="math inline">\(k \in \{1,2,3\}\)</span>, where 1 = Hungry/Thirsty, 2 = Popular/Unpopular, and 3 = Dating/Looking. Those parameters that account for idiosyncratic devation from RW-expected behavior (<span class="math inline">\(b,\xi\)</span>) will not vary by condition. Based on interactive simulation (<a href="https://jflournoy.shinyapps.io/rw_model/">here</a>), reasonable paramter values for the control condition might be <span class="math inline">\(\mu_\epsilon = -1.65\)</span> and <span class="math inline">\(\mu_\rho = -0.3\)</span>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>The functions that sample parameters as described above are:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sample_deltas &lt;-<span class="st"> </span>function(mu_vec, sigma, nsamples){
    deltas_vec &lt;-<span class="st"> </span><span class="kw">replicate</span>(nsamples,
                            <span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="kw">length</span>(mu_vec), 
                                  <span class="dt">mean =</span> mu_vec, <span class="dt">sd =</span> sigma))
    deltas &lt;-<span class="st"> </span><span class="kw">matrix</span>(deltas_vec,
                     <span class="dt">nrow =</span> nsamples,
                     <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
    <span class="kw">return</span>(deltas)
}
sample_betas &lt;-<span class="st"> </span>function(deltas, group_index, Sigma){
    <span class="kw">require</span>(MASS)
    deltas_mm &lt;-<span class="st"> </span>deltas[group_index, ]
    betas_l &lt;-<span class="st"> </span><span class="kw">lapply</span>(<span class="dv">1</span>:<span class="kw">length</span>(group_index), function(j){
        MASS::<span class="kw"><a href="http://www.rdocumentation.org/packages/MASS/topics/mvrnorm">mvrnorm</a></span>(<span class="dv">1</span>, <span class="dt">mu =</span> deltas[group_index[j], ], <span class="dt">Sigma =</span> Sigma)
    })
    betas &lt;-<span class="st"> </span><span class="kw">do.call</span>(rbind, betas_l)
    <span class="kw">return</span>(betas)
}
generate_responses &lt;-<span class="st"> </span>function(N, M, K, mm, Tsubj, cue, n_cues, condition, outcome, beta_xi, beta_b, beta_eps, beta_rho){
    <span class="co"># - N number of individuals</span>
    <span class="co"># - M number of samples</span>
    <span class="co"># - K number of conditions</span>
    <span class="co"># - mm sample ID for all individuals</span>
    <span class="co"># - Tsubj number of trials for each individual</span>
    <span class="co"># - cue an N x max(Tsubj) matrix of cue IDs for </span>
    <span class="co">#   each trial</span>
    <span class="co"># - n_cues total number of cues</span>
    <span class="co"># - condtion an N x max(Tsubj) matrix of condition </span>
    <span class="co">#   IDs for each trial</span>
    <span class="co"># - outcome is an array with dimensions N x T x 2</span>
    <span class="co">#   (response options) with the feedback for each </span>
    <span class="co">#   possible response. outcome[,,1] is for </span>
    <span class="co">#   correct left-presses, and outcome[,,2] is for</span>
    <span class="co">#   correct right-presses.</span>
    <span class="co"># - beta_xi, beta_b, beta_eps, beta_rho are N x K </span>
    <span class="co">#   matrices of the individually varying parameter </span>
    <span class="co">#   coeffients</span>
    
    press_right &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">nrow =</span> N, <span class="dt">ncol =</span> <span class="kw">max</span>(Tsubj)) <span class="co">#the matrix to return</span>
    
    for(i in <span class="dv">1</span>:N){
        wv_r &lt;-<span class="st"> </span><span class="kw">numeric</span>(n_cues) <span class="co">#action weight for press-right</span>
        wv_l &lt;-<span class="st"> </span><span class="kw">numeric</span>(n_cues) <span class="co">#action weight for press-left</span>
        qv_r &lt;-<span class="st"> </span><span class="kw">numeric</span>(n_cues) <span class="co">#Q value for right</span>
        qv_l &lt;-<span class="st"> </span><span class="kw">numeric</span>(n_cues) <span class="co">#Q value for left</span>
        p_right &lt;-<span class="st"> </span><span class="kw">numeric</span>(n_cues) <span class="co">#probability of pressing right</span>
        
        for(t in <span class="dv">1</span>:Tsubj[i]){
            wv_r[ cue[i, t] ]    &lt;-<span class="st"> </span>qv_r[ cue[i, t] ] +<span class="st"> </span>beta_b[ i, condition[i, t] ] <span class="co">#add bias</span>
            wv_l[ cue[i, t] ]    &lt;-<span class="st"> </span>qv_l[ cue[i, t] ]
            
            p_right[ cue[i, t] ] &lt;-<span class="st"> </span>arm::<span class="kw">invlogit</span>( wv_r[ cue[i, t] ] -<span class="st"> </span>wv_l[ cue[i, t] ] )
            p_right[ cue[i, t] ] &lt;-<span class="st"> </span>
<span class="st">                </span>p_right[ cue[i, t] ] *<span class="st"> </span>(<span class="dv">1</span> -<span class="st"> </span>beta_xi[ i, condition[i, t] ]) +<span class="st"> </span>
<span class="st">                </span>beta_xi[ i, condition[i, t] ] /<span class="st"> </span><span class="dv">2</span> <span class="co">#incorporate noise</span>
            
            press_right[i, t]    &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="dt">n =</span> <span class="dv">1</span>, <span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">prob =</span> p_right[ cue[i, t] ])
            
            if(press_right[i, t]){ <span class="co"># press_right[i, t] == 1</span>
                qv_r[ cue[i, t] ] &lt;-<span class="st"> </span>
<span class="st">                    </span>qv_r[ cue[i, t] ] +<span class="st"> </span>beta_eps[ i, condition[i, t] ] *
<span class="st">                    </span>(beta_rho[ i, condition[i, t] ] *<span class="st"> </span>outcome[i, t, <span class="dv">2</span>] -<span class="st"> </span>qv_r[ cue[i, t] ])
            } else { <span class="co"># press_right[i, t] == 0</span>
                qv_l[ cue[i, t] ] &lt;-<span class="st"> </span>
<span class="st">                    </span>qv_l[ cue[i, t] ] +<span class="st"> </span>beta_eps[ i, condition[i, t] ] *
<span class="st">                    </span>(beta_rho[ i, condition[i, t] ] *<span class="st"> </span>outcome[i, t, <span class="dv">1</span>] -<span class="st"> </span>qv_l[ cue[i, t] ])
            }
        } <span class="co"># t loop</span>
    }<span class="co"># i loop</span>
    <span class="kw">return</span>(press_right)
}

get_sample_index &lt;-<span class="st"> </span>function(splt_df, <span class="dt">id_col =</span> <span class="st">'id'</span>, <span class="dt">sample_col =</span> <span class="st">'sample'</span>, <span class="dt">levels =</span> <span class="kw">sort</span>(<span class="kw">unique</span>(splt_df[, sample_col]))) {
    mm &lt;-<span class="st"> </span><span class="kw">unique</span>(splt[, <span class="kw">c</span>(id_col, sample_col)])
    <span class="kw">rownames</span>(mm) &lt;-<span class="st"> </span><span class="dv">1</span>:<span class="kw">dim</span>(mm)[<span class="dv">1</span>]
    mm$m_fac &lt;-<span class="st"> </span><span class="kw">factor</span>(mm$sample,
                       <span class="dt">levels =</span> levels)
    mm$m &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(mm$m_fac)
    <span class="kw">return</span>(mm)
}

get_col_as_trial_matrix &lt;-<span class="st"> </span>function(splt_df, col, <span class="dt">id_col =</span> <span class="st">'id'</span>, <span class="dt">sample_col =</span> <span class="st">'sample'</span>, <span class="dt">trial_col =</span> <span class="st">'trial_index'</span>){
    <span class="co">#expects all rows with pressed_r == NA to have been removed</span>
    if(!(<span class="kw">is.factor</span>(splt_df[, col][[<span class="dv">1</span>]]) |<span class="st"> </span><span class="kw">is.numeric</span>(splt_df[, col][[<span class="dv">1</span>]]))){
        <span class="kw">stop</span>(<span class="st">"col must be numeric or a factor that will be coerced to numeric."</span>)
    }
    id_sample_index &lt;-<span class="st"> </span><span class="kw">paste0</span>(<span class="kw">unlist</span>(splt_df[,id_col]), <span class="kw">unlist</span>(splt_df[,sample_col]))
    ids &lt;-<span class="st"> </span><span class="kw">unique</span>(id_sample_index)
    max_trials &lt;-<span class="st"> </span><span class="kw">max</span>(<span class="kw">unlist</span>(<span class="kw">lapply</span>(<span class="kw">split</span>(splt_df, id_sample_index), 
                                    function(x) <span class="kw">dim</span>(x)[<span class="dv">1</span>])))
    col_mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">nrow =</span> <span class="kw">length</span>(ids), <span class="dt">ncol =</span> max_trials)
    for(id in ids){
        trials_index &lt;-<span class="st"> </span><span class="kw">unlist</span>(splt_df[id_sample_index ==<span class="st"> </span>id, trial_col])
        col_vec &lt;-<span class="st"> </span><span class="kw">unlist</span>(splt_df[id_sample_index ==<span class="st"> </span>id, col])[<span class="kw">order</span>(trials_index)]
        col_mat[<span class="kw">which</span>(ids ==<span class="st"> </span>id), <span class="dv">1</span>:<span class="kw">length</span>(col_vec)] &lt;-<span class="st"> </span>col_vec
    }<span class="co"># end id</span>
    <span class="kw">rownames</span>(col_mat) &lt;-<span class="st"> </span>ids
    <span class="kw">return</span>(col_mat)
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(probly)
<span class="kw">data</span>(splt)

splt &lt;-<span class="st"> </span>splt[!splt$id %in%<span class="st"> </span><span class="kw">c</span>(<span class="dv">43873</span>, <span class="st">'bad_pid'</span>, <span class="dv">43603</span>, <span class="dv">43991</span>, <span class="dv">43214</span>), ]
splt &lt;-<span class="st"> </span>splt[!(splt$id ==<span class="st"> </span><span class="dv">386</span> &amp;<span class="st"> </span>splt$sample ==<span class="st"> 'yads'</span>), ]
splt$sample &lt;-<span class="st"> </span><span class="kw">ifelse</span>(splt$sample ==<span class="st"> 'TDS3'</span>, <span class="st">'TDS1'</span>, splt$sample)
splt &lt;-<span class="st"> </span>splt[!<span class="kw">is.na</span>(splt$pressed_r), ]
splt$cue &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">as.factor</span>(<span class="kw">paste0</span>(splt$condition, <span class="st">'_'</span>, splt$sex)))
splt$condition &lt;-<span class="st"> </span><span class="kw">factor</span>(splt$condition, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">'HngT'</span>, <span class="st">'DtnL'</span>, <span class="st">'PplU'</span>))

<span class="co"># - N number of individuals</span>
<span class="co"># - M number of samples</span>
<span class="co"># - K number of conditions</span>
<span class="co"># - mm sample ID for all individuals</span>
<span class="co"># - Tsubj number of trials for each individual</span>
<span class="co"># - cue an N x max(Tsubj) matrix of cue IDs for </span>
<span class="co">#   each trial</span>
<span class="co"># - n_cues total number of cues</span>
<span class="co"># - condtion an N x max(Tsubj) matrix of condition </span>
<span class="co">#   IDs for each trial</span>
<span class="co"># - outcome is an array with dimensions N x T x 2</span>
<span class="co">#   (response options) with the feedback for each </span>
<span class="co">#   possible response. outcome[,,1] is for </span>
<span class="co">#   correct left-presses, and outcome[,,2] is for</span>
<span class="co">#   correct right-presses.</span>
<span class="co"># - beta_xi, beta_b, beta_eps, beta_rho are N x K </span>
<span class="co">#   matrices of the individually varying parameter </span>
<span class="co">#   coeffients</span>

group_index_mm &lt;-<span class="st"> </span><span class="kw">get_sample_index</span>(splt, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">"TDS1"</span>, <span class="st">"TDS2"</span>, <span class="st">"yads"</span>, <span class="st">"yads_online"</span>))
N &lt;-<span class="st"> </span><span class="kw">dim</span>(group_index_mm)[<span class="dv">1</span>]
M &lt;-<span class="kw">length</span>(<span class="kw">levels</span>(group_index_mm$m_fac)) 
K &lt;-<span class="st"> </span><span class="kw">length</span>(<span class="kw">unique</span>(splt$condition))
cue_mat &lt;-<span class="st"> </span><span class="kw">get_col_as_trial_matrix</span>(splt, <span class="st">'cue'</span>, <span class="dt">id_col =</span> <span class="st">'id'</span>, <span class="dt">sample_col =</span> <span class="st">'sample'</span>, <span class="dt">trial_col =</span> <span class="st">'trial_index'</span>)
condition_mat &lt;-<span class="st"> </span><span class="kw">get_col_as_trial_matrix</span>(splt, <span class="st">'condition'</span>, <span class="dt">id_col =</span> <span class="st">'id'</span>, <span class="dt">sample_col =</span> <span class="st">'sample'</span>, <span class="dt">trial_col =</span> <span class="st">'trial_index'</span>)
correct_r_mat &lt;-<span class="st"> </span><span class="kw">get_col_as_trial_matrix</span>(splt, <span class="st">'correct_r'</span>, <span class="dt">id_col =</span> <span class="st">'id'</span>, <span class="dt">sample_col =</span> <span class="st">'sample'</span>, <span class="dt">trial_col =</span> <span class="st">'trial_index'</span>)
reward_possible_mat &lt;-<span class="st"> </span><span class="kw">get_col_as_trial_matrix</span>(splt, <span class="st">'reward_possible'</span>, <span class="dt">id_col =</span> <span class="st">'id'</span>, <span class="dt">sample_col =</span> <span class="st">'sample'</span>, <span class="dt">trial_col =</span> <span class="st">'trial_index'</span>)
outcome_arr &lt;-<span class="st"> </span><span class="kw">array</span>(reward_possible_mat, <span class="dt">dim =</span> <span class="kw">c</span>(<span class="kw">dim</span>(reward_possible_mat), <span class="dv">2</span>))
outcome_arr[,,<span class="dv">1</span>][correct_r_mat ==<span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="dv">0</span>
outcome_arr[,,<span class="dv">2</span>][correct_r_mat ==<span class="st"> </span><span class="dv">0</span>] &lt;-<span class="st"> </span><span class="dv">0</span>

<span class="kw">set.seed</span>(<span class="dv">99232486</span>)

mu_xi &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">3</span>, <span class="kw">rep</span>(-<span class="fl">2.5</span>, <span class="dv">3</span>), .<span class="dv">25</span>)
mu_b &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">3</span>, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">3</span>), .<span class="dv">25</span>)
mu_eps &lt;-<span class="st"> </span><span class="kw">c</span>(-<span class="fl">1.65</span>, -<span class="fl">1.65</span> +<span class="st"> </span>.<span class="dv">3</span>, -<span class="fl">1.65</span> +<span class="st"> </span>.<span class="dv">2</span>)
mu_rho &lt;-<span class="st"> </span><span class="kw">c</span>(-.<span class="dv">3</span>, -.<span class="dv">3</span> +<span class="st"> </span>.<span class="dv">35</span>, -.<span class="dv">3</span> +<span class="st"> </span>.<span class="dv">45</span>)

delta_xi &lt;-<span class="st"> </span><span class="kw">sample_deltas</span>(mu_xi, .<span class="dv">1</span>, <span class="dv">4</span>)
delta_b &lt;-<span class="st"> </span><span class="kw">sample_deltas</span>(mu_b, .<span class="dv">1</span>, <span class="dv">4</span>)
delta_eps &lt;-<span class="st"> </span><span class="kw">sample_deltas</span>(mu_eps, .<span class="dv">1</span>, <span class="dv">4</span>)
delta_rho &lt;-<span class="st"> </span><span class="kw">sample_deltas</span>(mu_rho, .<span class="dv">1</span>, <span class="dv">4</span>)

Sigma &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(.<span class="dv">1</span>, <span class="dv">3</span>*<span class="dv">3</span>), <span class="dt">nrow =</span> <span class="dv">3</span>)
<span class="kw">diag</span>(Sigma) &lt;-<span class="st"> </span><span class="dv">1</span>

Sigma_eps &lt;-<span class="st"> </span>Sigma_rho &lt;-<span class="st"> </span>Sigma
Sigma_b &lt;-<span class="st"> </span>Sigma_xi &lt;-<span class="st"> </span><span class="kw">diag</span>(<span class="dv">3</span>)

beta_xi &lt;-<span class="st"> </span><span class="kw">sample_betas</span>(<span class="dt">deltas =</span> delta_xi,
                        <span class="dt">group_index =</span> group_index_mm$m,
                        <span class="dt">Sigma =</span> Sigma_xi)
<span class="co">#&gt; Loading required package: MASS</span>
beta_b &lt;-<span class="st"> </span><span class="kw">sample_betas</span>(<span class="dt">deltas =</span> delta_b,
                        <span class="dt">group_index =</span> group_index_mm$m,
                        <span class="dt">Sigma =</span> Sigma_b)
beta_eps &lt;-<span class="st"> </span><span class="kw">sample_betas</span>(<span class="dt">deltas =</span> delta_eps,
                        <span class="dt">group_index =</span> group_index_mm$m,
                        <span class="dt">Sigma =</span> Sigma_eps)
beta_rho &lt;-<span class="st"> </span><span class="kw">sample_betas</span>(<span class="dt">deltas =</span> delta_rho,
                        <span class="dt">group_index =</span> group_index_mm$m,
                        <span class="dt">Sigma =</span> Sigma_rho)</code></pre></div>
</div>
</div>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h1>
<div id="refs" class="references">
<div id="ref-ahn2017">
<p>Ahn, W.-Y., Haines, N., &amp; Zhang, L. (2017). Revealing Neurocomputational Mechanisms of Reinforcement Learning and Decision-Making With the hBayesDM Package. <em>Computational Psychiatry</em>, <em>1</em>, 24–57. doi:<a href="https://doi.org/10.1162/CPSY_a_00002">10.1162/CPSY_a_00002</a></p>
</div>
<div id="ref-guitart-masip2012">
<p>Guitart-Masip, M., Huys, Q. J., Fuentemilla, L., Dayan, P., Duzel, E., &amp; Dolan, R. J. (2012). Go and no-go learning in reward and punishment: Interactions between affect and effect. <em>NeuroImage</em>, <em>62</em>(1), 154–166. doi:<a href="https://doi.org/10.1016/j.neuroimage.2012.04.024">10.1016/j.neuroimage.2012.04.024</a></p>
</div>
</div>
</div>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>Note that these are the <em>raw</em> parameter values which are transformed such that <span class="math inline">\(\epsilon^\prime \in [0,1]\)</span> and <span class="math inline">\(\rho^\prime \in [0,\infty)\)</span>.<a href="#fnref1">↩</a></p></li>
</ol>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li>
<a href="#aim-1a-does-framing-reinforcement-learning-with-mate-seeking-and-status-motivational-contexts-sensitize-the-learner-and-potentiate-learning">Aim 1a: Does framing reinforcement learning with (mate-seeking and status) motivational contexts sensitize the learner and potentiate learning?</a><ul class="nav nav-pills nav-stacked">
<li><a href="#method">Method</a></li>
      <li><a href="#modeling-learning">Modeling learning</a></li>
      </ul>
</li>
      <li><a href="#references">References</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by John Flournoy.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
